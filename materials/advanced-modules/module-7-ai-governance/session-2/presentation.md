# **POWERPOINT PRESENTATION: ADVANCED MODULE 7 SESSION 2**
## **Implementation & Compliance**

**Module:** Advanced Module 7: Enterprise AI Governance
**Session Number:** 2 of 2
**Session Duration:** 45 minutes
**Delivery Format:** Live MS Teams workshop

**Target Audience:** Consulting professionals, risk officers, compliance leads, and Block 3 graduates

**Key Thesis:** Operationalizing AI governance requires translating regulatory requirements and responsible AI principles into documented organizational structures, automated audit trails, and risk-based agent autonomy classifications, because effective governance cannot remain abstract—it must manifest in specific policies, measurable controls, and accountable decision-making processes that survive regulatory scrutiny.

**Session Learning Objectives:** By the end of this session, participants will:
1. Map regulatory compliance requirements (EU AI Act, sector-specific) to AI systems
2. Design organizational governance structures (centralized vs. federated models)
3. Classify agent autonomy levels and define appropriate human oversight
4. Implement audit trail requirements and accountability frameworks

**Entry Criteria:**
- [ ] Governance maturity assessment completed (Exercise 1.1)
- [ ] Responsible AI principles mapped (Exercise 1.2)
- [ ] Initial risk register created (Exercise 1.3)
- [ ] Understanding of organizational context

**Exit Criteria:**
- [ ] Regulatory compliance requirements mapped
- [ ] Agent autonomy classification system implemented
- [ ] Governance policies drafted
- [ ] Organizational structure recommendations documented
- [ ] Module capstone completed

**Presentation Structure:**
1. Opening & Session 1 Recap (3 min) - Slides 1-3
2. Segment 1: Regulatory Compliance Landscape (12 min) - Slides 4-7
3. Segment 2: Organizational Structures (10 min) - Slides 8-10
4. Segment 3: Agent Autonomy Classification (12 min) - Slides 11-14
5. Segment 4: Audit Trails & Accountability (11 min) - Slides 15-17
6. Capstone Preview & Close (3 min) - Slides 18-20

**Total Slides:** 20

---

## Slide Definitions

### SLIDE 1: TITLE SLIDE

**Title:** Advanced Module 7 Session 2: Implementation & Compliance

**Subtitle:** Building Operational AI Governance Frameworks

**Content:**
- [Instructor Name]
- [Date/Cohort identifier]
- AI Practitioner Training Program

**Graphic:** Professional slide with green advanced module tones. Visual of governance structure or compliance framework.

**SPEAKER NOTES:**

"Welcome to Session 2 of Enterprise AI Governance. Last week we built the foundation - NIST frameworks, responsible AI principles, lifecycle governance, and data governance. Today we make it operational.

By the end of this session, you'll know how to navigate regulatory compliance - from EU AI Act to sector-specific regulations. You'll be able to design governance structures that fit your organizational model. You'll classify AI agents by autonomy level and define appropriate human oversight. And you'll understand what audit trails and accountability frameworks must include.

Most importantly, you'll complete the module capstone - a complete Enterprise AI Governance Framework that integrates everything from both sessions into a client-ready deliverable.

Let's start by recapping where we are."

[Transition]

---

### SLIDE 2: SESSION 1 RECAP

**Title:** Building on Session 1 Foundations

**Content:**

**What We Covered Last Week:**
- NIST AI RMF & ISO 42001 frameworks
- Six responsible AI principles (Fairness, Transparency, Accountability, Privacy, Safety, Human Oversight)
- Agent lifecycle governance (Design → Develop → Test → Deploy → Monitor → Retire)
- Data governance pillars (Lineage, Quality, Privacy, Access)

**Your Session 1 Deliverables:**
- Governance Maturity Assessment (7 domains scored)
- Responsible AI Principles Mapping (6 principles operationalized)
- Agent Risk Register (risks identified and scored)

**How Session 2 Builds On This:**
Your Session 1 assessments become Part 1 of today's capstone framework

**Graphic:** Visual showing Session 1 as foundation, Session 2 as implementation layer

**SPEAKER NOTES:**

"Quick recap of Session 1. We established the governance foundations - the frameworks like NIST that structure your approach, the principles that guide decisions, the lifecycle thinking that keeps governance continuous, and the data governance that underpins everything.

You created three deliverables: a maturity assessment showing where your organization stands, a principles mapping that operationalizes abstract concepts, and a risk register identifying what could go wrong.

[Pause]

Today we shift from 'what to govern' to 'how to govern in practice.' That means understanding regulatory requirements, designing organizational structures, classifying AI autonomy, and ensuring accountability through audit trails.

Your Session 1 deliverables aren't just exercises - they become Part 1 of your capstone governance framework. We're building on that foundation today."

[Transition]

---

### SLIDE 3: LEARNING OBJECTIVES

**Title:** Today's Outcomes

**Content:**

You will be able to:

1. **Map regulatory compliance requirements to AI systems**
   - EU AI Act risk tiers, sector-specific regulations, compliance gaps

2. **Design appropriate governance organizational structures**
   - Centralized vs. federated models, key roles, governance committees

3. **Classify agent autonomy and define oversight requirements**
   - Five autonomy levels (0-4), decision criteria, human oversight models

4. **Implement audit trails and accountability frameworks**
   - Logging requirements, retention policies, accountability mapping

**Graphic:** Four objectives as pillars supporting "Enterprise AI Governance"

**SPEAKER NOTES:**

"Here are our four learning objectives:

First, regulatory compliance. You'll learn how to map the EU AI Act's risk tiers to your AI systems, understand sector-specific regulations like HIPAA for healthcare or SR 11-7 for financial services, and identify compliance gaps.

Second, organizational structures. Should governance be centralized with a strong AI Governance Council, or federated with business unit autonomy? What roles do you need - Chief AI Officer, AI Ethics Officer, AI Risk Manager? What committees?

Third, agent autonomy classification. Not all agents should operate with the same level of autonomy. You'll learn how to classify agents from Level 0 (tool) to Level 4 (autonomous) and what human oversight each level requires.

Fourth, audit trails and accountability. What must be logged for each AI decision? How long must logs be retained? How do you map accountability from agent actions to specific individuals?

Exercise 2.1 addresses objectives 1. Exercise 2.2 covers 2-4. The capstone integrates everything.

Let's start with regulatory compliance."

[Transition: Segment 1]

---

## SEGMENT 1: REGULATORY COMPLIANCE LANDSCAPE
### Duration: 12 minutes | Slides 4-7

---

### SLIDE 4: THE COMPLIANCE CHALLENGE

**Title:** The Expanding AI Regulatory Landscape

**Content:**

**The Challenge:**
AI governance frameworks must address multiple, sometimes conflicting, regulatory requirements across jurisdictions and sectors.

**Current State:**
- EU AI Act: Comprehensive risk-based regulation (2024-2027 phased implementation)
- US: Sector-specific regulations, no comprehensive federal AI law
- State/Local: NYC Local Law 144 (employment AI), California CCPA (data), evolving landscape
- Industry: Financial services (SR 11-7), healthcare (FDA SaMD, HIPAA), insurance (state regulators)

**Why It Matters:**
- Non-compliance penalties can be severe (up to 6% global revenue for EU AI Act)
- Different regulations have conflicting requirements
- Landscape is rapidly evolving
- Procurement increasingly requires compliance demonstration

**Graphic:** World map showing different regulatory regimes, or complexity visual showing overlapping regulations

**SPEAKER NOTES:**

"Let me set the compliance challenge.

Two years ago, AI governance was largely voluntary. Organizations adopted principles because it was the right thing to do, or because it protected reputation. Regulatory requirements were indirect - data protection laws, sector-specific rules that happened to apply to AI.

That's changing rapidly.

The EU AI Act creates comprehensive, mandatory requirements for AI systems. High-risk AI faces strict obligations. Some AI uses are prohibited entirely. And penalties are massive - up to 6% of global annual revenue.

In the US, we don't have comprehensive federal AI regulation yet. But we have sector-specific requirements. Financial services has SR 11-7 for model risk management. Healthcare has FDA oversight for AI as medical devices. Employment AI now faces bias audit requirements in NYC.

[Point to graphic]

The challenge is navigating this patchwork. If you operate in multiple jurisdictions or sectors, you're dealing with overlapping, sometimes conflicting, requirements.

Today you'll learn a systematic approach to compliance mapping. Let's start with the EU AI Act - currently the most comprehensive AI regulation."

[Transition]

**BACKGROUND:**

**Rationale:**
- This slide establishes urgency and practical motivation for compliance-focused governance
- Creates business case for investment in governance frameworks through penalty exposure
- Frames the complexity challenge that subsequent slides will address with solutions

**Key Research & Citations:**
- **EU AI Act**: Adopted June 2024, phased implementation through 2027, applies extraterritorially to systems used in EU
- **Penalties**: Up to €35M or 7% global revenue for prohibited AI, €15M or 3% for non-compliance with obligations
- **US landscape**: Fragmented - NIST AI RMF (voluntary), sector regulations (mandatory), state laws (varying)
- **NYC Local Law 144**: Employment AI bias audit requirements effective July 2023
- **Financial services**: SR 11-7 model risk management, OCC fairness guidelines

**Q&A Preparation:**
- *"Do we need to comply if we're US-based?"*: If you serve EU customers or process EU resident data with AI, yes - extraterritorial application
- *"Which regulation should we prioritize?"*: Start with most stringent applicable regulation (often EU AI Act), then layer sector-specific requirements
- *"How do we stay current with evolving regulations?"*: Subscribe to regulatory monitoring services, join industry groups, quarterly compliance review cadence

---

### SLIDE 5: EU AI ACT RISK TIERS

**Title:** EU AI Act: Risk-Based Approach

**Content:**

```
┌─────────────────────────────────────────────────────────┐
│                  EU AI ACT RISK TIERS                   │
│                                                         │
│  ┌─────────────────────────────────────────────────┐   │
│  │ UNACCEPTABLE RISK - PROHIBITED                  │   │
│  │ Social scoring, real-time biometric ID          │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ HIGH RISK - STRICT REQUIREMENTS                 │   │
│  │ Credit scoring, hiring, medical devices         │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ LIMITED RISK - TRANSPARENCY OBLIGATIONS         │   │
│  │ Chatbots, emotion recognition                   │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ MINIMAL RISK - NO SPECIFIC REQUIREMENTS         │   │
│  │ AI-enabled games, spam filters                  │   │
│  └─────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

**High-Risk System Requirements:**
- Risk management system
- Data governance and lineage
- Technical documentation
- Record-keeping and audit trails
- Transparency to users
- Human oversight
- Accuracy, robustness, cybersecurity

**Graphic:** EU AI Act pyramid showing risk tiers

**GRAPHICS:**

**Graphic 1: EU AI Act Risk Pyramid**
- Purpose: Visualize the hierarchical risk classification system with decreasing regulatory burden
- Type: Pyramid diagram with four tiers
- Elements: Four horizontal sections stacked in pyramid shape; top (smallest) is Unacceptable/Prohibited in red; second tier is High Risk in orange; third is Limited Risk in yellow; bottom (largest) is Minimal Risk in green; prohibition symbol on top tier
- Labels: Tier name and regulatory requirement prominently displayed on each level; 3-4 example use cases per tier; compliance burden indicator (High/Medium/Low/None)
- Relationships: Vertical axis showing increasing regulatory requirements; size of sections indicates proportion of AI systems in each category; arrows showing where most enterprise AI falls (High/Limited tiers)

**Graphic 2: High-Risk Requirements Checklist**
- Purpose: Detail the seven mandatory requirements for high-risk AI systems
- Type: Circular hub-and-spoke or checklist diagram
- Elements: Central circle labeled "High-Risk AI System"; seven spokes radiating outward, each representing one requirement; each spoke terminates in box with requirement details
- Labels: Requirement names clearly visible; key implementation activities in each box; icons representing each requirement (shield for risk mgmt, database for data governance, etc.)
- Relationships: All requirements must be met (AND logic); dotted lines showing interdependencies (e.g., risk mgmt informs documentation); color coding showing implementation difficulty or timeline

**Graphic 3: Use Case Classification Guide**
- Purpose: Help organizations quickly determine which tier their AI system falls into
- Type: Decision matrix or quadrant chart
- Elements: Two axes (Impact on fundamental rights, Sector/domain); plotted examples showing where different use cases fall; colored zones corresponding to risk tiers
- Labels: Axis labels clearly defined; examples plotted with brief description; zone boundaries showing tier transitions; callout boxes for edge cases
- Relationships: Visual clustering of similar use cases; arrows or guidelines showing how to classify new systems; emphasis on high-risk sector list (employment, credit, healthcare, law enforcement)

**SPEAKER NOTES:**

"The EU AI Act uses a risk-based approach with four tiers.

At the top, Unacceptable Risk - these AI systems are prohibited. Social scoring by governments, real-time biometric identification in public spaces, certain manipulative AI. You can't deploy these in the EU, period.

Next, High Risk - these face strict requirements. This includes AI used in credit scoring, employment, education, law enforcement, critical infrastructure, and medical devices. If your AI falls in this category, you have extensive obligations.

[Point to requirements]

You need a formal risk management system - not just 'we think about risk,' but documented processes. You need data governance with lineage tracking. You need technical documentation that explains how your AI works. You need comprehensive record-keeping - audit trails for decisions. You must provide transparency to users - they need to know AI is being used. You need human oversight - humans must be able to intervene. And you need to demonstrate accuracy, robustness, and cybersecurity.

Limited Risk tier requires transparency - you must disclose AI use. Chatbots must tell users they're bots. Emotion recognition or biometric categorization must be disclosed.

Minimal Risk has no specific requirements - AI-enabled games, spam filters.

[Pause]

Most enterprise AI falls into High Risk or Limited Risk categories. If you're doing hiring, lending, insurance decisioning, medical diagnosis - you're High Risk under EU AI Act.

In Exercise 2.1, you'll map these tiers to your AI systems and identify compliance gaps."

[Transition]

**BACKGROUND:**

**Rationale:**
- EU AI Act represents the most comprehensive AI-specific regulation globally and sets precedent for other jurisdictions
- Risk-based approach provides practical framework that can be adapted to other regulatory contexts
- High-risk requirements define the compliance baseline that many other regulations will reference

**Key Research & Citations:**
- **EU AI Act** (Regulation (EU) 2024/1689): Adopted June 2024, phased implementation through 2027
- **Prohibited systems**: Article 5 lists including social scoring (5.1.c), real-time remote biometric identification in public spaces (5.1.d)
- **High-risk categories**: Annex III lists 8 categories including employment, education, law enforcement, critical infrastructure, biometric identification
- **Penalties**: Article 99 - up to €35M or 7% global revenue for prohibited AI; €15M or 3% for high-risk obligations violations
- **Extraterritorial scope**: Article 2 applies to providers/deployers in EU and to providers outside EU if output used in EU

**Q&A Preparation:**
- *"Does EU AI Act apply to our US company?"*: Yes if you provide AI to EU users or process EU data - extraterritorial application like GDPR
- *"How do we know if our AI is high-risk?"*: Check Annex III categories (employment, credit, law enforcement, etc.) - if your AI makes/significantly influences decisions in these areas, likely high-risk
- *"What if we're already GDPR compliant?"*: Helpful but insufficient - EU AI Act has additional requirements specific to AI systems (bias testing, human oversight, conformity assessment)
- *"Can we self-certify as high-risk compliant?"*: No for most high-risk systems - requires third-party conformity assessment by notified body

---

### SLIDE 6: SECTOR-SPECIFIC REGULATIONS

**Title:** Beyond EU AI Act: Sector-Specific Requirements

**Content:**

| Sector | Key Regulations | AI-Specific Implications |
|--------|----------------|-------------------------|
| **Financial Services** | SR 11-7 (US Fed), Basel III/IV, MiFID II | Model risk management, model validation, explainability requirements, documentation standards |
| **Healthcare** | HIPAA, FDA Software as Medical Device (SaMD), EU MDR | Patient safety validation, clinical evidence, adverse event reporting, data privacy |
| **Employment** | EEOC (US), NYC Local Law 144, GDPR Art 22 | Bias audits, candidate notification, right to human review, statistical fairness testing |
| **Insurance** | State insurance regulations, NAIC Model Bulletin | Actuarial justification, anti-discrimination rules, rate filing requirements |

**Compliance Mapping Approach:**
1. Identify applicable regulations for your industry and jurisdictions
2. Map specific requirements to your AI systems
3. Conduct gap assessment against current state
4. Create remediation plan with priorities
5. Establish ongoing monitoring for regulatory changes

**Graphic:** Table or matrix showing sectors and their regulatory requirements

**SPEAKER NOTES:**

"Beyond the EU AI Act, you need to understand sector-specific regulations.

Financial services: SR 11-7 from the Federal Reserve requires model risk management for any model used in decisions - including AI models. You need independent model validation, comprehensive documentation, and ongoing monitoring. Basel banking regulations and MiFID II in Europe add explainability requirements.

Healthcare: FDA regulates AI as Software as Medical Device if it diagnoses, treats, or monitors patients. You need clinical validation, adverse event reporting, and strict data privacy under HIPAA.

Employment: EEOC enforces anti-discrimination laws. NYC Local Law 144 now requires annual bias audits for AI used in hiring or promotion, with specific statistical tests. GDPR Article 22 gives candidates the right to human review of automated employment decisions.

Insurance: State regulators oversee insurance AI. You need actuarial justification for rate decisions, anti-discrimination compliance, and in many states, detailed rate filings that explain AI logic.

[Point to compliance approach]

The approach is systematic: identify your applicable regulations, map requirements to specific AI systems, assess gaps, remediate, and monitor for changes.

This is complex, which is why Exercise 2.1 provides a structured template for regulatory compliance mapping."

[Transition]

---

### SLIDE 7: COMPLIANCE SUMMARY

**Title:** Key Compliance Takeaways

**Content:**

**Remember:**
1. **Know your risk tier** - EU AI Act classification drives requirements intensity
2. **Sector regulations often stricter** - Industry-specific rules may exceed general AI regulations
3. **Compliance is ongoing** - Regulations evolve; establish monitoring processes
4. **Document everything** - Compliance requires demonstrable evidence, not just claims

**You'll Practice:**
Exercise 2.1 creates a regulatory compliance mapping for your organization

**Common Pitfall:**
Assuming compliance is a legal team problem. Compliance requires technical, operational, and governance changes - it's a cross-functional effort.

**Graphic:** Compliance cycle showing assess → remediate → monitor → reassess

**SPEAKER NOTES:**

"Before moving on, key compliance points:

One, know your risk tier under EU AI Act. If you're High Risk, you have extensive obligations. If you're Limited Risk, focus on transparency. This classification drives your compliance intensity.

Two, sector regulations are often stricter than general AI laws. If you're in financial services, healthcare, or employment, sector rules may require more than EU AI Act. Map both.

Three, compliance is ongoing. The EU AI Act has phased implementation through 2027. US states are passing new AI laws. Regulators are issuing guidance. You need processes to monitor regulatory changes.

Four, document everything. Saying 'we govern AI responsibly' isn't compliance. You need documented risk assessments, test results, audit trails, approval records. Evidence.

[Pause]

Common mistake: treating compliance as purely a legal issue. Legal needs to be involved, but compliance requires technical capabilities - bias testing tools, audit logging systems, monitoring infrastructure. It requires operational changes - approval workflows, review processes. It's cross-functional.

Exercise 2.1 gives you the compliance mapping template. Now let's talk about organizational structures to govern AI."

[Transition: Segment 2]

---

## SEGMENT 2: ORGANIZATIONAL STRUCTURES FOR AI GOVERNANCE
### Duration: 10 minutes | Slides 8-10

---

### SLIDE 8: GOVERNANCE STRUCTURE MODELS

**Title:** Centralized vs. Federated Governance

**Content:**

**Centralized Model:**
```
                  ┌─────────────────┐
                  │ AI Governance   │
                  │    Council      │
                  └────────┬────────┘
                           │
          ┌────────────────┼────────────────┐
          │                │                │
     ┌────┴────┐     ┌─────┴─────┐    ┌────┴────┐
     │ BU 1    │     │   BU 2    │    │  BU 3   │
     │ AI Team │     │  AI Team  │    │ AI Team │
     └─────────┘     └───────────┘    └─────────┘
```
- Central authority sets all standards and policies
- Consistent governance across organization
- Best for regulated industries, high-risk AI

**Federated Model:**
```
     ┌─────────────────────────────────────────────┐
     │        AI Center of Excellence              │
     │     (Standards, Best Practices)             │
     └─────────────────────────────────────────────┘
                          ↕
     ┌─────────┐    ┌─────────┐    ┌─────────┐
     │ BU 1    │    │  BU 2   │    │  BU 3   │
     │ AI Gov  │    │ AI Gov  │    │ AI Gov  │
     │ (Local) │    │ (Local) │    │ (Local) │
     └─────────┘    └─────────┘    └─────────┘
```
- Business units have autonomy with central guidance
- Flexible implementation for local contexts
- Best for diverse organizations, lower-risk AI

**Graphic:** Side-by-side comparison diagrams

**GRAPHICS:**

**Graphic 1: Centralized vs. Federated Org Chart Comparison**
- Purpose: Visually contrast the two governance models with clear reporting structure
- Type: Side-by-side organizational hierarchy diagrams
- Elements: Left side showing centralized model with strong top-down structure; right side showing federated with bidirectional flows; boxes representing organizational units; arrows showing authority/influence flow
- Labels: Left labeled "Centralized - Command & Control"; right labeled "Federated - Collaborate & Align"; role names in boxes; authority type indicated (Mandates vs. Guidelines)
- Relationships: Left shows vertical command structure; right shows horizontal collaboration plus vertical guidance; thickness of arrows indicates strength of authority; color coding for decision types (Policy=central, Implementation=local)

**Graphic 2: Decision Authority Matrix**
- Purpose: Show which governance decisions are made centrally vs. locally in each model
- Type: Comparison matrix table
- Elements: Rows listing different decision types (Policy creation, Risk classification, Tool selection, Budget allocation, etc.); columns for Centralized and Federated models; cells indicating Central/Local/Shared authority
- Labels: Decision category names; authority level (Central/BU/Shared); speed indicator (Fast/Moderate/Slow); typical owners listed
- Relationships: Color coding showing authority distribution; patterns showing where models differ most; callout boxes for critical decisions

**Graphic 3: Model Selection Decision Tree**
- Purpose: Guide organizations to choose appropriate governance model
- Type: Decision tree flowchart
- Elements: Starting point "Choose Governance Model" → decision diamonds based on org characteristics (Industry regulation, Org size, AI risk profile, Cultural preference) → terminal recommendations (Centralized/Federated/Hybrid)
- Labels: Clear decision criteria at each node; example organizations for each path; pros/cons callouts; hybrid model option shown as middle path
- Relationships: Flow from top to bottom; multiple paths leading to same outcome showing flexibility; emphasis on hybrid as most common real-world choice

**SPEAKER NOTES:**

"One of the first design decisions: centralized or federated governance?

Centralized model: You have a central AI Governance Council with strong authority. They set all standards, policies, and approval requirements. Business units implement AI, but governance decisions flow through the center.

[Point to centralized diagram]

Advantages: Consistency, strong control, clear accountability. If you're in a regulated industry like financial services or healthcare, centralized governance gives you the control you need for compliance.

Disadvantages: Can be slow, bureaucratic, and may not fit local business unit contexts well.

[Point to federated diagram]

Federated model: You have an AI Center of Excellence that provides guidance, best practices, templates. But each business unit maintains its own AI governance function with autonomy to adapt to local needs.

Advantages: Flexibility, faster decision-making, better alignment with business unit contexts.

Disadvantages: Less consistency, risk of governance gaps if some business units underinvest.

[Pause]

Most large organizations use a hybrid - central policies and standards from a governance council or center of excellence, but business units have autonomy within those guardrails.

The key is matching structure to your organization's culture, risk tolerance, and regulatory context. Your capstone will require you to recommend a structure and justify it."

[Transition]

**BACKGROUND:**

**Rationale:**
- This slide provides the foundational organizational design choices for AI governance
- Addresses common tension between control/consistency and agility/local adaptation
- Critical decision that impacts all subsequent governance implementation

**Key Research & Citations:**
- Organizational design patterns based on McKinsey/Gartner research on AI governance structures
- Centralized model common in financial services (regulatory compliance priority), healthcare (safety/liability)
- Federated model common in technology companies (innovation priority), diversified conglomerates (business unit autonomy)
- Hybrid model represents 60%+ of mature organizations according to Deloitte AI Governance Survey 2023

**Q&A Preparation:**
- *"Can we change governance structure later?"*: Yes, governance maturity typically progresses from ad-hoc → centralized → hybrid/federated as organization gains AI experience
- *"What if different AI systems need different governance?"*: Risk-based approach - high-risk AI requires centralized oversight even in federated model
- *"How do we handle third-party AI tools?"*: Center of Excellence reviews/approves tools, business units govern usage - natural hybrid pattern

---

### SLIDE 9: KEY GOVERNANCE ROLES

**Title:** Building the AI Governance Team

**Content:**

| Role | Responsibility | Reports To | When Needed |
|------|----------------|------------|-------------|
| **Chief AI Officer** | Enterprise AI strategy, governance oversight, executive sponsor | CEO / Board | Medium+ AI maturity |
| **AI Ethics Officer** | Responsible AI principles, ethics reviews, escalation resolution | CAO or Legal | High-risk AI deployments |
| **AI Risk Manager** | Risk framework, risk assessments, monitoring | CRO | Regulated industries |
| **AI Governance Lead** | Policy implementation, compliance, process management | CAO or CTO | All organizations with AI |
| **Model Validators** | Independent technical review of models/agents | Risk function | High-risk AI systems |

**Governance Committees:**
- AI Steering Committee (strategic direction)
- AI Ethics Board (principle conflicts, ethical escalations)
- Model Risk Committee (technical review, validation)
- Data Governance Council (data quality, lineage, privacy)

**Graphic:** Org chart showing roles and reporting relationships

**SPEAKER NOTES:**

"Let's talk about key roles in AI governance.

Chief AI Officer is your executive sponsor. Not every organization needs a dedicated CAO - smaller organizations might assign this to CTO or CDO. But someone at executive level must own AI strategy and governance. This person sets direction, allocates resources, and represents AI in C-suite discussions.

AI Ethics Officer handles responsible AI principles. When you have a conflict between transparency and IP protection, who decides? When an AI system produces unexpectedly biased outputs, who investigates? The Ethics Officer. This role is critical for high-risk AI.

AI Risk Manager implements your risk framework - NIST AI RMF or ISO 42001. They conduct risk assessments, track the risk register, ensure monitoring. Regulated industries need this role.

AI Governance Lead is your operational implementer. They write policies, manage approval workflows, track compliance, coordinate across business units. Every organization with AI needs this function, even if it's part-time initially.

Model Validators provide independent review. In financial services, this is often required by regulators - someone not involved in model development must validate it. For high-risk AI, independent validation is a best practice even if not legally required.

[Point to committees]

Governance committees provide oversight and decision-making forums. Steering committee for strategy, Ethics Board for principle conflicts, Risk Committee for technical reviews, Data Council for data-related decisions.

In your capstone, you'll define which roles and committees your organization needs."

[Transition]

**BACKGROUND:**

**Rationale:**
- This slide operationalizes governance structure with specific roles and accountabilities
- Addresses "who does what" question that determines governance effectiveness
- Provides RACI-style clarity for governance responsibilities

**Key Research & Citations:**
- Role definitions synthesized from ISO 42001:2023 AI management system standard
- Financial services model validator role based on SR 11-7 and OCC guidance requirements
- Chief AI Officer emergence documented in Fortune 500 trends (22% had CAO role in 2023, up from 3% in 2020)
- Committee structures based on corporate governance best practices adapted for AI-specific decisions

**Q&A Preparation:**
- *"Can one person hold multiple roles?"*: Yes in smaller organizations - common to combine AI Governance Lead + AI Ethics Officer, but maintain separation between Model Developers and Model Validators
- *"Do we need all these roles immediately?"*: No - start with AI Governance Lead, add roles based on AI maturity and risk profile
- *"How do these roles integrate with existing governance?"*: AI governance roles typically report through existing risk/compliance/technology governance structures, not separate reporting lines

---

### SLIDE 10: ORGANIZATIONAL STRUCTURE SUMMARY

**Title:** Designing Your Governance Organization

**Content:**

**Key Takeaway:** No one-size-fits-all structure - design based on your context

**Decision Factors:**
- Organization size and complexity
- AI maturity level
- Regulatory environment
- Risk appetite
- Organizational culture (centralized vs. distributed decision-making)
- Resource availability

**Recommended Approach:**
1. Start lean - don't build full structure before you need it
2. Begin with AI Governance Lead + Ethics Board
3. Add Chief AI Officer as AI becomes strategic
4. Add Risk Manager and Model Validators for high-risk or regulated AI
5. Evolve from centralized control to federated model as maturity increases

**You'll Practice:**
Exercise 2.2 and Capstone require organizational structure recommendations

**Graphic:** Maturity progression showing governance structure evolution

**SPEAKER NOTES:**

"There's no one-size-fits-all governance structure. Your design depends on context.

Small organization, early AI adoption? Start with an AI Governance Lead who can be part-time, maybe someone from IT or data analytics. Add an Ethics Board for escalations - doesn't need to be full-time roles, can meet quarterly.

Medium organization, growing AI maturity? Add a Chief AI Officer to provide executive leadership. If you're in regulated industry, add AI Risk Manager.

Large organization, mature AI, high-risk deployments? Full structure - CAO, Ethics Officer, Risk Manager, Governance Lead, Model Validators, plus committees.

[Point to recommended approach]

The mistake is building too much structure too soon. You don't need a Chief AI Officer when you have two AI pilots. But you also can't scale without structure - if you have 50 AI systems and no governance lead, you're in trouble.

Start lean, evolve as you grow.

Your capstone requires a structure recommendation with justification. Why is this the right model for your organization?"

[Transition: Segment 3]

---

## SEGMENT 3: AGENT AUTONOMY CLASSIFICATION
### Duration: 12 minutes | Slides 11-14

---

### SLIDE 11: THE AUTONOMY SPECTRUM

**Title:** Five Levels of Agent Autonomy

**Content:**

| Level | Name | Description | Human Oversight | Example |
|-------|------|-------------|-----------------|---------|
| **0** | Tool | Human initiates every action, AI assists | Full human control | Grammar checker |
| **1** | Assistant | AI suggests, human decides | Human approval for each action | Email draft suggestions |
| **2** | Supervised | AI acts, human monitors | Human can review and veto | Customer service bot with live agent monitoring |
| **3** | Managed | AI operates, human oversight | Exception-based human review | Fraud detection with human review of flagged cases |
| **4** | Autonomous | AI operates independently | Periodic audit only | Automated trading within preset limits |

**Key Principle:** Autonomy level should match risk level and reversibility

**Graphic:** Spectrum showing autonomy levels from 0-4 with human oversight decreasing

**GRAPHICS:**

**Graphic 1: Autonomy Spectrum with Human Oversight Gradient**
- Purpose: Visualize the five autonomy levels and inverse relationship with human control
- Type: Horizontal spectrum diagram with gradient
- Elements: Five levels arranged left to right (0 to 4); human icon decreasing in size from left to right; AI/robot icon increasing in size from left to right; oversight intensity shown as gradient bar below (dark green to light green)
- Labels: Level number and name prominently displayed for each; brief description under each level; human oversight type indicated; example use case in callout box
- Relationships: Inverse relationship between autonomy and human control shown visually; transition points between levels marked; risk indicators showing which levels require governance gates

**Graphic 2: Autonomy Level Decision Matrix**
- Purpose: Help organizations classify their AI agents into appropriate autonomy levels
- Type: Decision matrix with two axes
- Elements: Vertical axis showing "Impact/Risk" (Low to High); horizontal axis showing "Reversibility" (Easy to Difficult); five zones plotted corresponding to autonomy levels; example agents plotted in appropriate zones
- Labels: Axes clearly labeled; zones color-coded by autonomy level; examples with brief descriptions; decision criteria noted in corners
- Relationships: Higher risk + lower reversibility → lower autonomy level; diagonal pattern showing recommended classifications; arrows showing how agent capabilities might progress over time

**Graphic 3: Human Oversight Models Comparison**
- Purpose: Detail the three main human oversight patterns and when to use each
- Type: Three-column comparison diagram
- Elements: Three columns (Human-in-the-loop, Human-on-the-loop, Human-in-command); each showing timing of human involvement; workflow diagrams for each pattern
- Labels: Pattern names; timing of intervention (Before/During/After action); typical latency impact; appropriate use cases; compliance considerations
- Relationships: Show how oversight timing affects system behavior; map oversight patterns to autonomy levels; indicate which pattern satisfies various regulatory requirements

**SPEAKER NOTES:**

"Not all AI agents should operate at the same autonomy level. We need a classification system.

I'm going to walk you through five autonomy levels, 0 through 4.

Level 0 - Tool: Human initiates every action. AI assists but doesn't act independently. Example: grammar checker that suggests corrections but you choose whether to apply them.

Level 1 - Assistant: AI suggests actions, human approves before execution. Email suggests a draft reply, you review and send. Human approval for each action.

Level 2 - Supervised: AI takes actions, but human monitors in real-time with ability to intervene. Customer service chatbot handles routine queries, live agent is monitoring and can step in.

Level 3 - Managed: AI operates with exception-based human review. Fraud detection AI flags suspicious transactions, human reviews the flagged cases but most transactions process automatically.

Level 4 - Autonomous: AI operates independently within boundaries, human audit is periodic not real-time. Automated trading system executes trades within preset risk limits, humans review performance weekly.

[Pause]

The key principle: autonomy should match risk and reversibility. High impact + irreversible decisions = low autonomy. Low impact + easily reversible = higher autonomy acceptable.

We're going to see how to classify agents systematically."

[Transition]

---

### SLIDE 12: AUTONOMY DECISION CRITERIA

**Title:** How to Classify Agent Autonomy

**Content:**

**Decision Matrix:**
```
┌─────────────────────────────────────────────────────────┐
│           AUTONOMY LEVEL DETERMINATION                  │
│                                                         │
│  Impact │ Reversible │ Compliance │ → Autonomy Level   │
│  ───────┼────────────┼────────────┼──────────────────  │
│  Low    │ Yes        │ Low        │ → Level 3-4       │
│  Low    │ Yes        │ High       │ → Level 2         │
│  Low    │ No         │ Any        │ → Level 1-2       │
│  Med    │ Yes        │ Low        │ → Level 2-3       │
│  Med    │ Yes        │ High       │ → Level 1-2       │
│  Med    │ No         │ Any        │ → Level 1         │
│  High   │ Yes        │ Any        │ → Level 1-2       │
│  High   │ No         │ Any        │ → Level 0-1       │
└─────────────────────────────────────────────────────────┘
```

**Three Factors:**
1. **Impact:** What's the consequence if AI makes a mistake?
2. **Reversibility:** Can the decision be easily undone?
3. **Compliance Context:** Is this a regulated use case requiring human oversight?

**Examples:**
- Low impact + reversible + low compliance = Level 3-4 (recommendation engines)
- High impact + not reversible + any compliance = Level 0-1 (medical diagnosis)

**Graphic:** Decision tree or matrix showing how factors combine

**SPEAKER NOTES:**

"Here's how to classify autonomy systematically using three factors.

Impact: What happens if the AI makes a mistake? An FAQ chatbot giving wrong information is low impact - mildly annoying. A loan approval AI denying qualified applicants is high impact - financial harm, potential discrimination.

Reversibility: Can you undo the decision easily? An email draft suggestion is reversible - just don't send it. An insurance coverage denial is hard to reverse - now you have to appeal, which takes time and may leave someone uninsured.

Compliance context: Is this use case subject to regulations requiring human oversight? Employment decisions under NYC Local Law 144 need human review. Medical device AI under FDA requires human supervision.

[Point to matrix]

These factors combine to determine autonomy level. Low impact + reversible + low compliance risk = you can allow Level 3 or 4 autonomy. A recommendation engine for online shopping fits here.

High impact + not reversible + regulated = Level 0 or 1 only. Medical diagnosis AI fits here - high impact if wrong, diagnosis isn't easily reversed, heavily regulated. You need human approval.

[Pause]

This isn't just about what's technically possible. AI might be capable of fully autonomous operation, but governance says 'not without human oversight' for high-risk scenarios.

Let me show you real examples."

[Transition]

---

### SLIDE 13: AUTONOMY BOUNDARIES AND TRIGGERS

**Title:** Defining Operational Boundaries

**Content:**

**Autonomy Boundaries by Level:**

**Level 2 (Supervised):**
- Financial: Decisions under $100
- Stakeholder: Internal users only
- Data: Non-PII data only
- Regulatory: Non-regulated use cases

**Level 3 (Managed):**
- Financial: Decisions under $1,000
- Stakeholder: Customers, non-high-risk scenarios
- Data: PII allowed with controls
- Regulatory: Limited risk tier

**Level 4 (Autonomous):**
- Financial: Decisions under $10,000
- Stakeholder: Customers, routine scenarios
- Data: Comprehensive data with audit
- Regulatory: Minimal risk tier only

**Escalation Triggers (Force human review regardless of level):**
- AI confidence score below threshold (e.g., <0.7)
- Novel situation (not seen in training data)
- Error rate spike (exceeds baseline by >20%)
- Explicit user request for human review
- High-value or high-sensitivity scenario

**Graphic:** Boundaries as gates with escalation paths

**SPEAKER NOTES:**

"Once you've classified autonomy level, you need to define operational boundaries.

Boundaries limit what the agent can do autonomously. For Level 2 (supervised), you might allow decisions only under $100, only for internal users, only with non-PII data. Above those thresholds, human approval required.

Level 3 (managed) gets higher boundaries - maybe $1,000 financial decisions, customer-facing scenarios allowed, PII permitted with privacy controls.

Level 4 (autonomous) has highest boundaries - up to $10,000, broad customer scenarios, comprehensive data access with comprehensive audit.

[Point to escalation triggers]

But even within autonomy level, you need escalation triggers that force human review.

AI confidence score: If the AI isn't confident in its decision, it escalates. You might set threshold at 0.7 - below that, human reviews.

Novel situation: If the AI encounters a scenario significantly different from training data, it flags for human judgment.

Error rate spike: If performance suddenly degrades, automatic escalation and review.

User request: Users can always request human review.

High-value scenarios: Even if generally autonomous, specific high-stakes cases get human review.

[Pause]

These boundaries and triggers prevent autonomous agents from operating beyond appropriate limits. Your capstone will require defining these for your agent inventory."

[Transition]

---

### SLIDE 14: AUTONOMY CLASSIFICATION SUMMARY

**Title:** Key Autonomy Principles

**Content:**

**Remember:**
1. **Match autonomy to risk** - High impact + irreversible = low autonomy
2. **Document classification rationale** - Why is this agent Level 2 not Level 3?
3. **Define clear boundaries** - Financial limits, data access, stakeholder scope
4. **Build escalation triggers** - Confidence thresholds, novel situations, error spikes
5. **Review classifications periodically** - Agent capabilities and contexts change

**You'll Practice:**
Exercise 2.2 and Capstone include autonomy classification systems

**Common Mistake:**
Setting autonomy level based on technical capability rather than risk and governance requirements

**Graphic:** Autonomy classification process flow

**SPEAKER NOTES:**

"Before moving on, five autonomy principles:

One, match autonomy to risk, not capability. Your AI might be capable of fully autonomous operation, but if the risk is high, governance requires human oversight.

Two, document your classification rationale. Auditors and regulators will ask 'why is this Level 3?' You need to show your decision factors - impact, reversibility, compliance context.

Three, define clear boundaries. 'Level 3 managed autonomy' is too vague. 'Level 3 with $1,000 financial limit, customer-facing allowed, PII with privacy controls' is specific and enforceable.

Four, build escalation triggers. Don't rely solely on autonomy level. Even autonomous agents should escalate when they encounter situations outside normal parameters.

Five, review periodically. Agent capabilities improve. Business contexts change. Regulations evolve. What was Level 1 last year might be ready for Level 2 now. Or vice versa - new regulation might require reducing autonomy.

[Pause]

The common mistake is classifying based on what the AI can do rather than what it should be allowed to do. Technical capability doesn't determine autonomy - risk and governance do.

Now let's talk about how you ensure accountability through audit trails."

[Transition: Segment 4]

---

## SEGMENT 4: AUDIT TRAILS AND ACCOUNTABILITY
### Duration: 11 minutes | Slides 15-17

---

### SLIDE 15: AUDIT TRAIL REQUIREMENTS

**Title:** What Must Be Logged

**Content:**

**Comprehensive Audit Record:**
```json
{
  "audit_record": {
    "timestamp": "ISO-8601",
    "agent_id": "unique-identifier",
    "action_type": "decision|recommendation|automation",
    "input_summary": "what triggered the action",
    "output_summary": "what the agent did/said",
    "reasoning_trace": "explanation of decision",
    "data_accessed": ["list", "of", "sources"],
    "human_oversight": {
      "type": "approval|review|none",
      "approver": "user-id or null",
      "timestamp": "ISO-8601 or null"
    },
    "confidence_score": 0.0-1.0,
    "model_version": "version-identifier",
    "outcome": "success|failure|pending"
  }
}
```

**Why Each Field Matters:**
- **Timestamp:** When did this happen? (Regulatory requirement, audit requirement)
- **Agent ID:** Which AI made this decision? (Accountability, debugging)
- **Action type:** What category? (Classification for review)
- **Input/Output:** What happened? (Reconstruct decision)
- **Reasoning trace:** Why? (Explainability, audit)
- **Data accessed:** What data? (Privacy compliance, lineage)
- **Human oversight:** Was human involved? (Compliance, accountability)
- **Confidence:** How certain was AI? (Quality control, escalation)
- **Model version:** Which version? (Versioning, rollback)
- **Outcome:** Result? (Performance tracking)

**Graphic:** Audit record structure with arrows showing why each field matters

**SPEAKER NOTES:**

"Let's talk about audit trails - what must be logged for each AI decision.

This is a comprehensive audit record structure. Let me walk through why each field matters.

Timestamp: When did this decision happen? Regulatory requirements often specify log retention periods calculated from timestamp. Audit trails need chronological ordering.

Agent ID: Which specific AI agent made this decision? If you have multiple versions or multiple agents, you need to know which one.

Action type: Was this a decision, a recommendation, or automation? This helps classify for review and analysis.

Input and output summaries: What triggered the action, what did the agent do? This lets you reconstruct the decision for audit or dispute resolution.

Reasoning trace: This is your explainability requirement. Why did the AI make this decision? What factors did it consider? Regulators and users may demand explanations.

Data accessed: What data sources did the AI use? Privacy regulations require this. Data lineage tracking depends on it.

Human oversight: Was a human involved? If so, who approved or reviewed, and when? This is critical for compliance with regulations requiring human oversight.

Confidence score: How confident was the AI? This enables quality control and helps identify decisions that should have escalated.

Model version: Which version of the AI made this decision? Essential for versioning control and rollback if needed.

Outcome: Success or failure? Performance tracking depends on capturing outcomes.

[Pause]

This level of logging might seem excessive. But when a regulator asks 'explain this decision' or a user disputes an outcome, you need comprehensive audit trails. Missing any of these fields creates gaps."

[Transition]

---

### SLIDE 16: RETENTION AND ACCESS

**Title:** Managing Audit Trails

**Content:**

**Retention Requirements:**

| Driver | Typical Requirement | Example |
|--------|-------------------|---------|
| **Regulatory** | 5-7 years | Financial services: SR 11-7 requires 7 years |
| **Legal** | Statute of limitations + buffer | Employment decisions: 5 years in many states |
| **Operational** | Duration of AI deployment + 1 year | While agent operates + retention after retirement |
| **Litigation hold** | Indefinite until released | If litigation is pending or anticipated |

**Access Control:**
- **Who can request:** Compliance officers, auditors, legal, data subject (GDPR right of access)
- **Response time:** Internal audit (48 hours), regulatory audit (24 hours), data subject request (30 days under GDPR)
- **Format:** Structured data export, human-readable report, both
- **Chain of custody:** Audit trail access must itself be logged

**Storage Considerations:**
- Volume: High-frequency agents generate massive logs
- Cost: Long-term storage for compliance
- Security: Audit trails often contain PII - protect appropriately
- Immutability: Logs must be tamper-evident

**Graphic:** Retention timeline and access control workflow

**SPEAKER NOTES:**

"Logging is only half the challenge. You also need retention and access controls.

Retention requirements come from multiple sources. Regulatory requirements vary by industry - financial services often requires 7 years under SR 11-7. Legal requirements depend on statutes of limitations - employment decisions might need 5-year retention because that's how long discrimination claims can be filed.

Operationally, you need audit trails for the entire time an AI operates, plus a buffer after retirement in case issues arise.

And if litigation is pending or anticipated, you may need to preserve logs indefinitely until legal hold is released.

[Point to access control]

Access control is critical. Compliance officers and auditors need access for reviews. Legal needs access for litigation. Regulators can demand access. And under GDPR, data subjects can request information about automated decisions affecting them.

Response times matter. Regulators expect fast turnaround - 24 hours isn't unusual. Internal audits might allow 48 hours. Data subject requests have legal deadlines - 30 days under GDPR.

Format: Sometimes you need structured data exports for analysis, sometimes human-readable reports for explanation, often both.

Chain of custody: Access to audit trails must itself be logged. Who viewed what logs when? This prevents tampering and provides accountability.

[Point to storage considerations]

Storage is a real challenge. A high-frequency trading AI or customer service bot generates millions of log records. Long-term retention for compliance is expensive. Logs often contain PII, so you need security controls. And logs must be immutable - you can't allow modification after creation.

Your capstone should address how audit trails will be stored, retained, and accessed."

[Transition]

---

### SLIDE 17: ACCOUNTABILITY MAPPING

**Title:** From Audit Trail to Accountability

**Content:**

**Accountability Chain:**
```
Agent Action → Audit Log → Owner Identified → Accountability Assigned
                                  │
                   ┌──────────────┼──────────────┐
                   │              │              │
                Operator      Developer      Executive
               (immediate)   (system)       (policy)
```

**Three Levels of Accountability:**

**Operator Level (Immediate):**
- Person who deployed the agent or approved specific action
- Accountable for: Appropriate use, oversight execution, escalation when needed
- Example: Customer service manager who deployed chatbot

**Developer Level (System):**
- Team who built/configured the agent
- Accountable for: System design, testing adequacy, performance monitoring
- Example: Data science team that built recommendation engine

**Executive Level (Policy):**
- Leadership who approved agent deployment and set governance policies
- Accountable for: Strategic decisions, risk acceptance, resource allocation
- Example: Chief AI Officer who approved high-risk AI deployment

**RACI Matrix for AI Decisions:**
- **Responsible:** Who does the work (often the AI, with human oversight)
- **Accountable:** Who owns the outcome (must be a human)
- **Consulted:** Who provides input (subject matter experts, legal, ethics)
- **Informed:** Who needs to know (stakeholders, executives)

**Graphic:** Accountability pyramid showing three levels

**SPEAKER NOTES:**

"Audit trails enable accountability. But accountability isn't automatic - you need to map it explicitly.

Think of accountability at three levels.

Operator level: The person who deployed the AI or approved a specific action. If a customer service manager deploys a chatbot, they're accountable for appropriate use, oversight, and escalation. When something goes wrong operationally, accountability starts here.

Developer level: The team who built or configured the agent. Data scientists, ML engineers. They're accountable for system design, adequate testing, and performance monitoring. If the AI has a systematic flaw - bias in outputs, security vulnerability - accountability is at developer level.

Executive level: Leadership who approved deployment and set governance policies. The Chief AI Officer, Risk Committee, Board. They're accountable for strategic decisions, risk acceptance, resource allocation. If governance fails systematically - wrong risk classification, inadequate oversight - accountability is executive.

[Point to RACI]

RACI matrices clarify this. For any AI decision:
- Responsible: Who did the work? Often the AI, with human in oversight role
- Accountable: Who owns the outcome? Must be a human with authority
- Consulted: Who provided input? Legal on compliance, ethics on principles
- Informed: Who needs to know? Executives, affected stakeholders

[Pause]

The critical point: Accountable must always be a human. You can't make an AI accountable. You can hold the person who deployed it, the team who built it, or the executives who approved it accountable.

Your audit trails must enable tracing from agent action to accountable human. Exercise 2.2 includes accountability mapping."

[Transition: Closing]

---

## CLOSING SECTION
### Duration: 3 minutes | Slides 18-20

---

### SLIDE 18: CAPSTONE OVERVIEW

**Title:** Module Capstone: Enterprise AI Governance Framework

**Content:**

**What You'll Create:**
A complete, client-ready enterprise AI governance framework integrating:

**Part 1: Assessment** (from Session 1)
- Governance maturity scores
- Strengths and gaps analysis

**Part 2: Framework Design**
- Organizational structure (centralized/federated)
- Key roles and committees
- Policy framework (from Exercise 2.2)
- Risk framework (NIST/ISO)

**Part 3: Agent Autonomy Classification**
- Five-level classification system
- Current agent inventory with classifications
- Boundaries and escalation triggers

**Part 4: Compliance Roadmap**
- Regulatory requirements mapped (from Exercise 2.1)
- Compliance actions required
- Remediation priorities

**Part 5: Implementation Roadmap**
- Phase 1-3 timelines (12 months)
- Quick wins (30 days)
- Success metrics and KPIs

**Deliverable:** `enterprise-ai-governance-framework.md` (Exercise 2.3)

**Graphic:** Framework structure showing how all components integrate

**SPEAKER NOTES:**

"Your capstone brings everything together.

You're creating a complete Enterprise AI Governance Framework - not a theoretical exercise, but a client-ready deliverable.

Part 1 uses your Session 1 maturity assessment. You're documenting current state - where the organization is today across seven governance domains.

Part 2 is framework design. You'll recommend organizational structure - centralized, federated, or hybrid - with justification. You'll define key roles and committees. You'll include the policies from Exercise 2.2. And you'll specify which risk framework to adopt - NIST AI RMF, ISO 42001, or custom.

Part 3 is agent autonomy classification. You'll document your five-level system and classify actual agents in the organization's inventory.

Part 4 is compliance roadmap from Exercise 2.1. Which regulations apply, what compliance actions are required, what are priorities.

Part 5 is implementation roadmap. How do you move from current state to target state? Phase 1: Foundation (months 1-3), Phase 2: Build (4-6), Phase 3: Operate (7-12). What are quick wins achievable in 30 days? What metrics will you track?

This is what you'd present to an executive team or board. It's comprehensive, actionable, and implementable.

Estimated time: 25 minutes using your prior deliverables. Template provided."

[Transition]

---

### SLIDE 19: RESOURCES

**Title:** Resources for This Week

**Content:**

**Templates & Files:**
- Regulatory Compliance Mapping Template (Exercise 2.1)
- AI Governance Policy Suite Template (Exercise 2.2)
- Enterprise AI Governance Framework Template (Exercise 2.3 Capstone)

**Reference Materials:**
- [EU AI Act Official Text](https://artificialintelligenceact.eu/)
- [SR 11-7 Model Risk Management](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)
- [NYC Local Law 144](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page)
- [FDA AI/ML Software as Medical Device](https://www.fda.gov/medical-devices/software-medical-device-samd)

**Support:**
- Questions: [Async channel]
- Exercise review: Post completed capstone for peer feedback

**Graphic:** Resource icons with links

**SPEAKER NOTES:**

"You have comprehensive resources for this week.

Each exercise includes a complete template. Exercise 2.1 regulatory compliance mapping has placeholders for EU AI Act, sector-specific regulations, and data protection requirements. Exercise 2.2 policy suite includes three starter policies. Exercise 2.3 capstone template has all sections outlined.

Reference materials link to actual regulations. EU AI Act official text, SR 11-7 model risk management guidance from Federal Reserve, NYC Local Law 144 for employment AI, FDA guidance on AI as medical device.

If you get stuck, post in [async channel]. I check daily.

For the capstone specifically, consider posting your draft for peer feedback. Your cohort colleagues can provide valuable perspectives."

[Transition]

---

### SLIDE 20: CONCLUSION

**Title:** From Frameworks to Implementation

**Content:**

**What You've Learned:**
- **Session 1:** Governance frameworks, principles, lifecycle, data governance
- **Session 2:** Compliance requirements, organizational structures, autonomy classification, audit trails

**What You've Created:**
- Governance maturity assessment
- Responsible AI principles mapping
- Agent risk register
- Regulatory compliance mapping
- Governance policy suite
- **Enterprise AI Governance Framework (Capstone)**

**Next Steps:**
- Complete Exercise 2.1-2.3 (75 minutes total)
- Review your capstone framework for completeness
- Consider how to apply this in your organization or client work

**Final Message:**
Enterprise AI governance isn't optional anymore. With regulations like EU AI Act, sector requirements, and growing stakeholder expectations, organizations need formal governance frameworks. You now have the knowledge and templates to build them.

**Graphic:** Journey visual showing progression from frameworks to implementation to governance maturity

**SPEAKER NOTES:**

"Let's close.

Over two sessions, you've built comprehensive AI governance capability.

Session 1 gave you the foundations - NIST and ISO frameworks, responsible AI principles, lifecycle governance, data governance. You created maturity assessments, principles mappings, and risk registers.

Session 2 gave you implementation capability - regulatory compliance mapping, organizational design, autonomy classification, audit trail requirements. And you're creating the capstone that brings it all together.

[Pause]

Here's what's changed: Two years ago, AI governance was optional. It was good practice, risk mitigation, reputation protection. Today, it's mandatory in many contexts. EU AI Act creates legal requirements. Financial services regulators demand it. Procurement RFPs require it. Stakeholders expect it.

Organizations need enterprise AI governance frameworks. Not someday - now.

You have the knowledge to build them. You have the templates. You have example deliverables from your exercises.

[Final close]

The exercises this week total 75 minutes. Exercise 2.1 maps compliance requirements. Exercise 2.2 drafts core policies. Exercise 2.3 is your capstone - the complete framework.

This has been an intensive module. You've learned a lot. More importantly, you've created practical deliverables you can use immediately.

Thank you for your engagement. Good luck with the capstone. I look forward to seeing your governance frameworks."

---

## Appendix A: Slide Type Definitions (Condensed)

**CONCEPT**: Introduces new idea or framework - focus on clarity and single concept
**DEMO**: Live demonstration or walkthrough - have backup plan if tech fails
**INSIGHT**: Delivers key learning or aha moment - emphasize and pause after
**TRANSITION**: Bridges sections - keep brief, preview what's coming
**SUMMARY**: Reinforces key points - use repetition intentionally

## Appendix B: Visual Design Guidelines

**Color Palette - Advanced Green Theme:**
- Primary: Advanced Green #00CC99
- Secondary: Deep Blue #003D5C
- Accent: Bright Orange #FF6B35
- Neutral: Cool Gray #708090
- Warning/Alert: Amber #FFA500

**Typography:**
- Headers: Bold, size 32-44pt
- Body: Regular, size 18-24pt
- Code/Technical: Monospace, size 16-20pt
- Ensure sufficient contrast (WCAG AA minimum)

**Graphic Standards:**
- Every slide with technical content needs a supporting graphic
- Graphics must be referenced in speaker notes
- Use consistent icon set throughout presentation
- Label all diagram elements clearly
- Show relationships with arrows/connectors

**Layout Principles:**
- Maximum 3 main points per slide
- White space is valuable - don't overcrowd
- Align elements to grid
- Consistent margins across all slides

## Appendix C: Quality Checklist

**Content Quality:**
- [ ] All learning objectives explicitly addressed in slides
- [ ] Each segment has clear opening, body, and summary
- [ ] Technical accuracy verified (commands, code, architecture patterns)
- [ ] Examples are realistic and relevant to target audience
- [ ] Terminology consistent with Block 3 and prior modules

**Speaker Notes Quality:**
- [ ] Every content slide has speaker notes
- [ ] Notes include delivery cues ([Pause], [Emphasize], [Transition])
- [ ] Approximate timing aligns with segment durations
- [ ] Questions and transitions scripted
- [ ] Backup explanations prepared for complex topics

**Technical Quality:**
- [ ] All code examples are syntactically correct
- [ ] Architecture diagrams are technically sound
- [ ] Commands have been tested
- [ ] Links and references are valid
- [ ] Version numbers and dates are current

---

---

## Appendix D: EU AI Act Compliance Checklist

**Purpose:** Detailed compliance requirements for high-risk AI systems under EU AI Act

**Article 9: Risk Management System**
- [ ] Risk management system established (documented process)
- [ ] Risks identified across AI lifecycle (design, data, operation)
- [ ] Risk mitigation measures implemented
- [ ] Residual risks evaluated and documented
- [ ] Risk management updated throughout lifecycle

**Article 10: Data and Data Governance**
- [ ] Training, validation, testing datasets documented
- [ ] Data quality criteria defined and met
- [ ] Data examination for biases conducted
- [ ] Gaps or shortcomings identified and addressed
- [ ] Data governance practices implemented

**Article 11: Technical Documentation**
- [ ] General description of AI system
- [ ] Detailed description of development elements
- [ ] Monitoring, functioning, control information
- [ ] Validation and testing procedures
- [ ] Conformity assessment information

**Article 12: Record-Keeping**
- [ ] Automatic logging capability enabled
- [ ] Logs capture: inputs, outputs, timestamps
- [ ] Log retention period appropriate to risk
- [ ] Logs protected from tampering
- [ ] Logs accessible for audits

**Article 13: Transparency and User Information**
- [ ] Users informed AI system is being used
- [ ] Information provided on capabilities and limitations
- [ ] Instructions for use provided
- [ ] Human oversight information clear
- [ ] Contact point for queries established

**Article 14: Human Oversight**
- [ ] Human oversight measures determined and built-in
- [ ] Assigned individuals have authority and competence
- [ ] Can intervene or interrupt system
- [ ] Can disregard, override, or reverse output
- [ ] Monitoring includes logs and outputs

**Article 15: Accuracy, Robustness, Cybersecurity**
- [ ] Accuracy level declared and validated
- [ ] Robustness tested (edge cases, errors)
- [ ] Resilience against attacks verified
- [ ] Technical and organizational measures implemented
- [ ] System maintained and updated

---

## Appendix E: Autonomy Classification Decision Matrix

**Purpose:** Systematic framework for classifying agent autonomy levels

**Classification Decision Tree:**

```
START: Classify Agent Autonomy Level

Q1: What is the IMPACT if the AI makes an error?
├─ Minimal (FAQ answer wrong) → Continue to Q2
├─ Moderate (Inconvenience, rework needed) → Continue to Q2
├─ Major (Financial loss, harm to person) → MAX Level 2 (Supervised)
└─ Severe (Serious harm, major financial loss, legal liability) → MAX Level 1 (Assistant)

Q2: Is the decision REVERSIBLE?
├─ Easily reversed (can undo immediately) → Continue to Q3
├─ Reversible with effort (requires process) → Reduce autonomy 1 level
└─ Irreversible or very difficult to reverse → MAX Level 1 (Assistant)

Q3: What is the REGULATORY context?
├─ Highly regulated (employment, lending, healthcare) → MAX Level 2 (Supervised)
├─ Some regulation (customer service, marketing) → Continue to Q4
└─ Minimal regulation (internal tools) → Continue to Q4

Q4: What is the SCOPE of impact?
├─ Internal users only → Continue to Q5
├─ External customers (low volume) → Continue to Q5
└─ External customers (high volume) → Reduce autonomy 1 level

Q5: What is AI RELIABILITY?
├─ Highly reliable (>99% accuracy, well-tested) → SUGGESTED autonomy level
├─ Moderate reliability (95-99% accuracy) → Reduce 1 level
└─ Lower reliability (<95% accuracy) → MAX Level 1 (Assistant)

RESULT: Recommended Autonomy Level (0-4)
```

**Autonomy Level Definitions with Examples:**

**Level 0: Tool**
- **Description:** Human initiates every action, AI provides information
- **Examples:** Grammar checker, code suggestions, research assistant
- **Human Oversight:** Human reviews and decides on every suggestion
- **Appropriate For:** Any scenario where human judgment essential

**Level 1: Assistant**
- **Description:** AI suggests actions, human approves before execution
- **Examples:** Email draft generator, appointment scheduler, document summarizer
- **Human Oversight:** Human approves each action before it executes
- **Appropriate For:** Medium-high impact decisions, regulated contexts

**Level 2: Supervised**
- **Description:** AI acts, human monitors with ability to intervene
- **Examples:** Customer service chatbot with live agent monitoring, content moderation with human review queue
- **Human Oversight:** Human monitors in real-time, can step in anytime
- **Appropriate For:** Moderate impact, reversible decisions, some regulatory requirements

**Level 3: Managed**
- **Description:** AI operates independently, human reviews exceptions and samples
- **Examples:** Fraud detection (flags suspicious transactions), recommendation engines, automated pricing within bounds
- **Human Oversight:** Exception-based review, periodic sampling audits
- **Appropriate For:** Low-moderate impact, well-tested systems, minimal regulation

**Level 4: Autonomous**
- **Description:** AI operates independently within defined boundaries, periodic human audit only
- **Examples:** Automated trading (within risk limits), infrastructure auto-scaling, spam filtering
- **Human Oversight:** Periodic audits (weekly/monthly), alert-based review
- **Appropriate For:** Low impact, highly reliable systems, minimal regulation

---

## Appendix F: Organizational Structure Templates

**Purpose:** Ready-to-use org structure models for different organization sizes

**Template 1: Small Organization (<500 employees)**

```
┌───────────────────────────────┐
│  Chief Technology Officer     │
│  (AI Governance Oversight)    │
└───────────┬───────────────────┘
            │
    ┌───────┴────────┐
    │                │
┌───▼────────────┐   │
│ AI Governance  │   │
│ Lead           │   │
│ (Part-time,    │   │
│  15-20 hrs/wk) │   │
└────────────────┘   │
                     │
            ┌────────▼────────┐
            │  Ethics Board   │
            │  (Quarterly)    │
            │  - CTO          │
            │  - Legal        │
            │  - Privacy Off. │
            │  - Business Lead│
            └─────────────────┘
```

**Roles:**
- **CTO:** Executive sponsor, final decision authority, resources
- **AI Governance Lead:** Policy implementation, compliance tracking, risk register maintenance (can be existing role like Data Governance or Compliance Lead with added AI responsibilities)
- **Ethics Board:** Quarterly meetings to review high-risk AI, resolve principle conflicts, approve policies

**Budget:** $50K-$100K/year (primarily AI Governance Lead time, governance tools)

---

**Template 2: Medium Organization (500-5000 employees)**

```
┌────────────────────────────────┐
│  Chief AI Officer              │
│  (Executive Leadership)        │
└────────────┬───────────────────┘
             │
      ┌──────┴────────┐
      │               │
┌─────▼────────┐  ┌──▼────────────────┐
│AI Governance │  │ AI Ethics Officer │
│Lead          │  │ (Part-time)       │
│(Full-time)   │  │                   │
└──────────────┘  └───────────────────┘
      │
      │
┌─────▼──────────────────────────┐
│  AI Governance Committees:     │
│  - AI Steering (Monthly)       │
│  - Ethics Board (Quarterly)    │
│  - Model Risk (As-needed)      │
└────────────────────────────────┘
```

**Roles:**
- **Chief AI Officer:** Strategic direction, budget, executive advocacy, reports to CEO
- **AI Governance Lead:** Full-time role, policy development and enforcement, training, compliance
- **AI Ethics Officer:** Part-time (10-15 hrs/wk), ethics reviews, escalation resolution, principle conflicts (can be existing role like Chief Privacy Officer with added responsibilities)
- **Committees:** Steering for strategy, Ethics Board for escalations, Model Risk for technical validation

**Budget:** $250K-$500K/year (CAO, Governance Lead, tools, training)

---

**Template 3: Large Organization (>5000 employees)**

```
┌─────────────────────────────────────────┐
│          Chief AI Officer               │
│       (C-Suite Executive)               │
└────────────────┬────────────────────────┘
                 │
         ┌───────┴────────┬────────────────┐
         │                │                │
┌────────▼─────────┐  ┌──▼──────────┐  ┌──▼────────────┐
│ AI Risk Manager  │  │ AI Ethics   │  │ AI Governance │
│ (Full-time)      │  │ Officer     │  │ Lead          │
│                  │  │ (Full-time) │  │ (Full-time)   │
└──────────────────┘  └─────────────┘  └───────────────┘
         │                    │                │
         │                    │                │
┌────────▼────────────────────▼────────────────▼───────┐
│           AI Governance Committees                    │
│  - AI Steering Committee (Monthly)                    │
│  - AI Ethics Board (Monthly)                          │
│  - Model Risk Committee (Bi-weekly)                   │
│  - Data Governance Council (Monthly)                  │
│  - AI Security Working Group (Monthly)                │
└───────────────────────────────────────────────────────┘
         │
┌────────▼──────────────────────────────────┐
│  Business Unit AI Governance Leads        │
│  (Federated Model)                        │
│  - BU 1 Governance Lead                   │
│  - BU 2 Governance Lead                   │
│  - BU 3 Governance Lead                   │
└───────────────────────────────────────────┘
```

**Roles:**
- **Chief AI Officer:** C-suite role, strategic AI direction, governance oversight, reports to CEO, board presentations
- **AI Risk Manager:** Full-time, NIST AI RMF/ISO 42001 implementation, risk assessments, risk register, reports to Chief Risk Officer
- **AI Ethics Officer:** Full-time, responsible AI program, ethics reviews, training, principle operationalization
- **AI Governance Lead:** Full-time, policy administration, compliance monitoring, audit coordination
- **BU Governance Leads:** Part-time in each business unit, local governance implementation, escalation to central team
- **Committees:** Multiple specialized committees for different governance aspects

**Budget:** $1M-$3M/year (multiple full-time roles, enterprise tools, external consultants)

---

## Appendix G: Audit Trail Schema and Examples

**Purpose:** Technical specifications for comprehensive AI audit logging

**Comprehensive Audit Log Schema (JSON):**

```json
{
  "audit_record_version": "2.0",
  "record_id": "uuid-v4",
  "timestamp": "2026-01-03T14:23:45.123Z",

  "agent_info": {
    "agent_id": "customer-service-bot-v2.1",
    "agent_name": "Customer Service AI Assistant",
    "agent_version": "2.1.3",
    "model_provider": "anthropic",
    "model_name": "claude-3-opus",
    "model_version": "20240229",
    "deployment_environment": "production"
  },

  "request_info": {
    "request_id": "uuid-v4",
    "session_id": "uuid-v4",
    "user_id": "user_12345",
    "user_role": "customer",
    "channel": "web_chat",
    "ip_address": "198.51.100.42",
    "geolocation": "US-CA-San Francisco"
  },

  "input": {
    "prompt_text": "What is my account balance?",
    "prompt_tokens": 6,
    "input_classification": "account_inquiry",
    "pii_detected": false,
    "sentiment": "neutral"
  },

  "processing": {
    "data_sources_accessed": [
      "customer_database",
      "account_service_api"
    ],
    "retrieval_results": {
      "documents_retrieved": 2,
      "relevance_scores": [0.92, 0.85]
    },
    "processing_duration_ms": 1247
  },

  "output": {
    "response_text": "Your current account balance is $1,234.56.",
    "completion_tokens": 10,
    "total_tokens": 16,
    "confidence_score": 0.94,
    "output_classification": "account_information",
    "pii_redacted": false,
    "harm_check_passed": true
  },

  "decision_info": {
    "action_taken": "information_provided",
    "autonomy_level": 3,
    "human_review_required": false,
    "escalation_triggered": false,
    "reasoning_trace": "User authenticated, account query standard, confidence high, no PII in response",
    "alternative_actions_considered": []
  },

  "human_oversight": {
    "oversight_type": "managed",
    "reviewed_by": null,
    "review_timestamp": null,
    "review_outcome": null,
    "override_applied": false
  },

  "quality_metrics": {
    "latency_ms": 1247,
    "cost_usd": 0.0012,
    "user_satisfaction": null,
    "user_feedback": null,
    "error_occurred": false,
    "error_type": null
  },

  "compliance": {
    "data_retention_days": 2555,
    "regulatory_context": ["GDPR", "CCPA"],
    "consent_verified": true,
    "audit_flag": false,
    "legal_hold": false
  },

  "metadata": {
    "created_by": "ai_agent_platform",
    "log_version": "2.0",
    "environment": "production",
    "region": "us-west-2"
  }
}
```

**Minimum Required Fields (for basic compliance):**
- `record_id`, `timestamp`
- `agent_id`, `agent_version`
- `user_id`
- `input.prompt_text`, `output.response_text`
- `decision_info.action_taken`
- `quality_metrics.error_occurred`

**Retention Policies by Risk Level:**

| Risk Level | Retention Period | Rationale |
|------------|-----------------|-----------|
| **Critical** | 10 years | Regulatory requirements, liability protection |
| **High** | 7 years | Financial services standard, litigation statute of limitations |
| **Medium** | 3 years | GDPR maximum for routine processing |
| **Low** | 1 year | Operational troubleshooting, performance monitoring |

---

## Appendix H: Governance Policy Templates

**Purpose:** Starter policy templates that organizations can customize

**Template 1: AI System Approval Policy**

```markdown
# AI System Approval Policy

**Policy Owner:** Chief AI Officer
**Effective Date:** [DATE]
**Review Frequency:** Annual
**Version:** 1.0

## Purpose
This policy establishes approval requirements for AI systems based on risk classification.

## Scope
Applies to all AI systems deployed in [ORGANIZATION NAME] production environments.

## Risk-Based Approval Matrix

| Risk Level | Required Approvals | Timeline | Artifacts Required |
|------------|-------------------|----------|-------------------|
| **Critical** | - Executive Committee<br>- Board of Directors<br>- Legal<br>- Ethics Board | 4-8 weeks | - Risk assessment<br>- Bias audit<br>- Security review<br>- Privacy impact assessment |
| **High** | - Department Head<br>- AI Risk Manager<br>- Legal<br>- Ethics Board | 2-4 weeks | - Risk assessment<br>- Bias testing results<br>- Security checklist |
| **Medium** | - Department Head<br>- AI Governance Lead | 1 week | - Risk assessment<br>- Basic testing results |
| **Low** | - Team Lead | 1-2 days | - Risk classification justification |

## Approval Process

1. **Requestor** submits AI System Approval Request form
2. **AI Governance Lead** validates risk classification
3. **Required reviewers** conduct assessments (parallel)
4. **Approvers** provide approval or rejection with rationale
5. **AI Governance Lead** maintains approval records

## Emergency Approval Process

For urgent business needs, AI systems can receive temporary approval (max 30 days) from:
- Risk Level Critical/High: Chief AI Officer
- Risk Level Medium/Low: Department Head

Emergency approval must be ratified by standard process within temporary approval period.

## Non-Compliance
Deployment of AI systems without appropriate approval is a policy violation subject to disciplinary action.

## Related Policies
- AI Risk Classification Policy
- AI Ethics Policy
- Data Governance Policy
```

---

**Template 2: Agent Autonomy Boundaries Policy**

```markdown
# Agent Autonomy Boundaries Policy

**Policy Owner:** AI Risk Manager
**Effective Date:** [DATE]
**Review Frequency:** Quarterly
**Version:** 1.0

## Purpose
Define operational boundaries and escalation triggers for AI agents at each autonomy level.

## Autonomy Level Boundaries

### Level 4 (Autonomous)
**Financial Limits:**
- Maximum decision value: $10,000 per transaction
- Maximum daily aggregate: $100,000

**Stakeholder Limits:**
- Internal users: Unlimited
- External customers: Routine scenarios only

**Data Access:**
- Public and Internal classification only
- No Restricted data without explicit approval

**Regulatory:**
- Minimal risk tier only

### Level 3 (Managed)
**Financial Limits:**
- Maximum decision value: $1,000 per transaction
- Maximum daily aggregate: $25,000

**Stakeholder Limits:**
- Internal users: Unlimited
- External customers: Non-high-risk scenarios

**Data Access:**
- Public, Internal, and approved Confidential datasets
- No Restricted data

**Regulatory:**
- Limited risk tier only

### Level 2 (Supervised)
[Similar format for Levels 2, 1, 0...]

## Mandatory Escalation Triggers

Regardless of autonomy level, the following situations require immediate human review:

1. **Low Confidence:** AI confidence score <0.7
2. **Novel Situation:** Input pattern >2 standard deviations from training distribution
3. **Error Spike:** Error rate exceeds baseline by >20% in rolling 1-hour window
4. **User Request:** User explicitly requests human review
5. **High-Value Threshold:** Decision value exceeds level-specific limit
6. **PII Detected:** Unexpected personally identifiable information in inputs or outputs
7. **Harmful Content:** Content filter flags potential harm
8. **Regulatory Trigger:** Decision falls under regulated use case

## Escalation Procedures

**For Confidence/Novel Situation triggers:**
- Route to appropriate human reviewer within 5 minutes
- Continue conversation only with human approval

**For Error Spike triggers:**
- Alert AI operations team immediately
- Temporarily reduce autonomy level by 1 until resolved

**For User Request triggers:**
- Transfer to human operator immediately
- Provide full conversation context

## Monitoring and Enforcement

- AI Operations team monitors escalation metrics daily
- Monthly review of escalation patterns to identify needed boundary adjustments
- Quarterly policy review to update boundaries based on AI performance data
```

---

## Appendix I: Exercise Solutions and Grading Rubrics

**Exercise 2.1: Regulatory Compliance Mapping**

**Grading Rubric (10 points total):**
- **Regulation Identification (2 points):** Correct regulations identified for industry/geography
- **Requirement Mapping (3 points):** Specific requirements mapped to AI systems
- **Gap Assessment (3 points):** Current state vs. required state clearly documented
- **Remediation Plan (2 points):** Prioritized actions with realistic timelines and owners

**Sample High-Quality Answer:**
- Regulations: EU AI Act (High-Risk), GDPR Article 22, NYC Local Law 144 (if employment AI)
- Requirement: "EU AI Act Article 14 - Human Oversight required for hiring AI"
- Gap: "Current state: Automated screening with no human review. Required: Human-in-the-loop for all hiring decisions"
- Remediation: "Implement human review queue, train hiring managers on override procedures, deploy within 60 days"

---

**Exercise 2.2: Governance Policy Suite**

**Grading Rubric (10 points total):**
- **Policy Coverage (3 points):** At least 3 core policies (Approval, Autonomy Boundaries, Risk Classification)
- **Policy Structure (2 points):** Proper format (purpose, scope, requirements, enforcement)
- **Specificity (3 points):** Concrete requirements, not vague statements
- **Organizational Fit (2 points):** Adapted to organization's specific context

**Sample High-Quality Answer:**
- Policy 1: AI System Approval - "High-risk AI requires Ethics Board approval (5 members, quorum 3)"
- Policy 2: Autonomy Boundaries - "Level 3 agents limited to $1,000/transaction, escalate to human if >$1,000"
- Policy 3: Incident Response - "P0 incidents require notification to CAO within 15 minutes"

---

**Exercise 2.3 Capstone: Enterprise AI Governance Framework**

**Grading Rubric (25 points total):**

**Part 1: Assessment (5 points)**
- [ ] Maturity assessment completed for all 7 domains
- [ ] Strengths and gaps clearly identified
- [ ] Current state baseline established

**Part 2: Framework Design (5 points)**
- [ ] Organizational structure recommended with justification
- [ ] Key roles defined with responsibilities
- [ ] Committee structure specified
- [ ] Risk framework selected (NIST/ISO/hybrid)

**Part 3: Agent Classification (5 points)**
- [ ] Autonomy classification system defined (5 levels)
- [ ] Current agent inventory classified
- [ ] Boundaries and triggers documented

**Part 4: Compliance Roadmap (5 points)**
- [ ] Applicable regulations identified
- [ ] Requirements mapped to AI systems
- [ ] Compliance gaps prioritized
- [ ] Remediation actions with timelines

**Part 5: Implementation Roadmap (5 points)**
- [ ] Three-phase plan (Foundation, Build, Operate)
- [ ] Quick wins identified (30-day targets)
- [ ] Success metrics defined
- [ ] Resource requirements estimated

**Overall Quality:**
- Professional formatting and presentation
- Executive-ready deliverable
- Internally consistent across sections
- Realistic and actionable recommendations

**Sample High-Quality Capstone:**
A 15-20 page document that:
- Begins with executive summary (1 page)
- Documents current state maturity (2-3 pages with scoring)
- Recommends federated governance model for 2,500-employee organization
- Classifies 12 existing AI agents across autonomy levels with detailed justification
- Maps 3 applicable regulations (EU AI Act, GDPR, sector-specific)
- Provides 12-month implementation roadmap with quick wins in month 1
- Includes appendices with RACI matrix, policy templates, risk register
- Professional formatting suitable for board presentation

---

**Version History:**

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-02 | Initial presentation created | [Instructor] |
| 2.0 | 2026-01-03 | Enhanced with BACKGROUND sections, Key Thesis, and expanded appendices D-I | Claude |
