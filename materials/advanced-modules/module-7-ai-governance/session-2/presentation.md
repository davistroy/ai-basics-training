# **POWERPOINT PRESENTATION: ADVANCED MODULE 7 SESSION 2**
## **Implementation & Compliance**

**Module:** Advanced Module 7: Enterprise AI Governance
**Session Number:** 2 of 2
**Session Duration:** 45 minutes
**Delivery Format:** Live MS Teams workshop

**Target Audience:** Consulting professionals, risk officers, compliance leads, and Block 3 graduates

**Session Learning Objectives:** By the end of this session, participants will:
1. Map regulatory compliance requirements (EU AI Act, sector-specific) to AI systems
2. Design organizational governance structures (centralized vs. federated models)
3. Classify agent autonomy levels and define appropriate human oversight
4. Implement audit trail requirements and accountability frameworks

**Entry Criteria:**
- [ ] Governance maturity assessment completed (Exercise 1.1)
- [ ] Responsible AI principles mapped (Exercise 1.2)
- [ ] Initial risk register created (Exercise 1.3)
- [ ] Understanding of organizational context

**Exit Criteria:**
- [ ] Regulatory compliance requirements mapped
- [ ] Agent autonomy classification system implemented
- [ ] Governance policies drafted
- [ ] Organizational structure recommendations documented
- [ ] Module capstone completed

**Presentation Structure:**
1. Opening & Session 1 Recap (3 min) - Slides 1-3
2. Segment 1: Regulatory Compliance Landscape (12 min) - Slides 4-7
3. Segment 2: Organizational Structures (10 min) - Slides 8-10
4. Segment 3: Agent Autonomy Classification (12 min) - Slides 11-14
5. Segment 4: Audit Trails & Accountability (11 min) - Slides 15-17
6. Capstone Preview & Close (3 min) - Slides 18-20

**Total Slides:** 20

---

## Slide Definitions

### SLIDE 1: TITLE SLIDE

**Title:** Advanced Module 7 Session 2: Implementation & Compliance

**Subtitle:** Building Operational AI Governance Frameworks

**Content:**
- [Instructor Name]
- [Date/Cohort identifier]
- AI Practitioner Training Program

**Graphic:** Professional slide with green advanced module tones. Visual of governance structure or compliance framework.

**SPEAKER NOTES:**

"Welcome to Session 2 of Enterprise AI Governance. Last week we built the foundation - NIST frameworks, responsible AI principles, lifecycle governance, and data governance. Today we make it operational.

By the end of this session, you'll know how to navigate regulatory compliance - from EU AI Act to sector-specific regulations. You'll be able to design governance structures that fit your organizational model. You'll classify AI agents by autonomy level and define appropriate human oversight. And you'll understand what audit trails and accountability frameworks must include.

Most importantly, you'll complete the module capstone - a complete Enterprise AI Governance Framework that integrates everything from both sessions into a client-ready deliverable.

Let's start by recapping where we are."

[Transition]

---

### SLIDE 2: SESSION 1 RECAP

**Title:** Building on Session 1 Foundations

**Content:**

**What We Covered Last Week:**
- NIST AI RMF & ISO 42001 frameworks
- Six responsible AI principles (Fairness, Transparency, Accountability, Privacy, Safety, Human Oversight)
- Agent lifecycle governance (Design → Develop → Test → Deploy → Monitor → Retire)
- Data governance pillars (Lineage, Quality, Privacy, Access)

**Your Session 1 Deliverables:**
- Governance Maturity Assessment (7 domains scored)
- Responsible AI Principles Mapping (6 principles operationalized)
- Agent Risk Register (risks identified and scored)

**How Session 2 Builds On This:**
Your Session 1 assessments become Part 1 of today's capstone framework

**Graphic:** Visual showing Session 1 as foundation, Session 2 as implementation layer

**SPEAKER NOTES:**

"Quick recap of Session 1. We established the governance foundations - the frameworks like NIST that structure your approach, the principles that guide decisions, the lifecycle thinking that keeps governance continuous, and the data governance that underpins everything.

You created three deliverables: a maturity assessment showing where your organization stands, a principles mapping that operationalizes abstract concepts, and a risk register identifying what could go wrong.

[Pause]

Today we shift from 'what to govern' to 'how to govern in practice.' That means understanding regulatory requirements, designing organizational structures, classifying AI autonomy, and ensuring accountability through audit trails.

Your Session 1 deliverables aren't just exercises - they become Part 1 of your capstone governance framework. We're building on that foundation today."

[Transition]

---

### SLIDE 3: LEARNING OBJECTIVES

**Title:** Today's Outcomes

**Content:**

You will be able to:

1. **Map regulatory compliance requirements to AI systems**
   - EU AI Act risk tiers, sector-specific regulations, compliance gaps

2. **Design appropriate governance organizational structures**
   - Centralized vs. federated models, key roles, governance committees

3. **Classify agent autonomy and define oversight requirements**
   - Five autonomy levels (0-4), decision criteria, human oversight models

4. **Implement audit trails and accountability frameworks**
   - Logging requirements, retention policies, accountability mapping

**Graphic:** Four objectives as pillars supporting "Enterprise AI Governance"

**SPEAKER NOTES:**

"Here are our four learning objectives:

First, regulatory compliance. You'll learn how to map the EU AI Act's risk tiers to your AI systems, understand sector-specific regulations like HIPAA for healthcare or SR 11-7 for financial services, and identify compliance gaps.

Second, organizational structures. Should governance be centralized with a strong AI Governance Council, or federated with business unit autonomy? What roles do you need - Chief AI Officer, AI Ethics Officer, AI Risk Manager? What committees?

Third, agent autonomy classification. Not all agents should operate with the same level of autonomy. You'll learn how to classify agents from Level 0 (tool) to Level 4 (autonomous) and what human oversight each level requires.

Fourth, audit trails and accountability. What must be logged for each AI decision? How long must logs be retained? How do you map accountability from agent actions to specific individuals?

Exercise 2.1 addresses objectives 1. Exercise 2.2 covers 2-4. The capstone integrates everything.

Let's start with regulatory compliance."

[Transition: Segment 1]

---

## SEGMENT 1: REGULATORY COMPLIANCE LANDSCAPE
### Duration: 12 minutes | Slides 4-7

---

### SLIDE 4: THE COMPLIANCE CHALLENGE

**Title:** The Expanding AI Regulatory Landscape

**Content:**

**The Challenge:**
AI governance frameworks must address multiple, sometimes conflicting, regulatory requirements across jurisdictions and sectors.

**Current State:**
- EU AI Act: Comprehensive risk-based regulation (2024-2027 phased implementation)
- US: Sector-specific regulations, no comprehensive federal AI law
- State/Local: NYC Local Law 144 (employment AI), California CCPA (data), evolving landscape
- Industry: Financial services (SR 11-7), healthcare (FDA SaMD, HIPAA), insurance (state regulators)

**Why It Matters:**
- Non-compliance penalties can be severe (up to 6% global revenue for EU AI Act)
- Different regulations have conflicting requirements
- Landscape is rapidly evolving
- Procurement increasingly requires compliance demonstration

**Graphic:** World map showing different regulatory regimes, or complexity visual showing overlapping regulations

**SPEAKER NOTES:**

"Let me set the compliance challenge.

Two years ago, AI governance was largely voluntary. Organizations adopted principles because it was the right thing to do, or because it protected reputation. Regulatory requirements were indirect - data protection laws, sector-specific rules that happened to apply to AI.

That's changing rapidly.

The EU AI Act creates comprehensive, mandatory requirements for AI systems. High-risk AI faces strict obligations. Some AI uses are prohibited entirely. And penalties are massive - up to 6% of global annual revenue.

In the US, we don't have comprehensive federal AI regulation yet. But we have sector-specific requirements. Financial services has SR 11-7 for model risk management. Healthcare has FDA oversight for AI as medical devices. Employment AI now faces bias audit requirements in NYC.

[Point to graphic]

The challenge is navigating this patchwork. If you operate in multiple jurisdictions or sectors, you're dealing with overlapping, sometimes conflicting, requirements.

Today you'll learn a systematic approach to compliance mapping. Let's start with the EU AI Act - currently the most comprehensive AI regulation."

[Transition]

---

### SLIDE 5: EU AI ACT RISK TIERS

**Title:** EU AI Act: Risk-Based Approach

**Content:**

```
┌─────────────────────────────────────────────────────────┐
│                  EU AI ACT RISK TIERS                   │
│                                                         │
│  ┌─────────────────────────────────────────────────┐   │
│  │ UNACCEPTABLE RISK - PROHIBITED                  │   │
│  │ Social scoring, real-time biometric ID          │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ HIGH RISK - STRICT REQUIREMENTS                 │   │
│  │ Credit scoring, hiring, medical devices         │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ LIMITED RISK - TRANSPARENCY OBLIGATIONS         │   │
│  │ Chatbots, emotion recognition                   │   │
│  └─────────────────────────────────────────────────┘   │
│  ┌─────────────────────────────────────────────────┐   │
│  │ MINIMAL RISK - NO SPECIFIC REQUIREMENTS         │   │
│  │ AI-enabled games, spam filters                  │   │
│  └─────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

**High-Risk System Requirements:**
- Risk management system
- Data governance and lineage
- Technical documentation
- Record-keeping and audit trails
- Transparency to users
- Human oversight
- Accuracy, robustness, cybersecurity

**Graphic:** EU AI Act pyramid showing risk tiers

**SPEAKER NOTES:**

"The EU AI Act uses a risk-based approach with four tiers.

At the top, Unacceptable Risk - these AI systems are prohibited. Social scoring by governments, real-time biometric identification in public spaces, certain manipulative AI. You can't deploy these in the EU, period.

Next, High Risk - these face strict requirements. This includes AI used in credit scoring, employment, education, law enforcement, critical infrastructure, and medical devices. If your AI falls in this category, you have extensive obligations.

[Point to requirements]

You need a formal risk management system - not just 'we think about risk,' but documented processes. You need data governance with lineage tracking. You need technical documentation that explains how your AI works. You need comprehensive record-keeping - audit trails for decisions. You must provide transparency to users - they need to know AI is being used. You need human oversight - humans must be able to intervene. And you need to demonstrate accuracy, robustness, and cybersecurity.

Limited Risk tier requires transparency - you must disclose AI use. Chatbots must tell users they're bots. Emotion recognition or biometric categorization must be disclosed.

Minimal Risk has no specific requirements - AI-enabled games, spam filters.

[Pause]

Most enterprise AI falls into High Risk or Limited Risk categories. If you're doing hiring, lending, insurance decisioning, medical diagnosis - you're High Risk under EU AI Act.

In Exercise 2.1, you'll map these tiers to your AI systems and identify compliance gaps."

[Transition]

---

### SLIDE 6: SECTOR-SPECIFIC REGULATIONS

**Title:** Beyond EU AI Act: Sector-Specific Requirements

**Content:**

| Sector | Key Regulations | AI-Specific Implications |
|--------|----------------|-------------------------|
| **Financial Services** | SR 11-7 (US Fed), Basel III/IV, MiFID II | Model risk management, model validation, explainability requirements, documentation standards |
| **Healthcare** | HIPAA, FDA Software as Medical Device (SaMD), EU MDR | Patient safety validation, clinical evidence, adverse event reporting, data privacy |
| **Employment** | EEOC (US), NYC Local Law 144, GDPR Art 22 | Bias audits, candidate notification, right to human review, statistical fairness testing |
| **Insurance** | State insurance regulations, NAIC Model Bulletin | Actuarial justification, anti-discrimination rules, rate filing requirements |

**Compliance Mapping Approach:**
1. Identify applicable regulations for your industry and jurisdictions
2. Map specific requirements to your AI systems
3. Conduct gap assessment against current state
4. Create remediation plan with priorities
5. Establish ongoing monitoring for regulatory changes

**Graphic:** Table or matrix showing sectors and their regulatory requirements

**SPEAKER NOTES:**

"Beyond the EU AI Act, you need to understand sector-specific regulations.

Financial services: SR 11-7 from the Federal Reserve requires model risk management for any model used in decisions - including AI models. You need independent model validation, comprehensive documentation, and ongoing monitoring. Basel banking regulations and MiFID II in Europe add explainability requirements.

Healthcare: FDA regulates AI as Software as Medical Device if it diagnoses, treats, or monitors patients. You need clinical validation, adverse event reporting, and strict data privacy under HIPAA.

Employment: EEOC enforces anti-discrimination laws. NYC Local Law 144 now requires annual bias audits for AI used in hiring or promotion, with specific statistical tests. GDPR Article 22 gives candidates the right to human review of automated employment decisions.

Insurance: State regulators oversee insurance AI. You need actuarial justification for rate decisions, anti-discrimination compliance, and in many states, detailed rate filings that explain AI logic.

[Point to compliance approach]

The approach is systematic: identify your applicable regulations, map requirements to specific AI systems, assess gaps, remediate, and monitor for changes.

This is complex, which is why Exercise 2.1 provides a structured template for regulatory compliance mapping."

[Transition]

---

### SLIDE 7: COMPLIANCE SUMMARY

**Title:** Key Compliance Takeaways

**Content:**

**Remember:**
1. **Know your risk tier** - EU AI Act classification drives requirements intensity
2. **Sector regulations often stricter** - Industry-specific rules may exceed general AI regulations
3. **Compliance is ongoing** - Regulations evolve; establish monitoring processes
4. **Document everything** - Compliance requires demonstrable evidence, not just claims

**You'll Practice:**
Exercise 2.1 creates a regulatory compliance mapping for your organization

**Common Pitfall:**
Assuming compliance is a legal team problem. Compliance requires technical, operational, and governance changes - it's a cross-functional effort.

**Graphic:** Compliance cycle showing assess → remediate → monitor → reassess

**SPEAKER NOTES:**

"Before moving on, key compliance points:

One, know your risk tier under EU AI Act. If you're High Risk, you have extensive obligations. If you're Limited Risk, focus on transparency. This classification drives your compliance intensity.

Two, sector regulations are often stricter than general AI laws. If you're in financial services, healthcare, or employment, sector rules may require more than EU AI Act. Map both.

Three, compliance is ongoing. The EU AI Act has phased implementation through 2027. US states are passing new AI laws. Regulators are issuing guidance. You need processes to monitor regulatory changes.

Four, document everything. Saying 'we govern AI responsibly' isn't compliance. You need documented risk assessments, test results, audit trails, approval records. Evidence.

[Pause]

Common mistake: treating compliance as purely a legal issue. Legal needs to be involved, but compliance requires technical capabilities - bias testing tools, audit logging systems, monitoring infrastructure. It requires operational changes - approval workflows, review processes. It's cross-functional.

Exercise 2.1 gives you the compliance mapping template. Now let's talk about organizational structures to govern AI."

[Transition: Segment 2]

---

## SEGMENT 2: ORGANIZATIONAL STRUCTURES FOR AI GOVERNANCE
### Duration: 10 minutes | Slides 8-10

---

### SLIDE 8: GOVERNANCE STRUCTURE MODELS

**Title:** Centralized vs. Federated Governance

**Content:**

**Centralized Model:**
```
                  ┌─────────────────┐
                  │ AI Governance   │
                  │    Council      │
                  └────────┬────────┘
                           │
          ┌────────────────┼────────────────┐
          │                │                │
     ┌────┴────┐     ┌─────┴─────┐    ┌────┴────┐
     │ BU 1    │     │   BU 2    │    │  BU 3   │
     │ AI Team │     │  AI Team  │    │ AI Team │
     └─────────┘     └───────────┘    └─────────┘
```
- Central authority sets all standards and policies
- Consistent governance across organization
- Best for regulated industries, high-risk AI

**Federated Model:**
```
     ┌─────────────────────────────────────────────┐
     │        AI Center of Excellence              │
     │     (Standards, Best Practices)             │
     └─────────────────────────────────────────────┘
                          ↕
     ┌─────────┐    ┌─────────┐    ┌─────────┐
     │ BU 1    │    │  BU 2   │    │  BU 3   │
     │ AI Gov  │    │ AI Gov  │    │ AI Gov  │
     │ (Local) │    │ (Local) │    │ (Local) │
     └─────────┘    └─────────┘    └─────────┘
```
- Business units have autonomy with central guidance
- Flexible implementation for local contexts
- Best for diverse organizations, lower-risk AI

**Graphic:** Side-by-side comparison diagrams

**SPEAKER NOTES:**

"One of the first design decisions: centralized or federated governance?

Centralized model: You have a central AI Governance Council with strong authority. They set all standards, policies, and approval requirements. Business units implement AI, but governance decisions flow through the center.

[Point to centralized diagram]

Advantages: Consistency, strong control, clear accountability. If you're in a regulated industry like financial services or healthcare, centralized governance gives you the control you need for compliance.

Disadvantages: Can be slow, bureaucratic, and may not fit local business unit contexts well.

[Point to federated diagram]

Federated model: You have an AI Center of Excellence that provides guidance, best practices, templates. But each business unit maintains its own AI governance function with autonomy to adapt to local needs.

Advantages: Flexibility, faster decision-making, better alignment with business unit contexts.

Disadvantages: Less consistency, risk of governance gaps if some business units underinvest.

[Pause]

Most large organizations use a hybrid - central policies and standards from a governance council or center of excellence, but business units have autonomy within those guardrails.

The key is matching structure to your organization's culture, risk tolerance, and regulatory context. Your capstone will require you to recommend a structure and justify it."

[Transition]

---

### SLIDE 9: KEY GOVERNANCE ROLES

**Title:** Building the AI Governance Team

**Content:**

| Role | Responsibility | Reports To | When Needed |
|------|----------------|------------|-------------|
| **Chief AI Officer** | Enterprise AI strategy, governance oversight, executive sponsor | CEO / Board | Medium+ AI maturity |
| **AI Ethics Officer** | Responsible AI principles, ethics reviews, escalation resolution | CAO or Legal | High-risk AI deployments |
| **AI Risk Manager** | Risk framework, risk assessments, monitoring | CRO | Regulated industries |
| **AI Governance Lead** | Policy implementation, compliance, process management | CAO or CTO | All organizations with AI |
| **Model Validators** | Independent technical review of models/agents | Risk function | High-risk AI systems |

**Governance Committees:**
- AI Steering Committee (strategic direction)
- AI Ethics Board (principle conflicts, ethical escalations)
- Model Risk Committee (technical review, validation)
- Data Governance Council (data quality, lineage, privacy)

**Graphic:** Org chart showing roles and reporting relationships

**SPEAKER NOTES:**

"Let's talk about key roles in AI governance.

Chief AI Officer is your executive sponsor. Not every organization needs a dedicated CAO - smaller organizations might assign this to CTO or CDO. But someone at executive level must own AI strategy and governance. This person sets direction, allocates resources, and represents AI in C-suite discussions.

AI Ethics Officer handles responsible AI principles. When you have a conflict between transparency and IP protection, who decides? When an AI system produces unexpectedly biased outputs, who investigates? The Ethics Officer. This role is critical for high-risk AI.

AI Risk Manager implements your risk framework - NIST AI RMF or ISO 42001. They conduct risk assessments, track the risk register, ensure monitoring. Regulated industries need this role.

AI Governance Lead is your operational implementer. They write policies, manage approval workflows, track compliance, coordinate across business units. Every organization with AI needs this function, even if it's part-time initially.

Model Validators provide independent review. In financial services, this is often required by regulators - someone not involved in model development must validate it. For high-risk AI, independent validation is a best practice even if not legally required.

[Point to committees]

Governance committees provide oversight and decision-making forums. Steering committee for strategy, Ethics Board for principle conflicts, Risk Committee for technical reviews, Data Council for data-related decisions.

In your capstone, you'll define which roles and committees your organization needs."

[Transition]

---

### SLIDE 10: ORGANIZATIONAL STRUCTURE SUMMARY

**Title:** Designing Your Governance Organization

**Content:**

**Key Takeaway:** No one-size-fits-all structure - design based on your context

**Decision Factors:**
- Organization size and complexity
- AI maturity level
- Regulatory environment
- Risk appetite
- Organizational culture (centralized vs. distributed decision-making)
- Resource availability

**Recommended Approach:**
1. Start lean - don't build full structure before you need it
2. Begin with AI Governance Lead + Ethics Board
3. Add Chief AI Officer as AI becomes strategic
4. Add Risk Manager and Model Validators for high-risk or regulated AI
5. Evolve from centralized control to federated model as maturity increases

**You'll Practice:**
Exercise 2.2 and Capstone require organizational structure recommendations

**Graphic:** Maturity progression showing governance structure evolution

**SPEAKER NOTES:**

"There's no one-size-fits-all governance structure. Your design depends on context.

Small organization, early AI adoption? Start with an AI Governance Lead who can be part-time, maybe someone from IT or data analytics. Add an Ethics Board for escalations - doesn't need to be full-time roles, can meet quarterly.

Medium organization, growing AI maturity? Add a Chief AI Officer to provide executive leadership. If you're in regulated industry, add AI Risk Manager.

Large organization, mature AI, high-risk deployments? Full structure - CAO, Ethics Officer, Risk Manager, Governance Lead, Model Validators, plus committees.

[Point to recommended approach]

The mistake is building too much structure too soon. You don't need a Chief AI Officer when you have two AI pilots. But you also can't scale without structure - if you have 50 AI systems and no governance lead, you're in trouble.

Start lean, evolve as you grow.

Your capstone requires a structure recommendation with justification. Why is this the right model for your organization?"

[Transition: Segment 3]

---

## SEGMENT 3: AGENT AUTONOMY CLASSIFICATION
### Duration: 12 minutes | Slides 11-14

---

### SLIDE 11: THE AUTONOMY SPECTRUM

**Title:** Five Levels of Agent Autonomy

**Content:**

| Level | Name | Description | Human Oversight | Example |
|-------|------|-------------|-----------------|---------|
| **0** | Tool | Human initiates every action, AI assists | Full human control | Grammar checker |
| **1** | Assistant | AI suggests, human decides | Human approval for each action | Email draft suggestions |
| **2** | Supervised | AI acts, human monitors | Human can review and veto | Customer service bot with live agent monitoring |
| **3** | Managed | AI operates, human oversight | Exception-based human review | Fraud detection with human review of flagged cases |
| **4** | Autonomous | AI operates independently | Periodic audit only | Automated trading within preset limits |

**Key Principle:** Autonomy level should match risk level and reversibility

**Graphic:** Spectrum showing autonomy levels from 0-4 with human oversight decreasing

**SPEAKER NOTES:**

"Not all AI agents should operate at the same autonomy level. We need a classification system.

I'm going to walk you through five autonomy levels, 0 through 4.

Level 0 - Tool: Human initiates every action. AI assists but doesn't act independently. Example: grammar checker that suggests corrections but you choose whether to apply them.

Level 1 - Assistant: AI suggests actions, human approves before execution. Email suggests a draft reply, you review and send. Human approval for each action.

Level 2 - Supervised: AI takes actions, but human monitors in real-time with ability to intervene. Customer service chatbot handles routine queries, live agent is monitoring and can step in.

Level 3 - Managed: AI operates with exception-based human review. Fraud detection AI flags suspicious transactions, human reviews the flagged cases but most transactions process automatically.

Level 4 - Autonomous: AI operates independently within boundaries, human audit is periodic not real-time. Automated trading system executes trades within preset risk limits, humans review performance weekly.

[Pause]

The key principle: autonomy should match risk and reversibility. High impact + irreversible decisions = low autonomy. Low impact + easily reversible = higher autonomy acceptable.

We're going to see how to classify agents systematically."

[Transition]

---

### SLIDE 12: AUTONOMY DECISION CRITERIA

**Title:** How to Classify Agent Autonomy

**Content:**

**Decision Matrix:**
```
┌─────────────────────────────────────────────────────────┐
│           AUTONOMY LEVEL DETERMINATION                  │
│                                                         │
│  Impact │ Reversible │ Compliance │ → Autonomy Level   │
│  ───────┼────────────┼────────────┼──────────────────  │
│  Low    │ Yes        │ Low        │ → Level 3-4       │
│  Low    │ Yes        │ High       │ → Level 2         │
│  Low    │ No         │ Any        │ → Level 1-2       │
│  Med    │ Yes        │ Low        │ → Level 2-3       │
│  Med    │ Yes        │ High       │ → Level 1-2       │
│  Med    │ No         │ Any        │ → Level 1         │
│  High   │ Yes        │ Any        │ → Level 1-2       │
│  High   │ No         │ Any        │ → Level 0-1       │
└─────────────────────────────────────────────────────────┘
```

**Three Factors:**
1. **Impact:** What's the consequence if AI makes a mistake?
2. **Reversibility:** Can the decision be easily undone?
3. **Compliance Context:** Is this a regulated use case requiring human oversight?

**Examples:**
- Low impact + reversible + low compliance = Level 3-4 (recommendation engines)
- High impact + not reversible + any compliance = Level 0-1 (medical diagnosis)

**Graphic:** Decision tree or matrix showing how factors combine

**SPEAKER NOTES:**

"Here's how to classify autonomy systematically using three factors.

Impact: What happens if the AI makes a mistake? An FAQ chatbot giving wrong information is low impact - mildly annoying. A loan approval AI denying qualified applicants is high impact - financial harm, potential discrimination.

Reversibility: Can you undo the decision easily? An email draft suggestion is reversible - just don't send it. An insurance coverage denial is hard to reverse - now you have to appeal, which takes time and may leave someone uninsured.

Compliance context: Is this use case subject to regulations requiring human oversight? Employment decisions under NYC Local Law 144 need human review. Medical device AI under FDA requires human supervision.

[Point to matrix]

These factors combine to determine autonomy level. Low impact + reversible + low compliance risk = you can allow Level 3 or 4 autonomy. A recommendation engine for online shopping fits here.

High impact + not reversible + regulated = Level 0 or 1 only. Medical diagnosis AI fits here - high impact if wrong, diagnosis isn't easily reversed, heavily regulated. You need human approval.

[Pause]

This isn't just about what's technically possible. AI might be capable of fully autonomous operation, but governance says 'not without human oversight' for high-risk scenarios.

Let me show you real examples."

[Transition]

---

### SLIDE 13: AUTONOMY BOUNDARIES AND TRIGGERS

**Title:** Defining Operational Boundaries

**Content:**

**Autonomy Boundaries by Level:**

**Level 2 (Supervised):**
- Financial: Decisions under $100
- Stakeholder: Internal users only
- Data: Non-PII data only
- Regulatory: Non-regulated use cases

**Level 3 (Managed):**
- Financial: Decisions under $1,000
- Stakeholder: Customers, non-high-risk scenarios
- Data: PII allowed with controls
- Regulatory: Limited risk tier

**Level 4 (Autonomous):**
- Financial: Decisions under $10,000
- Stakeholder: Customers, routine scenarios
- Data: Comprehensive data with audit
- Regulatory: Minimal risk tier only

**Escalation Triggers (Force human review regardless of level):**
- AI confidence score below threshold (e.g., <0.7)
- Novel situation (not seen in training data)
- Error rate spike (exceeds baseline by >20%)
- Explicit user request for human review
- High-value or high-sensitivity scenario

**Graphic:** Boundaries as gates with escalation paths

**SPEAKER NOTES:**

"Once you've classified autonomy level, you need to define operational boundaries.

Boundaries limit what the agent can do autonomously. For Level 2 (supervised), you might allow decisions only under $100, only for internal users, only with non-PII data. Above those thresholds, human approval required.

Level 3 (managed) gets higher boundaries - maybe $1,000 financial decisions, customer-facing scenarios allowed, PII permitted with privacy controls.

Level 4 (autonomous) has highest boundaries - up to $10,000, broad customer scenarios, comprehensive data access with comprehensive audit.

[Point to escalation triggers]

But even within autonomy level, you need escalation triggers that force human review.

AI confidence score: If the AI isn't confident in its decision, it escalates. You might set threshold at 0.7 - below that, human reviews.

Novel situation: If the AI encounters a scenario significantly different from training data, it flags for human judgment.

Error rate spike: If performance suddenly degrades, automatic escalation and review.

User request: Users can always request human review.

High-value scenarios: Even if generally autonomous, specific high-stakes cases get human review.

[Pause]

These boundaries and triggers prevent autonomous agents from operating beyond appropriate limits. Your capstone will require defining these for your agent inventory."

[Transition]

---

### SLIDE 14: AUTONOMY CLASSIFICATION SUMMARY

**Title:** Key Autonomy Principles

**Content:**

**Remember:**
1. **Match autonomy to risk** - High impact + irreversible = low autonomy
2. **Document classification rationale** - Why is this agent Level 2 not Level 3?
3. **Define clear boundaries** - Financial limits, data access, stakeholder scope
4. **Build escalation triggers** - Confidence thresholds, novel situations, error spikes
5. **Review classifications periodically** - Agent capabilities and contexts change

**You'll Practice:**
Exercise 2.2 and Capstone include autonomy classification systems

**Common Mistake:**
Setting autonomy level based on technical capability rather than risk and governance requirements

**Graphic:** Autonomy classification process flow

**SPEAKER NOTES:**

"Before moving on, five autonomy principles:

One, match autonomy to risk, not capability. Your AI might be capable of fully autonomous operation, but if the risk is high, governance requires human oversight.

Two, document your classification rationale. Auditors and regulators will ask 'why is this Level 3?' You need to show your decision factors - impact, reversibility, compliance context.

Three, define clear boundaries. 'Level 3 managed autonomy' is too vague. 'Level 3 with $1,000 financial limit, customer-facing allowed, PII with privacy controls' is specific and enforceable.

Four, build escalation triggers. Don't rely solely on autonomy level. Even autonomous agents should escalate when they encounter situations outside normal parameters.

Five, review periodically. Agent capabilities improve. Business contexts change. Regulations evolve. What was Level 1 last year might be ready for Level 2 now. Or vice versa - new regulation might require reducing autonomy.

[Pause]

The common mistake is classifying based on what the AI can do rather than what it should be allowed to do. Technical capability doesn't determine autonomy - risk and governance do.

Now let's talk about how you ensure accountability through audit trails."

[Transition: Segment 4]

---

## SEGMENT 4: AUDIT TRAILS AND ACCOUNTABILITY
### Duration: 11 minutes | Slides 15-17

---

### SLIDE 15: AUDIT TRAIL REQUIREMENTS

**Title:** What Must Be Logged

**Content:**

**Comprehensive Audit Record:**
```json
{
  "audit_record": {
    "timestamp": "ISO-8601",
    "agent_id": "unique-identifier",
    "action_type": "decision|recommendation|automation",
    "input_summary": "what triggered the action",
    "output_summary": "what the agent did/said",
    "reasoning_trace": "explanation of decision",
    "data_accessed": ["list", "of", "sources"],
    "human_oversight": {
      "type": "approval|review|none",
      "approver": "user-id or null",
      "timestamp": "ISO-8601 or null"
    },
    "confidence_score": 0.0-1.0,
    "model_version": "version-identifier",
    "outcome": "success|failure|pending"
  }
}
```

**Why Each Field Matters:**
- **Timestamp:** When did this happen? (Regulatory requirement, audit requirement)
- **Agent ID:** Which AI made this decision? (Accountability, debugging)
- **Action type:** What category? (Classification for review)
- **Input/Output:** What happened? (Reconstruct decision)
- **Reasoning trace:** Why? (Explainability, audit)
- **Data accessed:** What data? (Privacy compliance, lineage)
- **Human oversight:** Was human involved? (Compliance, accountability)
- **Confidence:** How certain was AI? (Quality control, escalation)
- **Model version:** Which version? (Versioning, rollback)
- **Outcome:** Result? (Performance tracking)

**Graphic:** Audit record structure with arrows showing why each field matters

**SPEAKER NOTES:**

"Let's talk about audit trails - what must be logged for each AI decision.

This is a comprehensive audit record structure. Let me walk through why each field matters.

Timestamp: When did this decision happen? Regulatory requirements often specify log retention periods calculated from timestamp. Audit trails need chronological ordering.

Agent ID: Which specific AI agent made this decision? If you have multiple versions or multiple agents, you need to know which one.

Action type: Was this a decision, a recommendation, or automation? This helps classify for review and analysis.

Input and output summaries: What triggered the action, what did the agent do? This lets you reconstruct the decision for audit or dispute resolution.

Reasoning trace: This is your explainability requirement. Why did the AI make this decision? What factors did it consider? Regulators and users may demand explanations.

Data accessed: What data sources did the AI use? Privacy regulations require this. Data lineage tracking depends on it.

Human oversight: Was a human involved? If so, who approved or reviewed, and when? This is critical for compliance with regulations requiring human oversight.

Confidence score: How confident was the AI? This enables quality control and helps identify decisions that should have escalated.

Model version: Which version of the AI made this decision? Essential for versioning control and rollback if needed.

Outcome: Success or failure? Performance tracking depends on capturing outcomes.

[Pause]

This level of logging might seem excessive. But when a regulator asks 'explain this decision' or a user disputes an outcome, you need comprehensive audit trails. Missing any of these fields creates gaps."

[Transition]

---

### SLIDE 16: RETENTION AND ACCESS

**Title:** Managing Audit Trails

**Content:**

**Retention Requirements:**

| Driver | Typical Requirement | Example |
|--------|-------------------|---------|
| **Regulatory** | 5-7 years | Financial services: SR 11-7 requires 7 years |
| **Legal** | Statute of limitations + buffer | Employment decisions: 5 years in many states |
| **Operational** | Duration of AI deployment + 1 year | While agent operates + retention after retirement |
| **Litigation hold** | Indefinite until released | If litigation is pending or anticipated |

**Access Control:**
- **Who can request:** Compliance officers, auditors, legal, data subject (GDPR right of access)
- **Response time:** Internal audit (48 hours), regulatory audit (24 hours), data subject request (30 days under GDPR)
- **Format:** Structured data export, human-readable report, both
- **Chain of custody:** Audit trail access must itself be logged

**Storage Considerations:**
- Volume: High-frequency agents generate massive logs
- Cost: Long-term storage for compliance
- Security: Audit trails often contain PII - protect appropriately
- Immutability: Logs must be tamper-evident

**Graphic:** Retention timeline and access control workflow

**SPEAKER NOTES:**

"Logging is only half the challenge. You also need retention and access controls.

Retention requirements come from multiple sources. Regulatory requirements vary by industry - financial services often requires 7 years under SR 11-7. Legal requirements depend on statutes of limitations - employment decisions might need 5-year retention because that's how long discrimination claims can be filed.

Operationally, you need audit trails for the entire time an AI operates, plus a buffer after retirement in case issues arise.

And if litigation is pending or anticipated, you may need to preserve logs indefinitely until legal hold is released.

[Point to access control]

Access control is critical. Compliance officers and auditors need access for reviews. Legal needs access for litigation. Regulators can demand access. And under GDPR, data subjects can request information about automated decisions affecting them.

Response times matter. Regulators expect fast turnaround - 24 hours isn't unusual. Internal audits might allow 48 hours. Data subject requests have legal deadlines - 30 days under GDPR.

Format: Sometimes you need structured data exports for analysis, sometimes human-readable reports for explanation, often both.

Chain of custody: Access to audit trails must itself be logged. Who viewed what logs when? This prevents tampering and provides accountability.

[Point to storage considerations]

Storage is a real challenge. A high-frequency trading AI or customer service bot generates millions of log records. Long-term retention for compliance is expensive. Logs often contain PII, so you need security controls. And logs must be immutable - you can't allow modification after creation.

Your capstone should address how audit trails will be stored, retained, and accessed."

[Transition]

---

### SLIDE 17: ACCOUNTABILITY MAPPING

**Title:** From Audit Trail to Accountability

**Content:**

**Accountability Chain:**
```
Agent Action → Audit Log → Owner Identified → Accountability Assigned
                                  │
                   ┌──────────────┼──────────────┐
                   │              │              │
                Operator      Developer      Executive
               (immediate)   (system)       (policy)
```

**Three Levels of Accountability:**

**Operator Level (Immediate):**
- Person who deployed the agent or approved specific action
- Accountable for: Appropriate use, oversight execution, escalation when needed
- Example: Customer service manager who deployed chatbot

**Developer Level (System):**
- Team who built/configured the agent
- Accountable for: System design, testing adequacy, performance monitoring
- Example: Data science team that built recommendation engine

**Executive Level (Policy):**
- Leadership who approved agent deployment and set governance policies
- Accountable for: Strategic decisions, risk acceptance, resource allocation
- Example: Chief AI Officer who approved high-risk AI deployment

**RACI Matrix for AI Decisions:**
- **Responsible:** Who does the work (often the AI, with human oversight)
- **Accountable:** Who owns the outcome (must be a human)
- **Consulted:** Who provides input (subject matter experts, legal, ethics)
- **Informed:** Who needs to know (stakeholders, executives)

**Graphic:** Accountability pyramid showing three levels

**SPEAKER NOTES:**

"Audit trails enable accountability. But accountability isn't automatic - you need to map it explicitly.

Think of accountability at three levels.

Operator level: The person who deployed the AI or approved a specific action. If a customer service manager deploys a chatbot, they're accountable for appropriate use, oversight, and escalation. When something goes wrong operationally, accountability starts here.

Developer level: The team who built or configured the agent. Data scientists, ML engineers. They're accountable for system design, adequate testing, and performance monitoring. If the AI has a systematic flaw - bias in outputs, security vulnerability - accountability is at developer level.

Executive level: Leadership who approved deployment and set governance policies. The Chief AI Officer, Risk Committee, Board. They're accountable for strategic decisions, risk acceptance, resource allocation. If governance fails systematically - wrong risk classification, inadequate oversight - accountability is executive.

[Point to RACI]

RACI matrices clarify this. For any AI decision:
- Responsible: Who did the work? Often the AI, with human in oversight role
- Accountable: Who owns the outcome? Must be a human with authority
- Consulted: Who provided input? Legal on compliance, ethics on principles
- Informed: Who needs to know? Executives, affected stakeholders

[Pause]

The critical point: Accountable must always be a human. You can't make an AI accountable. You can hold the person who deployed it, the team who built it, or the executives who approved it accountable.

Your audit trails must enable tracing from agent action to accountable human. Exercise 2.2 includes accountability mapping."

[Transition: Closing]

---

## CLOSING SECTION
### Duration: 3 minutes | Slides 18-20

---

### SLIDE 18: CAPSTONE OVERVIEW

**Title:** Module Capstone: Enterprise AI Governance Framework

**Content:**

**What You'll Create:**
A complete, client-ready enterprise AI governance framework integrating:

**Part 1: Assessment** (from Session 1)
- Governance maturity scores
- Strengths and gaps analysis

**Part 2: Framework Design**
- Organizational structure (centralized/federated)
- Key roles and committees
- Policy framework (from Exercise 2.2)
- Risk framework (NIST/ISO)

**Part 3: Agent Autonomy Classification**
- Five-level classification system
- Current agent inventory with classifications
- Boundaries and escalation triggers

**Part 4: Compliance Roadmap**
- Regulatory requirements mapped (from Exercise 2.1)
- Compliance actions required
- Remediation priorities

**Part 5: Implementation Roadmap**
- Phase 1-3 timelines (12 months)
- Quick wins (30 days)
- Success metrics and KPIs

**Deliverable:** `enterprise-ai-governance-framework.md` (Exercise 2.3)

**Graphic:** Framework structure showing how all components integrate

**SPEAKER NOTES:**

"Your capstone brings everything together.

You're creating a complete Enterprise AI Governance Framework - not a theoretical exercise, but a client-ready deliverable.

Part 1 uses your Session 1 maturity assessment. You're documenting current state - where the organization is today across seven governance domains.

Part 2 is framework design. You'll recommend organizational structure - centralized, federated, or hybrid - with justification. You'll define key roles and committees. You'll include the policies from Exercise 2.2. And you'll specify which risk framework to adopt - NIST AI RMF, ISO 42001, or custom.

Part 3 is agent autonomy classification. You'll document your five-level system and classify actual agents in the organization's inventory.

Part 4 is compliance roadmap from Exercise 2.1. Which regulations apply, what compliance actions are required, what are priorities.

Part 5 is implementation roadmap. How do you move from current state to target state? Phase 1: Foundation (months 1-3), Phase 2: Build (4-6), Phase 3: Operate (7-12). What are quick wins achievable in 30 days? What metrics will you track?

This is what you'd present to an executive team or board. It's comprehensive, actionable, and implementable.

Estimated time: 25 minutes using your prior deliverables. Template provided."

[Transition]

---

### SLIDE 19: RESOURCES

**Title:** Resources for This Week

**Content:**

**Templates & Files:**
- Regulatory Compliance Mapping Template (Exercise 2.1)
- AI Governance Policy Suite Template (Exercise 2.2)
- Enterprise AI Governance Framework Template (Exercise 2.3 Capstone)

**Reference Materials:**
- [EU AI Act Official Text](https://artificialintelligenceact.eu/)
- [SR 11-7 Model Risk Management](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)
- [NYC Local Law 144](https://www.nyc.gov/site/dca/about/automated-employment-decision-tools.page)
- [FDA AI/ML Software as Medical Device](https://www.fda.gov/medical-devices/software-medical-device-samd)

**Support:**
- Questions: [Async channel]
- Exercise review: Post completed capstone for peer feedback

**Graphic:** Resource icons with links

**SPEAKER NOTES:**

"You have comprehensive resources for this week.

Each exercise includes a complete template. Exercise 2.1 regulatory compliance mapping has placeholders for EU AI Act, sector-specific regulations, and data protection requirements. Exercise 2.2 policy suite includes three starter policies. Exercise 2.3 capstone template has all sections outlined.

Reference materials link to actual regulations. EU AI Act official text, SR 11-7 model risk management guidance from Federal Reserve, NYC Local Law 144 for employment AI, FDA guidance on AI as medical device.

If you get stuck, post in [async channel]. I check daily.

For the capstone specifically, consider posting your draft for peer feedback. Your cohort colleagues can provide valuable perspectives."

[Transition]

---

### SLIDE 20: CONCLUSION

**Title:** From Frameworks to Implementation

**Content:**

**What You've Learned:**
- **Session 1:** Governance frameworks, principles, lifecycle, data governance
- **Session 2:** Compliance requirements, organizational structures, autonomy classification, audit trails

**What You've Created:**
- Governance maturity assessment
- Responsible AI principles mapping
- Agent risk register
- Regulatory compliance mapping
- Governance policy suite
- **Enterprise AI Governance Framework (Capstone)**

**Next Steps:**
- Complete Exercise 2.1-2.3 (75 minutes total)
- Review your capstone framework for completeness
- Consider how to apply this in your organization or client work

**Final Message:**
Enterprise AI governance isn't optional anymore. With regulations like EU AI Act, sector requirements, and growing stakeholder expectations, organizations need formal governance frameworks. You now have the knowledge and templates to build them.

**Graphic:** Journey visual showing progression from frameworks to implementation to governance maturity

**SPEAKER NOTES:**

"Let's close.

Over two sessions, you've built comprehensive AI governance capability.

Session 1 gave you the foundations - NIST and ISO frameworks, responsible AI principles, lifecycle governance, data governance. You created maturity assessments, principles mappings, and risk registers.

Session 2 gave you implementation capability - regulatory compliance mapping, organizational design, autonomy classification, audit trail requirements. And you're creating the capstone that brings it all together.

[Pause]

Here's what's changed: Two years ago, AI governance was optional. It was good practice, risk mitigation, reputation protection. Today, it's mandatory in many contexts. EU AI Act creates legal requirements. Financial services regulators demand it. Procurement RFPs require it. Stakeholders expect it.

Organizations need enterprise AI governance frameworks. Not someday - now.

You have the knowledge to build them. You have the templates. You have example deliverables from your exercises.

[Final close]

The exercises this week total 75 minutes. Exercise 2.1 maps compliance requirements. Exercise 2.2 drafts core policies. Exercise 2.3 is your capstone - the complete framework.

This has been an intensive module. You've learned a lot. More importantly, you've created practical deliverables you can use immediately.

Thank you for your engagement. Good luck with the capstone. I look forward to seeing your governance frameworks."

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-02 | Initial presentation created | [Instructor] |
