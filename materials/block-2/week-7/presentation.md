# **PRESENTATION: BLOCK 2 WEEK 7**
## **Block 2 Capstone Preparation**

**Block:** 2: AI Workflow Engineering
**Week:** 7 of 8
**Duration:** 45 minutes
**Theme Color:** Orange (Block 2)

**Key Thesis:** Real execution data transforms workflows from promising prototypes into proven business cases with quantifiable ROI, while comprehensive documentation ensures sustainable impact beyond individual creators.

---

## Slide Overview

| Slide # | Title | Duration | Type |
|---------|-------|----------|------|
| 1 | Title Slide | 0:30 | Title |
| 2 | Week 7 Learning Objectives | 1:00 | Overview |
| 3 | Where We Are in Block 2 | 1:30 | Context |
| 4 | Capstone: Required Components | 3:00 | Requirements |
| 5 | Capstone: Evaluation Criteria | 3:00 | Requirements |
| 6 | Scoring Guide | 2:00 | Requirements |
| 7 | Self-Evaluation Preview | 2:00 | Requirements |
| 8 | Why Real Data Matters | 2:00 | Concept |
| 9 | Data to Collect | 2:30 | Technical |
| 10 | Impact Calculation Walkthrough | 5:00 | Demo |
| 11 | Annual Projection | 3:00 | Calculation |
| 12 | Workflow Documentation Checklist | 3:00 | Checklist |
| 13 | Quality System Documentation | 2:00 | Checklist |
| 14 | MCP Documentation | 2:00 | Checklist |
| 15 | The Documentation Test | 2:00 | Concept |
| 16 | Block 3 Preview | 3:00 | Preview |
| 17 | Prerequisites for Block 3 | 2:00 | Preview |
| 18 | This Week's Exercises | 2:00 | Assignment |
| 19 | Week 8 Preview | 1:30 | Preview |
| 20 | Q&A | - | Interactive |

---

## Slide 1: Title Slide

### Block 2 Capstone Preparation
#### Finalizing Your AI Workflow Toolkit

**Block 2: AI Workflow Engineering**
Week 7 of 8

*[Company/Training Logo]*

**Speaker Notes:**
Welcome to Week 7 - Capstone Preparation. We're in the home stretch. You've built workflows, quality systems, MCP integration. Today we finalize everything for evaluation. We'll review requirements, calculate real impact, and prepare your portfolio. Next week is evaluation week.

---

## Slide 2: Week 7 Learning Objectives

### By the End of This Session, You Will:

1. **Know** all capstone requirements and evaluation criteria
2. **Calculate** real ROI from your workflow executions
3. **Prepare** complete documentation for your portfolio
4. **Understand** what comes in Block 3
5. **Be Ready** for self-evaluation next week

**Speaker Notes:**
These objectives are about synthesis and preparation. No new concepts today - we're bringing everything together. By the end of homework, your capstone should be complete and ready for Week 8 evaluation.

---

## Slide 3: Where We Are in Block 2

### Block 2 Journey

| Week | Focus | Status |
|------|-------|--------|
| Week 1 | Automation Platforms | Complete |
| Week 2 | First Workflow | Complete |
| Week 3 | Quality Systems | Complete |
| Week 4 | Optimization | Complete |
| Week 5 | MCP Integration | Complete |
| Week 6 | Integration Patterns | Complete |
| **Week 7** | **Capstone Prep** | **TODAY** |
| Week 8 | Capstone Evaluation | Next Week |

**Speaker Notes:**
Quick status check: Who has all three workflows working? [Count] Who has at least two? [Count] Who needs support? [Note names] If you're behind, that's your homework priority. We need everything ready for Week 8.

---

## Slide 4: Capstone Required Components

### 5 Required Deliverables

| Component | Description | Location |
|-----------|-------------|----------|
| **3 Workflow Templates** | Documented, tested, integrated | Automation platform |
| **Quality System** | Evaluation prompts, rubrics, gates | GitHub repo |
| **MCP Demonstration** | Functional integration | Claude Desktop |
| **Performance Docs** | Metrics, logs, analysis | GitHub repo |
| **Impact Measurement** | Real data, ROI calculation | Impact report |

### The Complete Package
Your capstone is a professional AI workflow toolkit that others could adopt.

**Speaker Notes:**
These five components make up your capstone. Think of it as a professional toolkit you could hand to a colleague. Workflows do the work. Quality system ensures standards. MCP enables tools. Performance docs show it works. Impact measurement proves the value.

**BACKGROUND:**

**Rationale:**
- This slide defines success criteria clearly - participants know exactly what's required, eliminating ambiguity and anxiety
- The five components represent a complete professional AI toolkit, not just academic exercises
- The "others could adopt" framing emphasizes professional value and sustainability over personal tools

**Key Research & Citations:**
- **Portfolio Assessment Research**: Comprehensive portfolios that demonstrate multiple competencies (technical, quality, documentation, impact) are more valid predictors of professional capability than single-artifact assessments
- **Technology Transfer Literature**: Successful technology implementations require documentation, proven results, and transferable knowledge - exactly what these five components provide
- **Professional Certification Standards**: Industry certifications increasingly emphasize portfolio-based evidence over exam-only approaches, recognizing that demonstrated capability matters more than theoretical knowledge

**Q&A Preparation:**
- *"Do I really need all five components?"*: Yes, for certification. Each serves a distinct purpose. Workflows prove you can build. Quality proves you can ensure standards. MCP proves tool integration. Performance proves it works. Impact proves it matters. Skip one and you have an incomplete toolkit.
- *"What if I only have 2 workflows completed?"*: Quality over quantity - two excellent, well-documented workflows are better than three incomplete ones. Focus on making what you have complete rather than adding partial third workflow.
- *"How detailed do the performance docs need to be?"*: Detailed enough that someone could understand system performance - execution logs, success rates, quality scores, error patterns. Raw data plus brief analysis. Not a research paper, but more than raw numbers.

**Sources:**
- Portfolio Assessment in Higher Education - Cambridge University Press, 2020
- Technology Transfer Best Practices - MIT Sloan, 2023

---

## Slide 5: Capstone Evaluation Criteria

### 8 Dimensions, 40 Points Maximum

| # | Criterion | What's Evaluated | Points |
|---|-----------|------------------|--------|
| 1 | Workflow Design | Architecture, error handling, efficiency | /5 |
| 2 | Integration Quality | Component connections, data flow | /5 |
| 3 | MCP Implementation | Functional use, configuration | /5 |
| 4 | Quality Systems | LLM-as-judge, rubrics, gates | /5 |
| 5 | Performance Monitoring | Logging, metrics, analysis | /5 |
| 6 | Documentation | All required docs present, clear | /5 |
| 7 | Practical Application | Real use cases addressed | /5 |
| 8 | Overall Toolkit Value | Holistic assessment | /5 |

**Speaker Notes:**
You'll be evaluated on eight dimensions. Each is worth 5 points, for 40 total. Notice these cover technical quality, documentation, and practical value. You'll score yourself on all eight next week. Honest self-assessment is the goal.

---

## Slide 6: Scoring Guide

### How Scores Translate

| Score Range | Level | Meaning |
|-------------|-------|---------|
| **32-40** | Excellent | Strong mastery, ready for Block 3 |
| **28-31** | Proficient | Solid competency, minor gaps |
| **24-27** | Developing | Meets requirements, room to grow |
| **Below 24** | Needs Work | Additional development required |

### Per-Criterion Scoring

| Score | Description |
|-------|-------------|
| 5 | Exceptional - Exceeds expectations |
| 4 | Strong - Meets all requirements well |
| 3 | Adequate - Meets basic requirements |
| 2 | Developing - Partial completion |
| 1 | Beginning - Minimal evidence |

**Speaker Notes:**
These thresholds guide your self-evaluation. Be honest. A 3 means you met requirements - that's not bad. A 5 means you exceeded expectations. Most people land in the 28-35 range. The goal is accurate self-assessment, not maximum scores.

---

## Slide 7: Self-Evaluation Preview

### Week 8 Process

1. **Score yourself** on all 8 criteria
2. **Provide evidence** for each score
3. **Reflect** on your learning journey
4. **Identify** areas for continued growth
5. **Plan** for Block 3

### Evidence Examples

| Criterion | Evidence Type |
|-----------|---------------|
| Workflow Design | Screenshot + architecture diagram |
| Integration Quality | Data flow documentation |
| MCP Implementation | Configuration file + usage example |
| Quality Systems | Rubric + sample evaluation |
| Performance | Execution logs + metrics |
| Documentation | Complete repo contents |
| Practical Application | Real use case description |
| Overall Value | Impact report summary |

**Speaker Notes:**
Next week you'll score yourself. Each score needs evidence - don't just claim a 5, show why it's a 5. This is professional self-assessment. It's a skill you'll use throughout your career. Come prepared with links and examples.

---

## Slide 8: Why Real Data Matters

### Projections vs. Actuals

| Projections | Actuals |
|-------------|---------|
| "Could save 10 hours/month" | "Saved 16.7 hours this month" |
| "Might improve quality" | "Quality scores average 4.2/5" |
| "Should have good ROI" | "Achieved 71,000% ROI" |

### Real Data Tells the Story

- **Stakeholders trust numbers** over estimates
- **You can learn** from actual performance
- **Future projections** are more credible with a baseline
- **Your business case** becomes undeniable

**Speaker Notes:**
Real data is your superpower. When you tell your manager "I saved 16 hours and it cost $3.50," that's a story they remember. Projections are ignored. Actuals get attention. This is the skill that justifies AI investment.

**BACKGROUND:**

**Rationale:**
- This slide shifts participants from builder mindset to business leader mindset - proving value, not just creating tools
- Real data transforms AI from "interesting technology" to "justified business investment" in organizational contexts
- The comparison makes viscerally clear why measurement matters for professional credibility

**Key Research & Citations:**
- **Evidence-Based Management**: Business research shows that data-driven decisions are 5-6x more likely to gain executive approval than intuition-based proposals. Actual results eliminate subjective debate.
- **Technology ROI Literature**: Studies of IT project success show that projects with measured baselines and tracked outcomes are 3x more likely to receive continued funding and expansion approval
- **Change Management Research**: Organizational change adoption correlates strongly with demonstrated value. Projections create skepticism; actuals create champions.

**Q&A Preparation:**
- *"What if I only have a few executions - is that enough data?"*: Even 5-10 executions provide real data points. Small sample with actual numbers beats no data. Document sample size and note it will get stronger with more executions.
- *"What if my results aren't impressive?"*: Report honestly. If you saved 2 hours at $5 cost, that's still a positive ROI story. Small wins compound. Learning what didn't work is also valuable data.
- *"How do I measure quality improvement if I don't have 'before' baseline?"*: Use your Block 1 self-assessment or stakeholder feedback as qualitative baseline. Quantitative quality scores from Week 3 forward. Note limitation but report what you have.

---

## Slide 9: Data to Collect

### From Your Automation Platform

| Data Point | Where to Find |
|------------|---------------|
| Total executions | Execution history |
| Execution timestamps | Execution logs |
| Duration per execution | Execution details |
| Success/failure rates | Status counts |
| Error details | Failed execution logs |

### From Your Quality System

| Data Point | Where to Find |
|------------|---------------|
| Quality scores | Evaluation outputs |
| Pass rates | Router metrics |
| Review rates | Review queue |
| Revision rates | Tracked revisions |

### From Block 1

| Data Point | Purpose |
|------------|---------|
| Manual process time | Baseline for comparison |
| Previous quality level | Quality improvement measure |

**Speaker Notes:**
Pull this data before you start calculations. Execution history in Make or n8n has most of what you need. Quality scores may be in your logs or router. Block 1 baseline is from your memory or records - estimate if needed, but document your estimation method.

**BACKGROUND:**

**Rationale:**
- This slide provides concrete, actionable guidance for data collection - participants know exactly where to look
- The three-source structure (platform, quality system, Block 1 baseline) ensures comprehensive measurement
- Documenting estimation methods for baseline data teaches professional data integrity practices

**Key Research & Citations:**
- **Evidence-Based Performance Measurement**: Comprehensive impact measurement requires both outcome data (time saved, quality scores) and baseline data (previous state). Without baseline, improvement claims lack credibility.
- **Instrumentation in Software Systems**: Modern automation platforms automatically log execution data - leveraging this instrumentation is best practice for performance monitoring and debugging
- **Data Quality Standards**: When exact historical data is unavailable, documented conservative estimates are acceptable in professional contexts. The key is transparency about methodology.

**Q&A Preparation:**
- *"What if I didn't track manual process time in Block 1?"*: Estimate conservatively now. Time yourself doing the task manually once or twice, average it, document that it's an estimate. Conservative estimates maintain credibility.
- *"How far back should I pull execution data?"*: All of it, or at minimum the last month. More data = more credible analysis. If you have 100 executions, use all 100. If you have 10, use 10 but note sample size.
- *"What if some of my quality scores are missing?"*: Use what you have and note the gap. If you have scores for 60% of executions, analyze those 60% and document that 40% weren't evaluated. Completeness is ideal, but partial data with transparency is acceptable.

**Sources:**
- Performance Measurement in IT Systems - Gartner Research, 2024
- Data Quality Framework - Data Management Association, 2023

---

## Slide 10: Impact Calculation Walkthrough

### Live Calculation Example

**Time Savings:**
```
Manual process time: 45 minutes
Automated process time: 5 minutes
Savings per execution: 40 minutes
Number of executions: 25
Total time saved: 25 x 40 = 1000 min = 16.7 hours
```

**Cost Analysis:**
```
AI API costs: $3.50
Platform costs: $0.00 (free tier)
Total cost: $3.50
```

**ROI Calculation:**
```
Time saved: 16.7 hours
Value at $150/hr: $2,505
Cost: $3.50
Net value: $2,501.50
ROI: $2,501.50 / $3.50 = 71,471%
```

**Speaker Notes:**
[Walk through this calculation live with actual or realistic numbers.] Notice we're using real execution data, not projections. The ROI looks massive because knowledge work is expensive and AI is cheap. Even with smaller numbers, you'll see significant ROI.

**BACKGROUND:**

**Rationale:**
- This walkthrough makes ROI calculation concrete and replicable - participants can apply this exact methodology to their own data
- The massive ROI numbers (often 1000%+) are real but need explanation to avoid sounding implausible
- Live calculation demonstrates that impact measurement is accessible, not requiring advanced analytics skills

**Key Research & Citations:**
- **Knowledge Work Economics**: Consulting/professional services typically value at $100-300/hour. Even simple automation creating 30 minutes of savings generates $50-150 value per execution. At scale, this compounds rapidly.
- **AI Cost Efficiency Research (2024)**: Current LLM API costs are 1/100th to 1/1000th the cost of equivalent human time. This fundamental economics drives the massive ROI numbers seen in real deployments.
- **Time Value in Automation**: McKinsey research shows that knowledge workers spend 20-30% of time on repetitive tasks amenable to automation. Capturing even a portion of this time creates substantial value.

**Q&A Preparation:**
- *"These ROI numbers seem too good to be true - are they realistic?"*: Yes, when you're automating knowledge work. The math is simple: expensive human time ($100-300/hr) vs. cheap AI cost ($0.01-0.10/task). The 1000%+ ROI is real because the cost differential is so extreme.
- *"How do I justify my hourly rate in the calculation?"*: Use your actual billing rate (for consultants) or standard industry rate for your role (for internal staff). Be conservative - if unsure, use $100/hr for professional work. Document your assumption.
- *"What if leadership questions my time savings estimate?"*: Use time logs or before/after comparisons. If you estimated, document method and offer to track precisely going forward. Conservative estimates build credibility.

---

## Slide 11: Annual Projection

### Scaling Your Impact

**From Actual to Annual:**
```
Current weekly rate: 6 executions
Annual executions: 6 x 52 = 312
Annual time saved: 312 x 40 min = 208 hours
Annual value: 208 x $150 = $31,200
Annual cost: $3.50 x 12 = $42
Annual net value: $31,158
```

**Team Scale Potential:**
```
Team size: 5 people
Team annual value: $31,158 x 5 = $155,790
```

### The Business Case Writes Itself

When you can show $31K annual value for $42 cost, approvals are easy.

**Speaker Notes:**
Annual projections are credible when based on actual data. Note we're multiplying your real weekly rate, not an optimistic guess. Team scale shows the potential. This is how you justify AI investments to leadership.

**BACKGROUND:**

**Rationale:**
- Annual projection transforms short-term results into strategic business case - essential for securing ongoing investment
- Team scaling calculation demonstrates how individual productivity gains multiply across organizations
- The dramatic ROI numbers (often 500-1000x) are real and credible when based on actual execution data

**Key Research & Citations:**
- **Business Case Development**: Professional business cases require annualized projections to account for seasonal variation and demonstrate sustained value. One-time savings are tactical; annualized value is strategic.
- **Scalability Economics in AI**: Research shows that AI workflow value scales linearly with user adoption (5 users = 5x value) while costs scale sub-linearly (shared infrastructure, one-time development). This creates favorable economics for team-wide deployment.
- **Executive Decision-Making Research**: Studies show executives approve investments based on annualized ROI projections, not point-in-time results. The annual framing aligns with how budgets are planned and justified.

**Q&A Preparation:**
- *"Should I include growth assumptions in annual projections?"*: Be conservative - use current execution rate unless you have specific growth plans. If you project growth, show two scenarios: current rate and projected growth, with assumptions documented.
- *"What if my execution rate varies week to week?"*: Use average weekly rate from your data. If you have 8 weeks of data with 30 total executions, that's 3.75/week average. Multiply by 52 for annual. Document the variability in your report.
- *"How do I estimate team scale value?"*: If others could use your workflow as-is, multiply by potential users. If workflows need customization per person, be more conservative. Document assumptions about which workflows are team-scalable versus personal.

**Sources:**
- Business Case Development Guide - Harvard Business Review, 2023
- Scaling AI in Organizations - McKinsey Global Institute, 2024

---

## Slide 12: Workflow Documentation Checklist

### For Each Workflow (3 total)

| Document | Purpose | Complete? |
|----------|---------|-----------|
| Architecture diagram | Visual overview | |
| Configuration guide | How to set up | |
| Integration dependencies | What's required | |
| Troubleshooting guide | How to fix issues | |
| Performance benchmarks | Expected metrics | |
| Example executions | Sample outputs | |

### The Handoff Test

> "Could someone else set up and maintain this workflow based on your documentation alone?"

If yes, you're done. If no, add more detail.

**Speaker Notes:**
Each workflow needs complete documentation. The handoff test is your quality check. If your documentation fails the handoff test, you own the workflow forever. Good documentation means freedom.

---

## Slide 13: Quality System Documentation

### Quality Components

| Document | Contents | Complete? |
|----------|----------|-----------|
| Evaluation prompts | All prompts used for evaluation | |
| Quality rubrics | Scoring criteria for each workflow | |
| Routing logic | How scores determine next steps | |
| Human review process | Queue, interface, feedback loop | |

### Organization Suggestion

```
/quality-system/
  /prompts/
    evaluation-wf1.md
    evaluation-wf2.md
    evaluation-wf3.md
  /rubrics/
    rubric-wf1.md
    rubric-wf2.md
    rubric-wf3.md
  routing-logic.md
  human-review-process.md
```

**Speaker Notes:**
Your quality system documentation proves you built systematic evaluation, not just one-off checks. Include all prompts, rubrics, and the routing logic. This is what makes your workflows production-grade.

---

## Slide 14: MCP Documentation

### MCP Configuration

| Document | Contents | Complete? |
|----------|----------|-----------|
| Configuration file | Sanitized config (no tokens) | |
| Setup instructions | How to configure | |
| Usage examples | How to use in practice | |
| Security notes | Token management, permissions | |

### Example Structure

```markdown
# MCP Configuration Guide

## Prerequisites
- Claude Desktop installed
- Node.js installed
- GitHub account with token

## Configuration File
Location: [path]
\`\`\`json
{
  "mcpServers": {
    "filesystem": { ... },
    "github": { ... }
  }
}
\`\`\`

## Usage Examples
- Reading templates: "Read my prompt template from GitHub"
- Accessing files: "List files in my prompts directory"
```

**Speaker Notes:**
MCP documentation shows you can configure AI tools, not just use them. Sanitize the config - remove actual tokens. Focus on explaining how someone else could replicate your setup.

---

## Slide 15: The Documentation Test

### Three Questions

1. **Setup:** Could someone else configure your system?
2. **Usage:** Could someone else run your workflows?
3. **Maintenance:** Could someone else fix issues?

### If Any Answer Is "No"

Add more documentation to that area:
- More screenshots
- More step-by-step instructions
- More troubleshooting tips
- More examples

### The Standard

> "If I go on vacation (or leave the company), could a colleague take over?"

**Speaker Notes:**
Apply these three questions to every component. Setup, usage, maintenance. Documentation that fails any of these tests is incomplete. This isn't busywork - it's professional responsibility. Well-documented work has lasting impact.

**BACKGROUND:**

**Rationale:**
- This "documentation test" provides a simple, actionable quality standard that participants can apply immediately
- It reframes documentation from compliance burden to professional value creator
- The vacation scenario makes the need personal and viscerally clear - everyone has been stuck supporting undocumented systems

**Key Research & Citations:**
- **Bus Factor in Software Engineering**: The "bus factor" measures how many people need to be unavailable before a project fails. Documentation is the primary factor increasing this number from 1 to many.
- **Knowledge Transfer Research**: Studies show that well-documented systems reduce onboarding time by 60-80% and reduce support burden on original creators by 70%+
- **Professional Services Economics**: Undocumented work creates key-person dependency, reducing business value. Documented, transferable work creates scalable service offerings with higher valuations.

**Q&A Preparation:**
- *"How much documentation is enough?"*: Apply the three questions. If a competent colleague could setup, use, and maintain based on your docs, you're done. If not, add what's missing. Quality over quantity.
- *"What if I'm the only one who will use this?"*: You in 6 months is a different person. You'll forget details. Documentation is for future-you as much as future-colleagues. Plus, career changes happen - make your work transferable.
- *"Isn't this overkill for a personal tool?"*: If it's truly personal and low-value, maybe. But if you're presenting it as a capstone demonstrating professional skills, it needs professional documentation standards. Documentation IS a deliverable skill.

---

## Slide 16: Block 3 Preview

### Block 3: AI Automation Architecture

**What You'll Build:**
- Reliable AI agents with guardrails
- Multi-agent orchestration patterns
- Production deployment systems
- Team-scale AI workflows

**The Progression:**
- Block 1: Prompt engineering (the brain)
- Block 2: Workflow automation (the body)
- Block 3: Agent architecture (autonomous systems)

### Your Workflows Become Building Blocks

The workflows you built in Block 2 become components that agents orchestrate in Block 3.

**Speaker Notes:**
Block 3 takes everything you've built and makes it autonomous. Your workflows become the building blocks that agents coordinate. MCP becomes how agents access tools. Quality systems become agent guardrails. Everything connects.

---

## Slide 17: Prerequisites for Block 3

### Required to Enroll

| Prerequisite | What It Means |
|--------------|---------------|
| Block 2 completion | Capstone submitted and evaluated |
| Strong workflow foundation | 3 working workflows |
| MCP proficiency | Configured and functional |
| Quality system thinking | LLM-as-judge implemented |

### What You're Bringing to Block 3

- Experience building automation
- Understanding of quality gates
- MCP configuration skills
- Documentation practice
- Impact measurement ability

### Enrollment Information

[Share timing, registration details, contact]

**Speaker Notes:**
Block 3 requires Block 2 completion - that's why we're finalizing your capstone now. Everything you've built carries forward. The skills become prerequisites. If you're interested in Block 3, complete your capstone well.

---

## Slide 18: This Week's Exercises

### Homework (60 minutes minimum)

**Exercise 7.1: Finalize All Workflows (Variable)**
- Review each workflow against requirements
- Run 3+ final tests per workflow
- Polish all documentation
- Deliverable: 3 finalized workflows

**Exercise 7.2: Calculate Real Impact (25 min)**
- Export execution data
- Calculate actual metrics
- Create impact report
- Deliverable: `block-2-impact-report.md`

**Exercise 7.3: Create Capstone Summary (10 min)**
- Document all components
- Link everything together
- Deliverable: `block-2-capstone-summary.md`

**Speaker Notes:**
This homework is about completion, not learning. Exercise 7.1 may take longer if you have gaps to fill. Do it. The impact report in 7.2 is your business case. The summary in 7.3 ties everything together. Spend what time you need.

---

## Slide 19: Week 8 Preview

### Capstone Evaluation Week

**What Happens:**
- Self-evaluation on all 8 criteria
- Evidence documentation
- Reflection on learning journey
- Block 3 planning

**What to Prepare:**
- Complete capstone (all components)
- Evidence for each evaluation criterion
- Honest self-assessment mindset
- Thoughts on Block 3 goals

### No New Exercises

Week 8 is evaluation only. All work should be complete.

**Speaker Notes:**
Week 8 is the finish line. No new exercises - just evaluation. Come with your complete capstone and evidence for self-assessment. Be honest in your scoring. This is about learning, not grades.

---

## Slide 20: Q&A

### Questions?

**Common Questions:**

- "What if I don't have enough executions?"
  - Use what you have. Even 10 is valuable data.

- "How honest should self-evaluation be?"
  - Completely. Evidence should support scores.

- "What if one workflow isn't working?"
  - Quality over quantity. Two excellent beats three incomplete.

**Resources:**
- Capstone rubric (provided)
- Impact report template (participant guide)
- Documentation examples (GitHub)

**Next Week:** Capstone Evaluation

**Speaker Notes:**
Take questions. Common concerns: not enough data (use what you have), self-evaluation honesty (be accurate), incomplete work (quality over quantity). Emphasize that support is available - reach out now, not next week.

---

## Appendix: Impact Report Template

### Quick Reference

```markdown
# Block 2 Impact Report

## Execution Summary
- Total executions: [N]
- Success rate: [X]%

## Time Savings
| Workflow | Manual | Automated | Saved | Count | Total |
|----------|--------|-----------|-------|-------|-------|
| WF1 | X min | X min | X min | N | X hr |
| WF2 | | | | | |
| WF3 | | | | | |
| **Total** | | | | | **X hr** |

## Cost Analysis
- AI costs: $X
- Platform costs: $X
- **Total cost:** $X
- **Value created:** $X
- **ROI:** X%

## Annual Projection
- Annual value: $X
- Annual cost: $X
- **Net annual ROI:** $X
```

---

## Appendix: Capstone Summary Template

### Quick Reference

```markdown
# Block 2 Capstone Summary

## Participant: [Name]
## Date: [Date]

### Workflows
1. [Name] - [Purpose] - [Link]
2. [Name] - [Purpose] - [Link]
3. [Name] - [Purpose] - [Link]

### Quality System
- [Description] - [Link]

### MCP Implementation
- [Description] - [Link]

### Impact Summary
- Time saved: X hours
- ROI: X%
- Annual projection: $X

### Repository: [URL]
```

---

## Appendices

### Appendix D: Impact Report Template

```markdown
# Block 2 Impact Report

**Participant:** [Your Name]
**Date:** [Submission Date]
**Program:** AI Practitioner Training - Block 2

## Executive Summary
[2-3 sentences summarizing total impact]

## Workflows Analyzed

### Workflow 1: [Name]
- **Purpose:** [What it does]
- **Pattern:** [Sequential/Parallel/Iterative/HITL]
- **Executions:** [Total count]
- **Time Period:** [Date range]

### Workflow 2: [Name]
[Same structure]

### Workflow 3: [Name]
[Same structure]

## Time Savings Analysis

| Workflow | Manual Time | Automated Time | Saved/Execution | Count | Total Saved |
|----------|-------------|----------------|-----------------|-------|-------------|
| WF1 | X min | X min | X min | N | X hrs |
| WF2 | X min | X min | X min | N | X hrs |
| WF3 | X min | X min | X min | N | X hrs |
| **TOTAL** | | | | **N** | **X hrs** |

**Methodology:**
- Manual time: [How estimated/measured]
- Automated time: [From execution logs]

## Cost Analysis

### API Costs
- Total AI API calls: [N]
- Cost per call: $[X]
- Total AI cost: $[X]

### Platform Costs
- Automation platform: $[X] ([free tier/paid plan])
- Other services: $[X]
- **Total cost:** $[X]

## Quality Impact

### Quality Scores
- Average quality score: X.X/5.0
- Pass rate: X%
- Human review rate: X%

### Quality Improvement
- Block 1 baseline: [Description of manual quality]
- Block 2 automated: [Quality score data]
- Improvement: [Qualitative or quantitative]

## ROI Calculation

### Direct ROI
- **Time value created:** X hours × $X/hour = $[X]
- **Total cost:** $[X]
- **Net value:** $[X]
- **ROI:** [X]%

### Annual Projection
- Current weekly rate: [X] executions/week
- Annual executions: [X] × 52 = [X]
- Annual time saved: [X] hours
- Annual value: $[X]
- Annual cost: $[X]
- **Annual net value:** $[X]

### Team Scale Potential
- Applicable workflows: [Which ones scale]
- Potential team users: [N] people
- **Team annual value:** $[X] × [N] = $[X]

## Key Findings

1. [Finding 1]
2. [Finding 2]
3. [Finding 3]

## Lessons Learned

### What Worked Well
- [Lesson 1]
- [Lesson 2]

### What I'd Improve
- [Improvement 1]
- [Improvement 2]

## Supporting Data
- Execution logs: [Link to file/folder]
- Quality evaluations: [Link]
- Configuration files: [Link]
```

### Appendix E: Capstone Summary Template

```markdown
# Block 2 Capstone Summary

**Participant:** [Your Name]
**Submission Date:** [Date]
**GitHub Repository:** [URL]

## Overview
[2-3 sentence summary of your toolkit]

## Component Checklist

### 1. Workflows (3 Required)

**Workflow 1: [Name]**
- Purpose: [What it does]
- Pattern: [Integration pattern used]
- Status: [Complete/In Progress]
- Link: [URL to documentation]
- Executions: [Count]

**Workflow 2: [Name]**
[Same structure]

**Workflow 3: [Name]**
[Same structure]

### 2. Quality System
- Evaluation prompts: [Link]
- Quality rubrics: [Link]
- Routing logic: [Link]
- Human review process: [Link]
- Status: [Complete/In Progress]

### 3. MCP Implementation
- Configuration documented: [Yes/No]
- Servers configured: [filesystem, github, other]
- Usage examples provided: [Yes/No]
- Link: [URL to MCP docs]
- Status: [Complete/In Progress]

### 4. Performance Documentation
- Execution logs: [Link]
- Success/failure metrics: [Link]
- Performance analysis: [Link]
- Status: [Complete/In Progress]

### 5. Impact Measurement
- Impact report completed: [Yes/No]
- Real execution data: [Yes/No]
- ROI calculated: [Yes/No]
- Link: [URL to impact report]
- Status: [Complete/In Progress]

## Key Metrics Summary

- **Total executions:** [N]
- **Time saved:** [X] hours
- **ROI:** [X]%
- **Average quality score:** X.X/5.0
- **Annual projected value:** $[X]

## Repository Structure

```
/workflows/
  workflow-1-[name].md
  workflow-2-[name].md
  workflow-3-[name].md
/quality-system/
  /prompts/
  /rubrics/
  routing-logic.md
/mcp/
  configuration-guide.md
  usage-examples.md
/performance/
  execution-logs.csv
  performance-analysis.md
/impact/
  impact-report.md
README.md
integration-architecture.md
```

## Readiness for Block 3

- [ ] All components complete
- [ ] Documentation passes handoff test
- [ ] Real execution data collected
- [ ] Impact quantified
- [ ] Ready for self-evaluation
```

### Appendix F: Data Collection Checklist

**From Automation Platform:**
- [ ] Export execution history (CSV or JSON)
- [ ] Document total execution count
- [ ] Calculate success rate
- [ ] Identify failure patterns
- [ ] Extract execution timestamps
- [ ] Record average execution duration

**From Quality System:**
- [ ] Compile quality scores (all executions)
- [ ] Calculate average quality score
- [ ] Determine pass rate
- [ ] Determine review rate
- [ ] Document revision patterns
- [ ] Capture evaluation reasoning

**From Your Records:**
- [ ] Estimate manual process time (or measure)
- [ ] Document estimation methodology
- [ ] Note Block 1 quality baseline
- [ ] Record any stakeholder feedback

**For Cost Analysis:**
- [ ] Count total AI API calls
- [ ] Calculate API costs (check platform dashboard)
- [ ] Document platform subscription cost
- [ ] Note any other service costs

**For Team Scale Analysis:**
- [ ] Identify which workflows are team-scalable
- [ ] Estimate potential user count
- [ ] Document assumptions about adoption

### Appendix G: ROI Calculation Formulas

**Time Savings:**
```
Time_Saved_Per_Execution = Manual_Time - Automated_Time
Total_Time_Saved = Time_Saved_Per_Execution × Execution_Count
```

**Cost Calculation:**
```
Total_Cost = API_Costs + Platform_Costs + Other_Costs
```

**Value Calculation:**
```
Hourly_Rate = [Your billing rate or standard professional rate]
Time_Value_Created = Total_Time_Saved_Hours × Hourly_Rate
```

**ROI:**
```
Net_Value = Time_Value_Created - Total_Cost
ROI_Percentage = (Net_Value / Total_Cost) × 100
```

**Annual Projection:**
```
Weekly_Execution_Rate = Total_Executions / Weeks_Of_Data
Annual_Executions = Weekly_Execution_Rate × 52
Annual_Time_Saved = Annual_Executions × Time_Saved_Per_Execution
Annual_Value = Annual_Time_Saved_Hours × Hourly_Rate
Annual_Cost = (Total_Cost / Execution_Count) × Annual_Executions
Annual_Net_Value = Annual_Value - Annual_Cost
```

**Team Scale:**
```
Team_Annual_Value = Annual_Net_Value × Number_Of_Team_Members
```

### Appendix H: Self-Evaluation Evidence Guide

**For Each Evaluation Criterion, Provide:**

**1. Workflow Design (/5)**
- Evidence: Links to workflow documentation
- Screenshots of workflow architecture
- Error handling examples
- Complexity appropriate to use case

**2. Integration Quality (/5)**
- Evidence: Integration architecture diagram
- Data flow documentation
- Component connection points
- API/tool integrations working

**3. MCP Implementation (/5)**
- Evidence: Configuration file (sanitized)
- Usage examples/screenshots
- Templates accessed via MCP
- Functional demonstration

**4. Quality Systems (/5)**
- Evidence: Evaluation prompts
- Quality rubrics
- Routing logic documentation
- Sample evaluations with scores

**5. Performance Monitoring (/5)**
- Evidence: Execution logs
- Success/failure metrics
- Performance analysis
- Trend documentation

**6. Documentation (/5)**
- Evidence: Complete repository structure
- README present and comprehensive
- All components documented
- Passes handoff test

**7. Practical Application (/5)**
- Evidence: Real use cases addressed
- Execution data (not test runs)
- Stakeholder value demonstrated
- Business impact clear

**8. Overall Toolkit Value (/5)**
- Evidence: Integration architecture
- Complete portfolio
- Impact report
- Professional presentation

### Appendix I: Documentation Handoff Test Questions

**Setup Questions:**
1. Could someone find and access your configuration files?
2. Are all prerequisites documented?
3. Are credentials/access requirements clear?
4. Could someone set up your workflow from scratch?

**Usage Questions:**
1. Is it clear how to trigger workflows?
2. Are input requirements documented?
3. Are expected outputs specified?
4. Could someone run your workflow successfully?

**Maintenance Questions:**
1. Are common errors and solutions documented?
2. Is troubleshooting guidance provided?
3. Are component dependencies clear?
4. Could someone fix issues without asking you?

**Quality Standard:**
If you answered "yes" to 10+ of 12 questions, documentation passes handoff test.
If you answered "no" to 3+ questions, documentation needs improvement.

---

**Slide Type Definitions:** TITLE SLIDE | PROBLEM STATEMENT | INSIGHT / REVELATION | CONCEPT INTRODUCTION | FRAMEWORK / MODEL | COMPARISON | DEEP DIVE | CASE STUDY | PATTERN / BEST PRACTICE | METRICS / DATA | ARCHITECTURE / DIAGRAM | OBJECTION HANDLING | ACTION / NEXT STEPS | SUMMARY / RECAP | SECTION DIVIDER | CLOSING / CALL TO ACTION | Q&A / CONTACT

**Content Guidelines:** Use parallel structure in bullets | Clear tables with headers | Bad/Good example contrasts | Key principle callouts | Stage directions in speaker notes `[Pause]` `[Point to X]` `[Emphasize]`

**Block 2 Orange Theme:** Primary Orange (#FF6B35) for branding | Accent blues/grays | Orange highlights for emphasis | Consistent color across all Block 2 presentations

**Quality Checklist:** Learning objectives align | Key Thesis clear | Complete speaker notes | Technical accuracy | Relevant examples | Research citations specific | Q&A addresses objections | Orange theme consistent | Progressive building | Realistic timing

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2025-01-01 | Initial presentation created | Training Team |
| 2.0 | 2026-01-03 | Enhanced with comprehensive slide structure, BACKGROUND sections, Sources, Implementation Guidance, and expanded appendices | Claude |

---

**Navigation:** [<- Week 6 Presentation](../week-6/presentation.md) | **Week 7** | [Week 8 Presentation ->](../week-8/presentation.md)
