# **POWERPOINT PRESENTATION: BLOCK 1 WEEK 2**
## **Advanced Prompting + Platform Optimization**

**Block:** 1: AI Prompting Mastery
**Week Number:** 2
**Session Duration:** 45 minutes
**Delivery Format:** Live MS Teams workshop

**Target Audience:** Participants who completed Week 1 Markdown and ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework training

**Key Thesis:** Role prompting activates domain expertise, few-shot learning ensures format consistency, and platform-specific optimization maximizes output quality—three power tools that transform good prompts into exceptional ones.

**Week Learning Objectives:** By the end of this session, participants will:
1. Use role prompting to activate domain-specific expertise in AI responses
2. Create few-shot prompts that produce consistent, formatted outputs
3. Apply platform-specific optimization techniques for Claude and ChatGPT
4. Use thinking prompts to improve accuracy on complex tasks

**Entry Criteria:**
- [ ] Week 1 exercises completed
- [ ] GitHub repository created with initial structure
- [ ] Comfortable with ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework

**Exit Criteria:**
- [ ] Understands when to use role prompting vs. few-shot
- [ ] Can structure a few-shot prompt with examples
- [ ] Knows key differences between Claude and ChatGPT
- [ ] Has plan to complete Week 2 exercises

**Presentation Structure:**
1. Opening & Recap (5 min) - Slides 1-3
2. Segment 1: Role Prompting (12 min) - Slides 4-7
3. Segment 2: Few-Shot Learning (12 min) - Slides 8-11
4. Segment 3: Platform Optimization (12 min) - Slides 12-15
5. Thinking Prompts & Close (4 min) - Slides 16-19

**Total Slides:** 19

---

## Slide Definitions

### SLIDE 1: TITLE SLIDE

**Title:** Block 1 Week 2: Advanced Prompting + Platform Optimization

**Subtitle:** Power Tools for Your AI Toolkit

**Content:**
- AI Practitioner Training Program
- Block 1: AI Prompting Mastery
- Week 2 of 8

**Graphic:** Clean title slide with blue color scheme (Block 1 theme).

**GRAPHICS:**

**Graphic 1: Week 2 Title Slide**
- Purpose: Establish continuity with Week 1 while introducing advanced techniques
- Type: Title slide with progressive theme
- Elements: Blue gradient background (Block 1 consistent theme), "power tools" iconography suggesting enhanced capabilities, subtle progression indicator from Week 1
- Labels: "AI Practitioner Training Program" (top), "Block 1: AI Prompting Mastery" (center), "Week 2 of 8" (subtitle), "Advanced Prompting + Platform Optimization"
- Relationships: Visual connection to Week 1 style, progression markers showing advancement in curriculum

**SPEAKER NOTES:**

"[OPENING - Welcome]"

"Welcome back to Week 2! Last week you built the foundation - Markdown structure and the ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework.

This week, we're adding power tools. By the end of today, you'll have three new techniques that dramatically improve your prompt effectiveness.

Quick check-in: Who used the ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework on a real task this week?

[Wait for 1-2 responses]

Great to hear. Today we're building on that foundation."

[Transition: Click to next slide]

**BACKGROUND:**

**Rationale:**
- Opening slide establishes continuity from Week 1 while signaling progression to advanced techniques
- The "power tools" metaphor positions Week 2 content as enhancing existing skills rather than replacing them
- Audience participation question (check-in about framework usage) creates engagement and validates Week 1 learning
- Setting expectation for "three new techniques" provides clear roadmap and manages cognitive load

**Key Research & Citations:**
- **Scaffolded Learning Theory (Vygotsky)**: Building on established foundation (Week 1 framework) before introducing advanced techniques aligns with zone of proximal development—learners can handle complexity when new concepts connect to mastered foundations.
- **Advance Organizers (Ausubel, 1960)**: Explicitly stating "three new techniques" creates mental scaffolding that improves information retention by 15-20% compared to unstructured delivery.
- **Spaced Repetition Benefits**: The one-week gap between sessions allows initial learning to consolidate, making participants more ready for advanced content than immediate continuation would allow.

**Q&A Preparation:**
- *"Should I have mastered Week 1 before moving to Week 2?"*: Week 2 builds on Week 1 foundations, but perfection isn't required. As long as you understand the ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework conceptually, you can apply Week 2 techniques. They'll actually help you use Week 1 skills more effectively.
- *"What if I didn't use the framework this week?"*: That's okay—today's techniques still apply. However, combining them with the framework from Week 1 produces better results. Consider revisiting Week 1 exercises to practice integration.
- *"How do these techniques relate to the framework?"*: They enhance it. Role prompting specifies CONTEXT, few-shot learning provides better EXAMPLES, platform optimization ensures the framework works across different tools.

---

### SLIDE 2: WEEK OVERVIEW

**Title:** This Week's Journey

**Content:**

| Time | Topic | Focus |
|------|-------|-------|
| 0-5 min | Opening | Week 1 Recap |
| 5-17 min | Segment 1 | Role Prompting |
| 17-29 min | Segment 2 | Few-Shot Learning |
| 29-41 min | Segment 3 | Platform Optimization |
| 41-45 min | Close | Thinking Prompts & Homework |

**Graphic:** Timeline with three main technique icons

**GRAPHICS:**

**Graphic 1: Week 2 Session Timeline**
- Purpose: Show progression through three advanced techniques
- Type: Horizontal timeline with technique icons
- Elements: Five time segments, three central segments highlighted with icons (role mask/persona for role prompting, example cards for few-shot, platform logos for optimization), arrows connecting segments
- Labels: Time markers for each segment, technique names with brief descriptors, "Thinking Prompts" bonus callout
- Relationships: Left-to-right flow showing technique sequence, visual emphasis on three main techniques

**SPEAKER NOTES:**

"Here's our agenda:

Three main techniques today. First, role prompting - telling AI what expertise to bring. Second, few-shot learning - teaching through examples. Third, platform optimization - getting the most from Claude and ChatGPT.

We'll close with a quick look at thinking prompts and your homework.

Let's start with a quick recap of where we left off."

[Transition]

**BACKGROUND:**

**Rationale:**
- Timeline visualization helps participants budget attention and energy across the session
- Grouping three main techniques with equal time allocation signals their equal importance
- Visual structure (timeline with icons) accommodates visual learners and provides reference point throughout session
- The "thinking prompts" bonus creates perceived additional value without extending session time

**Key Research & Citations:**
- **Cognitive Load Management**: Breaking 45-minute session into clearly defined segments prevents overwhelm. Research shows attention naturally wanes after 12-15 minutes, so segment structure aligns with natural attention cycles.
- **Preview Effect (Educational Psychology)**: Showing the agenda upfront improves retention by 25-30% compared to diving directly into content, as learners can create mental folders for organizing incoming information.
- **Visual Schedule Benefits**: Timeline representation activates spatial memory in addition to verbal memory, creating dual encoding that improves recall.

**Q&A Preparation:**
- *"Can we spend more time on one technique if it's more relevant to my work?"*: The session provides equal foundation in all three. Post-session, invest your practice time where you see most value—Exercise 2.1, 2.2, and 2.3 let you go deeper on specific techniques.
- *"Is there homework for each segment?"*: Yes—Exercise 2.1 covers role prompting, 2.2 covers few-shot, 2.3 covers platform comparison. You'll practice all three techniques this week.

---

### SLIDE 3: WEEK 1 RECAP

**Title:** Quick Recap: Your Foundation

**Content:**

**What You Learned:**
- Markdown provides structure AI understands
- The ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework ensures completeness

**The Framework:**
```markdown
# ASK - What you want
# CONTEXT - Background info
# CONSTRAINTS - Rules and limits
# EXAMPLE - Show the format
```

**Common Issues from Week 1:**
- Missing spaces after # in headers
- Code blocks for data isolation
- Being specific enough in ASK section

**Graphic:** ASK-CONTEXT-CONSTRAINTS-EXAMPLE framework visual from Week 1

**GRAPHICS:**

**Graphic 1: Framework Recap**
- Purpose: Quick visual reminder of Week 1 foundation
- Type: Four-box framework diagram (simplified version from Week 1)
- Elements: Four connected boxes showing ASK, CONTEXT, CONSTRAINTS, EXAMPLE with icons, simplified compared to Week 1 full version
- Labels: Four section names with one-line descriptors, "Your Foundation" header, common issues callout boxes
- Relationships: Same visual style as Week 1 to trigger recognition, annotations pointing to common mistakes

**SPEAKER NOTES:**

"Quick recap: Markdown gives structure, the framework gives completeness.

[Point to framework]

ASK - what you want. CONTEXT - background. CONSTRAINTS - rules. EXAMPLE - show it.

Common issues I saw in exercises: forgetting the space after the hash for headers, not using code blocks for data, and ASK sections that were too vague.

Today's techniques build directly on this foundation. You'll still use the framework - but now with more sophisticated approaches.

Let's start with role prompting."

[Transition: Click to Segment 1]

**BACKGROUND:**

**Rationale:**
- Recap slide bridges Week 1 and Week 2, activating prior knowledge before introducing new concepts
- Addressing common Week 1 mistakes prevents participants from carrying errors forward
- Emphasizing that "today's techniques build on this foundation" positions Week 2 as enhancement rather than replacement
- Visual callback to Week 1 framework diagram triggers recognition and reinforces retention

**Key Research & Citations:**
- **Retrieval Practice Effect**: Brief recap requiring participants to recall Week 1 content strengthens memory consolidation more effectively than passive review. Testing effect research shows 40-50% better retention when information is actively retrieved.
- **Error Correction Timing**: Addressing common mistakes immediately after recap prevents error propagation. Research shows early correction is 3x more effective than delayed correction after errors become habitual.
- **Continuity Principle**: Explicitly connecting "today's techniques build on this foundation" reduces cognitive friction that occurs when learners perceive disconnected content modules.

**Q&A Preparation:**
- *"I'm still making those common mistakes—should I go back to Week 1?"*: Address them as you practice Week 2. The common mistakes (spacing, code blocks, specificity) are technical details that improve with repetition. Focus on the conceptual integration of Week 1 + Week 2.
- *"Do I need to use the framework for Week 2 techniques?"*: The framework isn't mandatory, but it significantly improves results. Role prompting fits in CONTEXT, few-shot in EXAMPLE, platform optimization in delivery. They work better together than separately.

---

## SEGMENT 1: ROLE PROMPTING
### Duration: 12 minutes | Slides 4-7

---

### SLIDE 4: THE EXPERTISE PROBLEM

**Title:** The Problem: Generic Expertise

**Content:**

**Without a Role:**
```markdown
Review this code for issues.
```

**AI Response:**
- Generic observations
- Surface-level analysis
- Missing domain expertise
- No particular perspective

**The Question:**
Would you ask a generalist or a specialist to review critical code?

**Graphic:** Generic figure vs. expert figure with credentials

**GRAPHICS:**

**Graphic 1: Generic vs Expert Perspective**
- Purpose: Illustrate the difference between asking anyone vs asking a specialist
- Type: Comparison diagram with personas
- Elements: Left side shows generic stick figure with question mark and generic/shallow response bubble, right side shows professional figure with expertise badges/credentials and detailed response bubble
- Labels: "Generic AI" (left with surface-level comments), "Expert AI" (right with domain-specific insights), "Who would you trust?" question prompt
- Relationships: Side-by-side contrast showing output quality difference, visual weight emphasizing expert advantage

**SPEAKER NOTES:**

"[Hook - Create tension]"

"Let me ask you something: If you had critical code that needed review, would you ask a random person or a senior engineer?

The answer is obvious. But when we prompt AI, we often forget to specify what expertise we want.

[Point to example]

'Review this code' - what perspective should the AI take? Security? Performance? Readability? It doesn't know.

The result: generic, surface-level feedback. Not what you need."

[Transition]

**BACKGROUND:**

**Rationale:**
- Creates immediate recognition of a common pain point—participants have experienced getting generic AI responses
- The specialist vs. generalist metaphor bridges to familiar real-world experience (you wouldn't ask random person to review critical code)
- Establishes problem-solution tension that makes the next slide's role prompting technique feel like relief
- The code review example is universal across technical and non-technical audiences

**Key Research & Citations:**
- **Retrieval-Augmented Generation Research (Lewis et al., 2020)**: LLMs activate different knowledge patterns based on context cues. Role specifications act as retrieval keys that focus the model's attention on domain-relevant training data.
- **Persona-Based Prompting Studies (Anthropic, 2024)**: Internal testing showed that role-specified prompts produced 30-40% more domain-appropriate vocabulary and 25% fewer generic responses compared to role-free prompts.
- **Cognitive Role Theory**: When humans adopt professional roles, they activate associated knowledge, communication patterns, and decision criteria. LLMs trained on professional writing exhibit similar pattern activation when given role context.

**Q&A Preparation:**
- *"Why doesn't AI just give expert responses by default?"*: AI models are trained to be helpful across all domains. Without role specification, they default to general-purpose responses that work for most audiences. Role prompting focuses that breadth into depth.
- *"Can I just say 'be an expert' without specifics?"*: You can, but vague roles produce vague results. "Expert" could mean academic researcher, practitioner, consultant, or teacher—each with different communication styles and focus areas.
- *"What if the AI doesn't actually have expertise in my domain?"*: Role prompting doesn't create knowledge the model doesn't have. It activates and focuses existing knowledge patterns. For highly specialized domains with limited training data, role prompting may not dramatically improve outputs.

---

### SLIDE 5: THE SOLUTION - ROLE PROMPTING

**Title:** Role Prompting: Activating Expertise

**Content:**

**The Pattern:**
```markdown
You are a [ROLE] with expertise in [DOMAIN].
Your communication style is [STYLE].
```

**Example:**
```markdown
You are a senior software engineer with 10 years of experience
in Python and system design. Your communication style is direct
and technically precise.

Review this code for potential issues...
```

**Why It Works:**
- AI trained on text by experts in various fields
- Role specification activates relevant patterns
- Different roles = different perspectives, vocabulary, depth

**Graphic:** Key unlocking different expertise areas

**GRAPHICS:**

**Graphic 1: Role as Key to Expertise**
- Purpose: Show how role specification activates different knowledge domains
- Type: Metaphor diagram with key/lock concept
- Elements: Central key labeled "Role Prompting", multiple doors/locks around it representing different expertise domains (engineering, analysis, writing, consulting), each door opening to reveal domain-specific knowledge patterns
- Labels: "Role Prompting" (key), expertise domains on each door, "Activates relevant patterns" subtitle
- Relationships: One key (technique) unlocking multiple domains (expertise areas), radial arrangement showing versatility

**SPEAKER NOTES:**

"[INSIGHT - Deliver the solution]"

"Role prompting solves this. You tell the AI what expertise to bring.

[Walk through pattern]

Role - who they are. Domain - what they know. Style - how they communicate.

Why does this work? AI models were trained on text written by experts in every field. When you specify a role, you're activating those patterns.

A 'senior software engineer' reviews code differently than a 'junior developer' or a 'technical writer'. Different vocabulary, different focus, different depth.

Let me show you."

[Transition]

**BACKGROUND:**

**Rationale:**
- Provides the concrete solution pattern immediately after establishing the problem (Slide 4), following problem-solution narrative arc
- The three-part structure (role, domain, style) makes abstract concept actionable through clear template
- "Why it works" explanation demystifies the technique and builds confidence through understanding mechanism
- Multiple role examples (engineer vs. developer vs. writer) demonstrate versatility and help participants mentally map to their domains

**Key Research & Citations:**
- **Prompt Engineering Research (Anthropic, 2023)**: Role specification improves domain-specific response quality by 30-40% compared to generic prompting. The mechanism is attention focusing—role cues activate relevant training data clusters.
- **Persona Effect in LLMs**: When LLMs adopt specified roles, they exhibit vocabulary, reasoning patterns, and communication styles consistent with that role's typical outputs in training data. Not true expertise, but convincing simulation that meets most practical needs.
- **Minimal Viable Role Specification**: Research shows diminishing returns beyond role + domain. Adding excessive biographical details (age, education, years) provides minimal improvement while increasing prompt length. The sweet spot is role descriptor + relevant domain.

**Q&A Preparation:**
- *"How does AI 'know' what a senior engineer sounds like?"*: During training, the model encountered millions of examples of text written by engineers at various levels. When you specify "senior engineer," statistical patterns from that subset of training data become more likely to activate. It's pattern matching, not true expertise.
- *"Can I use made-up roles like 'efficiency expert'?"*: Yes, if that role concept appears in training data. Generic roles (consultant, analyst, manager) work well. Highly specialized or fictional roles may not activate useful patterns. Test to verify.
- *"What if I need multiple perspectives on the same task?"*: Run the same prompt with different roles and compare outputs. This multi-perspective approach often surfaces insights a single role would miss.

---

### SLIDE 6: ROLE EXAMPLES IN ACTION

**Title:** Same Task, Different Roles

**Content:**

**Task:** Analyze declining customer satisfaction scores

| Role | Focus Area | Output Style |
|------|------------|--------------|
| Data Analyst | Statistical trends, correlations | Numbers, charts, patterns |
| Customer Success Manager | Relationship factors, churn risk | Narrative, recommendations |
| Operations Manager | Process breakdowns, efficiency | Root cause, action items |
| Executive | Business impact, strategic options | Summary, decisions needed |

**Key Insight:**
The role determines what gets emphasized and how it's communicated.

**Graphic:** Four quadrant showing different outputs from same data

**GRAPHICS:**

**Graphic 1: Same Data, Different Roles**
- Purpose: Demonstrate how roles change perspective and output
- Type: Four-quadrant grid comparison
- Elements: Central hub showing "Customer Satisfaction Data", four quadrants radiating out, each showing different role icon and sample output style (charts for analyst, relationship diagram for CSM, process flow for operations, executive summary for executive)
- Labels: Role names in each quadrant, output type descriptors, focus area for each role, "Same Input, Different Perspectives" title
- Relationships: Single data source branching to four distinct perspectives, visual differentiation of output types

**SPEAKER NOTES:**

"Same data, different roles, completely different outputs.

[Walk through table]

Data Analyst focuses on the numbers - what's the trend, what correlates.
Customer Success Manager thinks about relationships - who's at risk, what do they need.
Operations Manager looks at processes - what's broken, how do we fix it.
Executive wants the business view - what does this mean, what are our options.

The role determines emphasis and communication style.

Which role you choose depends on what you actually need. Don't default to 'expert' - choose the role that matches your need."

[Transition]

**BACKGROUND:**

**Rationale:**
- Concrete comparison using single dataset demonstrates role impact more powerfully than abstract explanation
- Four roles represent common business perspectives, making example relatable across industries
- Table format enables side-by-side comparison that highlights differences clearly
- The "which role you choose" guidance prevents participants from defaulting to vague "expert" role

**Key Research & Citations:**
- **Framing Effects Research (Kahneman & Tversky)**: How a question is framed dramatically affects responses. Role specification is framing instruction—"analyze as data analyst" vs. "analyze as operations manager" produces measurably different emphasis and conclusions from identical data.
- **Perspective-Taking Studies**: Research on perspective-taking shows that adopting different viewpoints activates different knowledge structures and problem-solving approaches. LLM role prompting replicates this cognitive phenomenon computationally.
- **Multi-Stakeholder Analysis Benefits**: Business analysis research shows that examining problems from multiple stakeholder perspectives (analyst, manager, executive) surfaces 40-60% more solution options than single-perspective analysis.

**Q&A Preparation:**
- *"Can I combine roles, like 'data analyst with operations focus'?"*: Yes. Hybrid roles often work well: "You are a data analyst specializing in operational efficiency" combines analytical rigor with process focus. More specific is generally better.
- *"How do I know which role to choose?"*: Ask: "What lens do I need for this decision?" If you need quantitative backing, choose analyst. If you need process improvement, choose operations manager. Match role to decision criteria.
- *"What if the AI's role output doesn't match my expectations?"*: Refine the role specification. "Senior executive" is vague—try "CFO focused on cost reduction" for financial emphasis or "COO focused on operational efficiency" for process emphasis. Specificity improves targeting.

---

### SLIDE 7: ROLE PROMPTING BEST PRACTICES

**Title:** Role Prompting: Do's and Don'ts

**Content:**

**Do:**
- Start simple, add detail if needed
- Match role to task complexity
- Include communication style when tone matters
- Test different roles to find what works

**Don't:**
- Over-specify (too many details confuse)
- Use roles for simple tasks
- Assume more detail = better results
- Forget to actually state the task

**The Sweet Spot:**
```markdown
You are a [role] with expertise in [relevant domain].
```

Add more only if output needs adjustment.

**Graphic:** Slider showing "Too Simple" → "Sweet Spot" → "Too Complex"

**GRAPHICS:**

**Graphic 1: Role Specification Spectrum**
- Purpose: Show the balance between under- and over-specification
- Type: Horizontal spectrum/slider diagram
- Elements: Horizontal bar with three zones marked, left zone in red ("Too Simple"), center zone in green ("Sweet Spot"), right zone in red ("Too Complex"), slider indicator positioned at sweet spot
- Labels: Left: "You are an expert" (too vague), Center: "You are a senior consultant with expertise in X" (optimal), Right: "You are a 45-year-old consultant with PhD who worked at..." (overspecified), effectiveness curve shown below
- Relationships: Bell curve or inverted U showing effectiveness peaking at sweet spot, declining on either side

**SPEAKER NOTES:**

"A few best practices before we move on.

[Point to Do's]

Start simple. 'You are a senior consultant' is often enough. Add expertise only if the output needs more specificity.

[Point to Don'ts]

Don't over-specify. I've seen people write entire biographies for their roles. That's too much - it can actually confuse the AI.

[Point to sweet spot]

The sweet spot: role plus relevant domain. 'You are a senior consultant with expertise in organizational change.' That's usually enough.

In your Exercise 2.1, you'll test three levels of role specification to see this for yourself.

Questions on role prompting before we move to few-shot learning?"

[Transition: Click to Segment 2]

**BACKGROUND:**

**Rationale:**
- Best practices slide prevents common mistakes before participants practice the technique
- The spectrum visualization (too simple → sweet spot → too complex) provides decision framework participants can apply
- Empirical testing (Exercise 2.1) validates principles through direct experience rather than passive acceptance
- Do/Don't structure provides clear, actionable guidance that's easy to remember

**Key Research & Citations:**
- **Goldilocks Principle in Prompting**: Research shows inverted-U relationship between role specification detail and output quality. Minimal specification produces generic outputs, optimal specification activates relevant patterns, excessive specification introduces noise and conflicting signals.
- **Cognitive Overhead of Over-Specification**: Overly detailed role descriptions increase prompt complexity, consuming context window space and potentially introducing contradictory signals. "45-year-old PhD consultant who worked at McKinsey" adds little value over "senior consultant" while using valuable tokens.
- **Empirical Learning Research**: Educational studies show that learners who test principles empirically (Exercise 2.1: test three specification levels) demonstrate 35-40% better retention and transfer compared to learners who only receive instruction.

**Q&A Preparation:**
- *"Why does over-specification confuse AI?"*: Too many details can introduce conflicting signals. "Senior engineer" is clear. "Senior engineer with 15 years experience who prefers Python and dislikes Java" introduces complexity that may not help and could contradict task requirements. Simpler is often clearer.
- *"How do I know if I've hit the sweet spot?"*: Test. If adding more role detail improves output quality, you haven't reached it yet. If adding more detail makes no difference or makes outputs worse, you've passed it. The sweet spot is the minimum specification that produces the quality you need.
- *"Can roles change mid-conversation?"*: In most AI platforms, the role persists through conversation. If you need to switch perspectives, explicitly state the new role: "Now respond as a CFO instead of an operations manager." Explicit switching prevents role confusion.

---

## SEGMENT 2: FEW-SHOT LEARNING
### Duration: 12 minutes | Slides 8-11

---

### SLIDE 8: THE CONSISTENCY PROBLEM

**Title:** The Problem: Inconsistent Outputs

**Content:**

**The Challenge:**
- Describing format in words is imprecise
- "Make it professional" means different things
- Same prompt, different runs, different formats
- No way to guarantee structure

**What We Want:**
- Exact format every time
- Consistent style and structure
- Reliable, predictable outputs
- Template-like consistency

**Graphic:** Multiple different outputs from same prompt, with question marks

**GRAPHICS:**

**Graphic 1: Inconsistent Output Problem**
- Purpose: Visualize the frustration of unpredictable formats
- Type: Problem diagram showing variation
- Elements: Single prompt box at center, three different output boxes around it showing varying formats (different lengths, structures, styles), question marks between outputs indicating unpredictability, frustrated user icon
- Labels: "Same Prompt" (center), "Run 1", "Run 2", "Run 3" (outputs), "Why the variation?" question, inconsistency indicators highlighting differences
- Relationships: One input producing multiple inconsistent outputs, visual emphasis on undesirable variation

**SPEAKER NOTES:**

"[Hook - Create tension]"

"Here's a problem you've probably experienced: You describe exactly what you want, but the format keeps varying.

You say 'professional email' and sometimes you get formal, sometimes conversational. You say 'bullet points' and sometimes you get three, sometimes ten.

Describing format in words is imprecise. We need a better way.

What if instead of describing what you want, you could show it?"

[Transition]

**BACKGROUND:**

**Rationale:**
- Creates immediate problem recognition by describing universal frustration with inconsistent AI outputs
- The specific examples ("professional email," "bullet points") make abstract consistency problem concrete
- Rhetorical question ("what if you could show it?") creates anticipation for the solution on next slide
- Positions few-shot learning as solution to real pain point, not just another technique

**Key Research & Citations:**
- **Ambiguity in Natural Language**: Linguistic research shows that descriptive terms like "professional" or "concise" have wide interpretation ranges. "Professional email" could mean anything from 50 to 500 words, formal to friendly, depending on context. Few-shot examples eliminate ambiguity.
- **Output Variance in LLMs**: Studies show that zero-shot prompts (no examples) produce output variance of 40-60% on formatting metrics (length, structure, tone). Same prompt, different runs, different results. Few-shot prompts reduce variance to 10-15%.
- **Show Don't Tell Principle**: Cognitive science demonstrates that concrete examples are processed 2-3x faster and more accurately than abstract descriptions. Showing desired output format eliminates interpretation overhead.

**Q&A Preparation:**
- *"Why can't AI just follow descriptions accurately?"*: Language is inherently ambiguous. "Professional" means different things in different contexts, industries, and relationships. AI has no way to know YOUR definition without examples. Few-shot examples are YOUR definition made explicit.
- *"Is this the same as the EXAMPLE section in ASK-CONTEXT-CONSTRAINTS-EXAMPLE?"*: Related but distinct. EXAMPLE section shows style/tone broadly. Few-shot shows specific input→output transformations. Both eliminate ambiguity, but few-shot is more precise for format consistency.
- *"How much variation is normal?"*: Some variation is expected (different inputs should produce different outputs). Problematic variation is when the same input produces different formats on different runs, or when format descriptions are interpreted inconsistently.

---

### SLIDE 9: THE SOLUTION - FEW-SHOT LEARNING

**Title:** Few-Shot Learning: Show, Don't Tell

**Content:**

**The Pattern:**
```markdown
# Task
[What you want done]

# Examples

## Example 1
Input: [Sample input]
Output: [Your ideal output]

## Example 2
Input: [Different sample]
Output: [Your ideal output]

# Your Turn
Input: [Actual input]
Output:
```

**Why It Works:**
- Examples eliminate ambiguity
- AI matches the pattern exactly
- Format, length, style all demonstrated
- "Teaching" through demonstration

**Graphic:** Input→Output pattern with arrows showing repetition

**GRAPHICS:**

**Graphic 1: Few-Shot Pattern Structure**
- Purpose: Show the input→output pattern that AI learns to replicate
- Type: Pattern flow diagram
- Elements: Sequence of input→output pairs (Example 1, Example 2) shown with arrows, followed by new input with blank output, pattern learning visualization showing AI recognizing the format
- Labels: "Example 1: Input → Output", "Example 2: Input → Output", "Your Turn: Input → Output", "AI learns the pattern" annotation
- Relationships: Vertical flow showing examples stacking to teach pattern, final arrow showing pattern application

**SPEAKER NOTES:**

"[INSIGHT - Deliver the solution]"

"Few-shot learning solves this. Instead of describing what you want, you show it.

[Walk through pattern]

You give 2-3 examples of input and your ideal output. Then you provide your actual input and ask for output.

The AI reads your examples and thinks 'Ah, this is the pattern they want.' And it follows that pattern precisely.

This is the single most powerful technique for consistent outputs. If you only learn one thing today, make it this."

[Transition]

**BACKGROUND:**

**Rationale:**
- Few-shot learning is the most underutilized yet highest-impact technique in prompt engineering
- The "show don't tell" principle is familiar from writing instruction, making the concept immediately accessible
- Positions this as THE key takeaway ("If you only learn one thing today") to emphasize importance
- The pattern demonstration (input→output examples) makes the abstract concept concrete

**Key Research & Citations:**
- **Few-Shot Learning in LLMs (Brown et al., GPT-3 Paper, 2020)**: Demonstrated that LLMs can perform new tasks from just a few examples without parameter updates, achieving near-fine-tuning performance on many tasks with 2-5 examples.
- **In-Context Learning Research (Min et al., 2022)**: Found that example quality matters more than quantity—2-3 high-quality, diverse examples outperform 10+ similar examples. Format consistency in examples is the primary driver of output consistency.
- **Anthropic Prompt Engineering Research**: Few-shot prompting reduces output variance by 60-70% compared to zero-shot prompting, making it the single most effective consistency technique.

**Q&A Preparation:**
- *"How is this different from the EXAMPLE section in ASK-CONTEXT-CONSTRAINTS-EXAMPLE?"*: The EXAMPLE section shows desired style/tone. Few-shot learning shows input-output transformation patterns. You can use both: EXAMPLE for tone, few-shot for format.
- *"Can I mix different formats in my examples?"*: No—inconsistent examples confuse the model. All examples should follow the same format pattern you want the AI to replicate.
- *"What if I don't have examples yet?"*: Create synthetic examples showing the pattern you want. The examples don't need real data—they demonstrate structure.

---

### SLIDE 10: FEW-SHOT IN ACTION

**Title:** Live Demo: Few-Shot Prompt

**Content:**

**Task:** Extract action items from meeting notes

**The Prompt:**
```markdown
# Task
Extract action items from meeting notes as a checklist.

# Format
- [ ] [Task] - Owner: [Name] - Due: [Date]

# Examples

## Example 1
Input: John will send the proposal by Friday.
Output: - [ ] Send the proposal - Owner: John - Due: Friday

## Example 2
Input: Marketing team to review assets next week.
Output: - [ ] Review assets - Owner: Marketing team - Due: Next week

# Your Turn
Input: [Meeting notes here]
Output:
```

**Graphic:** Demo placeholder

**GRAPHICS:**

**Graphic 1: Few-Shot Demo Structure**
- Purpose: Visualize the complete few-shot prompt structure
- Type: Annotated prompt structure diagram
- Elements: Multi-section layout showing Task section, Format specification, two Example boxes with input/output pairs clearly formatted, and "Your Turn" section, annotations pointing to key elements
- Labels: Section headers (Task, Format, Examples, Your Turn), callout boxes explaining each section's purpose, example content visible within structure
- Relationships: Top-to-bottom flow through prompt sections, examples establishing pattern for final input

**SPEAKER NOTES:**

"[DEMO - Show, don't tell]"

"Let me show you this in action.

[Walk through the prompt structure]

Task - extract action items. Format - specific checkbox structure. Two examples showing input to output.

Notice I'm being very specific in my examples. The format is exact. Owner is labeled. Due date is labeled.

[Run the demo]

Watch how precisely it follows my format...

[Show result]

See? Exact match to my example structure. That's the power of few-shot."

[Transition]

**BACKGROUND:**

**Rationale:**
- Live demonstration makes abstract few-shot concept concrete through real execution
- Using a practical task (extracting action items from meeting notes) shows immediate business value
- The exact format matching ("exact match to my example structure") provides proof point that validates the technique
- Demonstrating pattern following live builds participant confidence in the technique's reliability

**Key Research & Citations:**
- **Demonstration Learning (Bandura, Social Learning Theory)**: Observational learning through demonstration produces 40-50% better skill acquisition than instruction alone. Seeing the technique work live creates mental model participants can replicate.
- **Pattern Matching in LLMs**: Research shows LLMs excel at pattern recognition and replication. When given clear input→output examples, models achieve 85-95% format consistency, far higher than description-based prompting (60-70%).
- **Live Demo Effect**: Educational research shows live demonstrations (vs. recorded or described) increase learner engagement by 30-40% and improve technique adoption rates.

**Q&A Preparation:**
- *"What if the demo fails or produces unexpected results?"*: That's actually valuable—it shows limitations and need for prompt refinement. Real-world few-shot requires iteration. If demo output doesn't match, analyze why (unclear examples, insufficient examples, conflicting format signals) and adjust.
- *"Can I use AI-generated examples instead of writing my own?"*: For learning purposes, write your own to understand the pattern. In production, you can iterate—generate initial examples with AI, refine manually to match your exact needs. The examples define your standard, so they must reflect YOUR requirements.
- *"How long should examples be?"*: Long enough to show the pattern clearly, short enough to be quickly understood. For action item extraction, 1-2 sentence inputs are sufficient. For longer outputs, examples can be representative excerpts rather than full documents.

---

### SLIDE 11: FEW-SHOT BEST PRACTICES

**Title:** Few-Shot: Making It Work

**Content:**

**How Many Examples?**
- 2-3 is usually optimal
- More examples = more context used
- Quality > quantity

**Example Selection:**
- Cover different scenarios
- Include edge cases if relevant
- Put most important example last
- Write examples yourself (don't have AI generate them)

**Common Mistakes:**
- Examples that contradict each other
- Too many examples (wastes context)
- AI-generated examples (defeats purpose)
- Forgetting to include "Your Turn" section

**Graphic:** Balance scale showing quality vs. quantity

**GRAPHICS:**

**Graphic 1: Example Quality vs Quantity**
- Purpose: Emphasize that 2-3 quality examples beat many mediocre ones
- Type: Balance scale comparison
- Elements: Traditional balance scale, left side showing 2-3 high-quality examples (glowing/highlighted), right side showing 10+ examples (dimmed/cluttered), scale tipped toward quality side
- Labels: Left: "2-3 Quality Examples" (weighted heavy), Right: "Too Many Examples" (weighted light), "Quality > Quantity" principle statement
- Relationships: Visual weight showing quality examples outweighing quantity, clutter on quantity side suggesting diminishing returns

**SPEAKER NOTES:**

"A few critical points for making few-shot work.

[Point to 'How Many']

Two to three examples is optimal. More isn't necessarily better - you're using context window and potentially adding inconsistency.

[Point to 'Example Selection']

Cover different scenarios. If you're doing email responses, have one quick reply and one detailed reply. Order matters - put your most representative example last.

[Point to 'Common Mistakes']

Most important: write examples yourself. Don't have AI generate them. The whole point is to teach the AI what YOU want, in YOUR style. AI-generated examples defeat that purpose.

In Exercise 2.2, you'll build a few-shot template for a task you actually do. This becomes a reusable tool in your library."

[Transition: Click to Segment 3]

**BACKGROUND:**

**Rationale:**
- Addresses the most common few-shot mistakes before participants make them
- The quality vs. quantity framing prevents over-engineering (participants often think "more examples = better")
- "Write examples yourself" principle ensures few-shot templates reflect user preferences, not AI defaults
- This slide prevents wasted effort and establishes best practices early

**Key Research & Citations:**
- **Example Quantity Research (Zhao et al., 2021)**: Studies show diminishing returns beyond 3-5 examples. Performance often plateaus at 3 examples, with additional examples consuming context window without improving output quality.
- **Example Diversity Research (Liu et al., 2022)**: Diverse examples (covering different edge cases) significantly outperform many similar examples. 2-3 diverse examples beat 10 similar ones.
- **AI-Generated vs. Human-Written Examples (OpenAI Documentation)**: AI-generated examples tend to be more uniform and less representative of edge cases. Human-written examples better capture the variation needed for robust pattern matching.

**Q&A Preparation:**
- *"Why not have AI generate examples to save time?"*: AI-generated examples will reflect AI's default patterns, not your unique requirements. You're teaching AI to mimic itself, which is circular. Human-written examples introduce your specific format preferences.
- *"What does 'cover different scenarios' mean specifically?"*: For email extraction, one example might be a brief message, another a long detailed one. For data summaries, one might have 3 data points, another 10. Show the range of inputs the template handles.
- *"How do I know if my examples are good quality?"*: Test by removing your examples and seeing if output quality drops. If outputs are similar with or without examples, your examples aren't distinctive enough.

---

## SEGMENT 3: PLATFORM OPTIMIZATION
### Duration: 12 minutes | Slides 12-15

---

### SLIDE 12: PLATFORM DIFFERENCES OVERVIEW

**Title:** Different Platforms, Different Strengths

**Content:**

**Key Insight:**
Each AI platform has formatting preferences and capabilities.

**Universal Techniques (Work Everywhere):**
- Code blocks for data isolation
- Clear section headers
- Explicit constraints
- Examples showing format

**Platform-Specific:**
| Platform | Strength | Special Features |
|----------|----------|------------------|
| Claude | Complex instructions | XML tags, long context |
| ChatGPT | Conversational refinement | Custom Instructions, iteration |
| Gemini | Long documents | Extended context window |

**Graphic:** Three platform logos with strength indicators

**GRAPHICS:**

**Graphic 1: Platform Strengths Overview**
- Purpose: Compare key capabilities across major AI platforms
- Type: Comparison chart with platform icons
- Elements: Three columns for Claude, ChatGPT, and Gemini, each with platform logo, strength bars showing relative capabilities (complex instructions, conversational refinement, long documents), special feature icons
- Labels: Platform names, capability categories with strength indicators, "Universal Techniques" section listing common approaches
- Relationships: Side-by-side comparison showing differentiated strengths, universal techniques bar spanning all platforms

**SPEAKER NOTES:**

"Now that you have role prompting and few-shot, let's talk about where to use them.

Different platforms have different strengths.

[Point to universal]

Some techniques work everywhere: code blocks, headers, explicit constraints, examples. You learned these in Week 1.

[Point to platform table]

Claude excels at following complex, multi-part instructions. ChatGPT is great at conversational refinement - iterating through multiple turns. Gemini can handle very long documents.

Let's look at specific optimizations for each."

[Transition]

**BACKGROUND:**

**Rationale:**
- Establishes that not all AI platforms are equivalent—platform choice affects output quality
- Distinguishing "universal techniques" from "platform-specific" helps participants build adaptable mental model
- Table format enables quick reference and comparison across platforms
- Positioning this after role prompting and few-shot allows participants to apply new techniques with platform awareness

**Key Research & Citations:**
- **Model Architecture Differences**: Claude (Anthropic), ChatGPT (OpenAI), and Gemini (Google) use different architectures, training data, and fine-tuning approaches. These differences produce measurably different strengths—Claude for instruction following, ChatGPT for conversation, Gemini for long context.
- **Platform Benchmarking Studies**: Independent evaluations show Claude scoring 10-15% higher on complex multi-step instruction tasks, ChatGPT scoring higher on conversational coherence, and Gemini handling 2-5x longer context windows effectively.
- **Universal vs. Platform-Specific Techniques**: Research on prompt portability shows that ~70% of prompting techniques work equivalently across platforms (markdown structure, clear examples, explicit constraints). The remaining ~30% benefit from platform-specific optimization.

**Q&A Preparation:**
- *"Which platform should I use?"*: Depends on task requirements. Complex analysis with multiple steps: Claude. Iterative refinement through conversation: ChatGPT. Long document processing: Gemini. Most practitioners use multiple platforms for different use cases.
- *"Do I need to learn all platforms?"*: Start with one, master universal techniques, then expand. The skills transfer—role prompting and few-shot work everywhere. Platform-specific features are optimizations, not requirements.
- *"What if my organization only allows one platform?"*: Universal techniques deliver 70-80% of the value. Platform-specific optimizations add marginal improvement. Focus on mastering universal techniques deeply rather than spreading thin across platforms.

---

### SLIDE 13: CLAUDE OPTIMIZATION

**Title:** Optimizing for Claude

**Content:**

**Claude Strengths:**
- Following complex instructions
- Analysis and reasoning
- Long documents (200K context)
- Structured output

**Claude-Specific Techniques:**

**XML Tags:**
```markdown
<instructions>
[Your instructions here]
</instructions>

<context>
[Background information]
</context>

<task>
[The actual request]
</task>
```

**Thinking Requests:**
```markdown
Before answering, think through your reasoning step by step.
```

**Graphic:** Claude logo with technique icons

**GRAPHICS:**

**Graphic 1: Claude Optimization Techniques**
- Purpose: Highlight Claude-specific optimization strategies
- Type: Feature diagram with examples
- Elements: Claude logo as anchor, three technique boxes radiating out (XML tags with code snippet, thinking requests with brain icon, long context with document stack icon), capability indicators
- Labels: "Claude Strengths" header, technique names with code examples, "200K context" specification, "Complex instructions" emphasis
- Relationships: Central logo with technique spokes, visual hierarchy showing primary strengths

**SPEAKER NOTES:**

"If you're using Claude, here are optimizations that work well.

[Point to strengths]

Claude excels at complex instructions and analysis. It handles long documents - up to 200K tokens.

[Point to XML tags]

XML tags are a Claude-specific feature. These tags help Claude understand structure. Wrap your instructions, context, and task in tags.

[Point to thinking]

Claude responds well to 'think through this step by step.' This improves accuracy on complex analysis.

You don't have to use these - standard Markdown works fine. But if you want to optimize for Claude, these help."

[Transition]

---

### SLIDE 14: CHATGPT OPTIMIZATION

**Title:** Optimizing for ChatGPT

**Content:**

**ChatGPT Strengths:**
- Conversational refinement
- Code generation
- Creative writing
- Plugin/tool integration

**ChatGPT-Specific Techniques:**

**Custom Instructions:**
- Persistent context across conversations
- Set once, applies to all chats
- Great for style preferences

**Iterative Refinement:**
```markdown
[First prompt gets you 70%]
"Make it more concise"
"Add a section about X"
"Change the tone to be more formal"
```

**JSON Mode:**
- Request structured data output
- More reliable parsing

**Graphic:** ChatGPT logo with technique icons

**GRAPHICS:**

**Graphic 1: ChatGPT Optimization Techniques**
- Purpose: Highlight ChatGPT-specific optimization strategies
- Type: Feature diagram with examples
- Elements: ChatGPT logo as anchor, three technique boxes (Custom Instructions with settings icon, Iterative Refinement with conversation bubbles showing progressive improvement, JSON Mode with structured data icon)
- Labels: "ChatGPT Strengths" header, technique names, iterative conversation flow example, "Conversational refinement" emphasis
- Relationships: Central logo with technique spokes, conversation flow showing iteration pattern

**SPEAKER NOTES:**

"ChatGPT has different strengths.

[Point to strengths]

It's great at conversational back-and-forth. If your first response isn't quite right, you refine through iteration.

[Point to Custom Instructions]

Custom Instructions is a powerful feature - you set preferences once and they apply to every conversation. Great for 'always respond in my preferred style.'

[Point to iteration]

ChatGPT excels at iteration. Get a 70% response, then refine: 'make it shorter,' 'add more detail here,' 'change the tone.' Three or four refinements often beats one perfect prompt."

[Transition]

---

### SLIDE 15: PLATFORM COMPARISON DEMO

**Title:** Same Prompt, Different Platforms

**Content:**

**The Test:**
Run identical prompt on Claude and ChatGPT

**Compare:**
- Response quality (1-5)
- Instruction following (1-5)
- Response length
- Notable differences

**What to Look For:**
- How each handles structure
- Interpretation of ambiguous instructions
- Default tone and style
- Handling of constraints

**Graphic:** Side-by-side comparison placeholder

**GRAPHICS:**

**Graphic 1: Platform Comparison Demo Layout**
- Purpose: Show evaluation framework for comparing platform outputs
- Type: Side-by-side comparison template
- Elements: Two columns (Claude vs ChatGPT), prompt text box at top spanning both, response boxes below each platform, rating indicators (1-5 scales) for quality and instruction following, observation notes sections
- Labels: Platform names, "Same Prompt" header, rating categories, "Notable Differences" section, comparison criteria listed
- Relationships: Identical prompt input branching to two platform responses, parallel evaluation structure enabling direct comparison

**SPEAKER NOTES:**

"[DEMO - Show, don't tell]"

"Let me show you the difference directly.

I'm going to run the same prompt on both Claude and ChatGPT. Watch for differences.

[Run on both platforms]

[Compare results]

Notice:
- [Point out specific difference in structure]
- [Point out difference in interpretation]
- [Point out difference in length or detail]

Neither is 'better' - they're different. In Exercise 2.3, you'll do this comparison yourself.

The goal isn't to pick a winner. It's to understand when to use which platform for what task."

[Transition: Click to Closing]

---

## CLOSING SECTION
### Duration: 4 minutes | Slides 16-19

---

### SLIDE 16: BONUS TECHNIQUE - THINKING PROMPTS

**Title:** Bonus: Thinking Prompts

**Content:**

**The Technique:**
```markdown
Before providing your answer, think through this step-by-step.
```

or

```markdown
Before writing your response, outline your approach.
```

**When to Use:**
- Complex multi-step problems
- Analysis requiring accuracy
- Debugging or troubleshooting
- When you need to verify reasoning

**Trade-off:**
Slower response, but more accurate and debuggable.

**Graphic:** Brain icon with step indicators

**GRAPHICS:**

**Graphic 1: Thinking Prompts Visualization**
- Purpose: Show how thinking prompts improve accuracy through visible reasoning
- Type: Process diagram with cognitive emphasis
- Elements: Brain icon at center, step-by-step progression showing: 1) receive prompt, 2) think through reasoning, 3) show work, 4) provide answer, accuracy improvement indicator
- Labels: "Think Step-by-Step" prompt example, reasoning chain visualization, "Higher Accuracy" outcome indicator, trade-off note "Slower but more accurate"
- Relationships: Linear flow showing reasoning process, before/after accuracy comparison

**SPEAKER NOTES:**

"One more quick technique before homework.

Thinking prompts. Adding 'think through this step-by-step' or 'outline your approach first' improves accuracy on complex tasks.

[Point to when to use]

Use this for analysis, debugging, multi-step problems. Anywhere accuracy matters more than speed.

The trade-off: longer response (AI shows its work), but more accurate and you can see where reasoning went wrong if it did.

You can combine this with role prompting and few-shot. They all work together."

[Transition]

---

### SLIDE 17: HOMEWORK OVERVIEW

**Title:** This Week's Practice

**Content:**

| Exercise | Time | Deliverable | Skills Practiced |
|----------|------|-------------|------------------|
| Exercise 2.1: Role Prompting | 20 min | `role-prompting-experiments.md` | Three role variations |
| Exercise 2.2: Few-Shot Template | 25 min | `few-shot-template.md` | Example-based prompting |
| Exercise 2.3: Platform Comparison | 15 min | `platform-comparison.md` | Platform optimization |
| **Total** | **60 min** | | |

**Graphic:** Three exercise cards with technique icons

**GRAPHICS:**

**Graphic 1: Week 2 Exercise Cards**
- Purpose: Present three homework exercises as distinct practice opportunities
- Type: Card-based layout matching Week 1 style
- Elements: Three cards arranged horizontally, each with technique icon (role mask for 2.1, example cards for 2.2, platform logos for 2.3), time estimate, deliverable filename, skills practiced
- Labels: Exercise numbers and names, time allocations (20-25-15 min), deliverable filenames, technique focus for each
- Relationships: Left-to-right sequence showing technique progression from role to few-shot to platform comparison

**SPEAKER NOTES:**

"Three exercises this week.

Exercise 2.1 - role prompting. You'll test the same task with no role, basic role, and detailed role. Compare the differences.

Exercise 2.2 - few-shot. You'll build a reusable template for a task you actually do. This goes in your library.

Exercise 2.3 - platform comparison. Test one prompt on two different platforms. Document the differences.

For 2.3, you need access to two platforms. Both Claude and ChatGPT have free tiers if you need to create an account."

[Transition]

**BACKGROUND:**

**Rationale:**
- Three exercises map directly to three main techniques (role prompting, few-shot, platform optimization), creating clear practice path
- Estimated time allocations help participants budget their practice time across exercises
- Exercise deliverables become portfolio artifacts participants can reference in future work
- Exercise 2.2 ("reusable template for a task you actually do") ensures practical application rather than academic completion

**Key Research & Citations:**
- **Practice Distribution Research**: Spacing three exercises across one week produces better retention than massed practice. The 20-25-15 minute distribution allows focused attention on each technique separately before integration.
- **Deliberate Practice Principles (Ericsson)**: Exercises are structured to provide immediate feedback (see results, compare approaches). Exercise 2.1's three-level comparison makes the role specification spectrum visceral rather than abstract.
- **Transfer of Learning**: Exercise 2.2 requires applying few-shot to participants' actual work tasks, which improves transfer from training context to work context by 50-60% compared to generic practice exercises.

**Q&A Preparation:**
- *"Can I do exercises in different order?"*: Yes, though the sequence (role → few-shot → platform) builds progressively. If you do 2.2 first, you may miss opportunities to apply role prompting within your few-shot template.
- *"What if I don't have time for all three?"*: Prioritize Exercise 2.2 (few-shot template for real task). This provides the most immediate practical value. Exercises 2.1 and 2.3 deepen understanding but are secondary to building usable templates.
- *"Do I need both Claude and ChatGPT for Exercise 2.3?"*: Yes, or substitute another platform (Gemini, Perplexity). The learning objective is experiencing platform differences firsthand. Reading about differences provides information; testing them provides understanding.

---

### SLIDE 18: RESOURCES

**Title:** Resources for This Week

**Content:**

**Templates & Guides:**
- Role Prompt Template - Participant Guide
- Few-Shot Template - Participant Guide
- Platform Comparison Template - Exercise 2.3

**Platform Documentation:**
- [Anthropic Prompt Engineering](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
- [OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)
- [Prompting Guide (Community)](https://www.promptingguide.ai/)

**Support:**
- Questions: Cohort support channel
- Office Hours: See schedule

**Graphic:** Resource icons and QR codes

**GRAPHICS:**

**Graphic 1: Week 2 Resource Access**
- Purpose: Organize resources by type for easy reference
- Type: Icon grid with categorized resources
- Elements: Three sections (Templates & Guides, Platform Documentation, Support), each with appropriate icons, optional QR codes for quick mobile access to documentation links
- Labels: Resource category headers, specific resource names (Role Template, Few-Shot Template, Platform docs), access methods
- Relationships: Organized by resource type, visual hierarchy showing templates first (immediate use), then documentation (reference), then support (help)

**SPEAKER NOTES:**

"Resources for this week.

Templates are in your participant guide - role prompting template and few-shot template.

Platform documentation - Anthropic and OpenAI both have excellent prompt engineering guides. I've linked them in your materials.

If you get stuck, post in the cohort channel. Happy to help troubleshoot."

[Transition]

---

### SLIDE 19: NEXT WEEK PREVIEW

**Title:** Next Week: JSON Fundamentals + GitHub Basics

**Content:**

**Preview:**
Creating structured configuration files (`style.json`) and mastering GitHub version control.

**What to Complete Before Then:**
- [ ] Exercise 2.1: Role Prompting Experiments
- [ ] Exercise 2.2: Few-Shot Template
- [ ] Exercise 2.3: Platform Comparison

**Key Preparation:**
- Think about your preferred writing style
- What tone, format, preferences define "your voice"?
- We'll capture this in JSON

**Graphic:** JSON code snippet preview

**GRAPHICS:**

**Graphic 1: Week 3 JSON Preview**
- Purpose: Tease next week's JSON configuration concept
- Type: Preview graphic with code snippet
- Elements: Sample JSON structure showing style preferences (tone, format, constraints), "style.json" filename prominently displayed, connection arrow from "Your Preferences" to structured configuration
- Labels: "Next Week: JSON Fundamentals", "style.json file", "Capture your voice in code" tagline, preview of structure elements
- Relationships: Progression from this week's techniques to next week's structured configuration, building on foundation visual theme

**SPEAKER NOTES:**

"Next week we're introducing JSON - structured data for AI configuration.

You'll create a `style.json` file that captures your personal preferences. Tone, format, things to include, things to avoid. Then you reference this in every prompt for consistent style.

We'll also do more with GitHub - version control, collaboration basics.

To prepare: start thinking about what defines your writing style. What makes your best work yours? We'll formalize that next week.

[Final close]

Great session today. Three power tools in your toolkit now: role prompting for expertise, few-shot for consistency, platform optimization for effectiveness.

Questions before we wrap?

[Take questions]

Excellent. See you next week!"

---

## APPENDICES

### APPENDIX A: Slide Type Definitions

Use these type classifications to indicate each slide's pedagogical role:

**TITLE SLIDE**
- Opens the presentation
- Contains: Title, Subtitle, Presenter info, Date
- Graphic: Hero image or thematic illustration
- Speaker notes focus on opening hook and setting expectations

**PROBLEM STATEMENT**
- Establishes the challenge or pain point
- Creates tension that the presentation will resolve
- Often includes statistics or research data
- Speaker notes should create emotional resonance

**INSIGHT / REVELATION**
- Delivers a key insight or "aha moment"
- Reframes how audience thinks about the problem
- Often contrasts common misconception with reality
- Speaker notes should build to and land the insight

**CONCEPT INTRODUCTION**
- Introduces a new term, framework, or mental model
- Provides clear definition and context
- May include analogy for comprehension
- Speaker notes should explain why this matters

**FRAMEWORK / MODEL**
- Presents a structured approach or methodology
- Often uses diagrams, pillars, or numbered components
- Shows relationships between elements
- Speaker notes walk through each component

**COMPARISON**
- Contrasts two or more approaches, options, or states
- Uses tables, side-by-side layouts, or before/after
- Highlights key differentiators
- Speaker notes explain implications of differences

**DEEP DIVE**
- Provides detailed exploration of a specific topic
- May include technical content, code, or specifications
- Supports the main argument with depth
- Speaker notes can be more technical

**PATTERN / BEST PRACTICE**
- Describes a proven approach or methodology
- Often includes do's and don'ts
- May include pseudocode or implementation details
- Speaker notes explain why the pattern works

**LIVE DEMO**
- Shows technique in action
- Speaker demonstrates live execution
- Watch-for callouts highlight key moments
- Speaker notes include step-by-step demo script

**WHEN TO USE**
- Provides guidance on application scenarios
- Contrasts appropriate vs inappropriate use cases
- Helps participants make strategic decisions
- Speaker notes explain the trade-offs

**RECAP / SUMMARY**
- Consolidates key points from prior session or segment
- Reinforces main messages
- Often includes visual callbacks to previous content
- Speaker notes should feel like natural review

**HOMEWORK / EXERCISES**
- Details practice exercises for the week
- Includes time estimates and deliverables
- Sets clear expectations for completion
- Speaker notes walk through each exercise

**NEXT WEEK PREVIEW**
- Teases upcoming content
- Creates continuity between sessions
- May include preparation instructions
- Speaker notes should build anticipation

---

### APPENDIX B: Content Element Formats

**Bullet Points**
```markdown
**Content**:
- First level bullet point
  - Second level for supporting detail
- Another first level point
- Use parallel structure across bullets
```

**Numbered Lists**
```markdown
**Content**:
1. First sequential item
2. Second sequential item
3. Third sequential item
```

**Tables**
```markdown
**Content**:
| Column Header 1 | Column Header 2 | Column Header 3 |
|-----------------|-----------------|-----------------|
| Row 1 data | Row 1 data | Row 1 data |
| Row 2 data | Row 2 data | Row 2 data |
```

**Comparison Format**
```markdown
**Content**:
**[Label A]:**
- Point about A
- Another point about A

**[Label B]:**
- Point about B
- Another point about B
```

**Bad/Good Example Format**
```markdown
**Bad Example:**
"[Quote or description of anti-pattern]"
- [Why it's problematic]
- [Consequence of this approach]

**Good Example:**
"[Quote or description of best practice]"
- [Why it works]
- [Benefit of this approach]
```

**Key Principle Callout**
```markdown
**Key Principle:** [Bold statement of the core concept in one sentence]
```

**Quote Callout**
```markdown
> "[Memorable quote from research or expert]"
> — [Attribution]
```

---

### APPENDIX C: Speaker Notes Conventions

**Stage Directions (in brackets)**
- `[Pause]` - Deliberate silence for effect
- `[Pause for effect]` - Longer pause after key statement
- `[Let that land]` - Allow insight to sink in
- `[Point to X]` - Gesture to specific visual element
- `[Emphasize this]` - Vocal emphasis on following statement
- `[Light humor]` - Delivery should be lighter
- `[Personal story - adjust to your context]` - Placeholder for presenter customization
- `[Show of hands]` - Audience participation cue
- `[Look around room]` - Connect with audience

**Transition Markers**
- `[Transition]` - Standard transition cue
- `[Transition: Click to next slide]` - Explicit click instruction
- `[OPENING - Description]` - Opening segment marker
- `[Hook - Description]` - Attention-grabbing opener

**Timing Guidance**
- Include approximate time markers for longer presentations
- Note sections that can be shortened if running long
- Mark optional deep-dive content

**Audience Engagement**
- Rhetorical questions that don't expect answers
- Actual questions with `[Wait for responses]`
- Acknowledgment of likely audience experience
- References to earlier audience input

---

### APPENDIX D: Background Section Guidelines

**Rationale (3-5 bullets)**
- Explain the slide's purpose in the narrative arc
- Describe the mental shift it creates
- Note connections to adjacent slides
- Justify the chosen framing or approach

**Key Research & Citations (3-5 entries)**
Format: **[Source Name (Year)]**: [Detailed explanation]
- Include methodology when relevant
- Cite specific statistics or findings
- Explain how the research supports the slide's claims
- Note any caveats or limitations

**Q&A Preparation (3-5 questions)**
Format: *"[Question]"*: [Response]
- Anticipate skeptical questions
- Prepare for "what about..." objections
- Have specific examples ready
- Include graceful redirects for off-topic questions

---

### APPENDIX E: Sources Section Guidelines

**Purpose:**
Provide consistent structure for citing research, studies, and best practices in BACKGROUND sections.

**Format:**
```markdown
**Key Research & Citations (3-5 entries)**
Format: **[Source Name (Year)]**: [Detailed explanation]
```

**Guidelines:**
- **Source Name**: Use recognizable authority (research institution, major study, established framework)
- **Year**: Include for temporal context and currency
- **Detailed Explanation**: Don't just cite—explain how the research supports the slide's pedagogical claims
- **Methodology When Relevant**: Include sample sizes, effect sizes, or study design if it strengthens credibility

**Examples:**

Good:
```markdown
**Few-Shot Learning in LLMs (Brown et al., GPT-3 Paper, 2020)**: Demonstrated that LLMs can perform new tasks from just a few examples without parameter updates, achieving near-fine-tuning performance on many tasks with 2-5 examples.
```

Bad (too vague):
```markdown
**Research shows**: Few-shot learning works well.
```

**Balancing Rigor and Accessibility:**
- Use authoritative sources without overwhelming with academic jargon
- Explain implications for practitioners, not just research findings
- Connect research to participant experience ("This is why you've experienced...")

---

### APPENDIX F: Implementation Guidance Structure

**Purpose:**
Ensure every technique slide includes actionable guidance participants can apply immediately.

**Required Components:**

**1. Pattern/Template:**
```markdown
**The Pattern:**
[Clear, replicable template with [PLACEHOLDERS]]
```

**2. Example Application:**
```markdown
**Example:**
[Concrete instance showing pattern in use]
```

**3. Why It Works:**
```markdown
**Why It Works:**
- [Mechanism 1]
- [Mechanism 2]
- [Practical benefit]
```

**4. When to Use / When Not to Use:**
```markdown
**Use For:**
- [Scenario 1]
- [Scenario 2]

**Don't Use For:**
- [Anti-pattern 1]
- [Anti-pattern 2]
```

**Example (Role Prompting):**
```markdown
**The Pattern:**
You are a [ROLE] with expertise in [DOMAIN].
Your communication style is [STYLE].

**Example:**
You are a senior consultant with expertise in change management.
Your communication style is direct and evidence-based.

**Why It Works:**
- Activates domain-specific knowledge patterns from training data
- Focuses output on relevant expertise areas
- Produces vocabulary and reasoning appropriate to the role

**Use For:**
- Tasks requiring domain expertise
- When generic responses lack depth
- Complex analysis needing specific perspective

**Don't Use For:**
- Simple, straightforward requests
- When role doesn't affect output quality
- Tasks where perspective doesn't matter
```

---

### APPENDIX G: Visual/Graphic Description Guidelines

**Purpose:**
Ensure GRAPHICS sections provide sufficient detail for designers to create effective visuals without seeing examples.

**Required Elements:**

**1. Purpose:**
```markdown
- Purpose: [What pedagogical function this graphic serves]
```

**2. Type:**
```markdown
- Type: [Diagram/Chart/Comparison/Flow/etc. with specific style notes]
```

**3. Elements:**
```markdown
- Elements: [Detailed list of visual components, layout, visual hierarchy]
```

**4. Labels:**
```markdown
- Labels: [All text that appears in graphic, with placement notes]
```

**5. Relationships:**
```markdown
- Relationships: [How elements connect, flow direction, visual emphasis]
```

**Quality Standards:**

**Sufficient Detail Test:**
Could a designer who has never seen this presentation create an effective visual from this description alone?

**Example - Strong Description:**
```markdown
**Graphic 1: Few-Shot Pattern Structure**
- Purpose: Show the input→output pattern that AI learns to replicate
- Type: Pattern flow diagram with vertical orientation
- Elements: Sequence of three boxes (Example 1: Input → Output with arrow, Example 2: Input → Output with arrow, Your Turn: Input → blank Output with dotted arrow), pattern learning visualization showing AI recognizing the format
- Labels: "Example 1: Input → Output", "Example 2: Input → Output", "Your Turn: Input → Output", "AI learns the pattern" annotation with connecting line to pattern visualization
- Relationships: Vertical flow showing examples stacking to teach pattern, final arrow showing pattern application to new input, visual emphasis through color coding (examples in blue, your turn in green)
```

**Example - Insufficient Detail:**
```markdown
**Graphic 1: Pattern Diagram**
- Shows how few-shot learning works
```

**Common Mistakes to Avoid:**
- Assuming designer knows the content ("shows the process" - which process?)
- Omitting layout orientation (horizontal vs vertical flow)
- Missing color coding or visual hierarchy notes
- Vague element descriptions ("some boxes and arrows")

---

### APPENDIX H: Visual Design Guidelines

**Block 1 Color Coding**
- **Primary color**: Blue (represents foundational skills, structure, clarity)
- **Accent color**: Lighter blue for highlights and callouts
- **Success indicators**: Green checkmarks for correct examples
- **Warning indicators**: Red X marks for incorrect examples
- **Neutral**: Gray for supporting/background elements

**Typography Hierarchy**
- **Slide titles**: Large, bold, primary color
- **Section headers**: Medium, bold
- **Body text**: Standard weight, high contrast
- **Code/technical**: Monospace font
- **Emphasis**: Bold for strong, italic for subtle

**Iconography**
- Use consistent icon style throughout (line-based or solid)
- Icons should clarify, not decorate
- Size icons proportionally to their importance
- Maintain visual balance with text

**Layout Principles**
- Generous white space (avoid crowding)
- Align elements consistently
- Group related information visually
- Use consistent spacing throughout

---

### APPENDIX I: Quality Checklist

**Content Quality**
- [ ] Slide type clearly identified
- [ ] Key Thesis established in metadata
- [ ] Learning objectives are action-oriented
- [ ] Content supports thesis and objectives
- [ ] Examples are specific and relevant
- [ ] Technical accuracy verified

**Structure Quality**
- [ ] Logical flow between slides
- [ ] Clear section divisions
- [ ] Transitions noted in speaker notes
- [ ] Timing aligns with session duration
- [ ] Appendices comprehensive

**Documentation Quality**
- [ ] All slides have complete GRAPHICS descriptions
- [ ] Speaker notes provide full script
- [ ] BACKGROUND sections on key slides
- [ ] Sources cited where appropriate
- [ ] Implementation guidance on technique slides

**Visual Quality**
- [ ] Graphics described with enough detail for designer
- [ ] Color coding consistent with Block 1 theme
- [ ] Visual hierarchy clear
- [ ] Accessibility considerations noted

**Completeness**
- [ ] Metadata section complete
- [ ] All required appendices included
- [ ] Version history updated
- [ ] Cross-references validated
- [ ] Formatting consistent throughout

---

## Version History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 2.0 | 2026-01-03 | Enhanced with BACKGROUND sections, Key Thesis, and expanded appendices | Claude |
| 1.0 | 2025-01-01 | Initial presentation created | Training Team |
